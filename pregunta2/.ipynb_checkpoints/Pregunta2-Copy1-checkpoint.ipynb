{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>a) Cargue los datos de entrenamiento y pruebas (\\train 32x32.mat\" y \\test 32x32.mat\"). Determine el\n",
    "tama~no de las im\u0013agenes, el n\u0013umero de clases diferentes y de ejemplos en cada categor\u0013\u0010a. Finalmente,\n",
    "visualice 5 im\u0013agenes de entrenamiento y 5 de test (elegidas aleatoriamente).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Cantidad de ejemplos por categoría en dataset de entrenamiento\n",
      "clase: 0\n",
      "13861\n",
      "___________________________________________-\n",
      "clase: 1\n",
      "10585\n",
      "___________________________________________-\n",
      "clase: 2\n",
      "8497\n",
      "___________________________________________-\n",
      "clase: 3\n",
      "7458\n",
      "___________________________________________-\n",
      "clase: 4\n",
      "6882\n",
      "___________________________________________-\n",
      "clase: 5\n",
      "5727\n",
      "___________________________________________-\n",
      "clase: 6\n",
      "5595\n",
      "___________________________________________-\n",
      "clase: 7\n",
      "5045\n",
      "___________________________________________-\n",
      "clase: 8\n",
      "4659\n",
      "___________________________________________-\n",
      "clase: 9\n",
      "4948\n",
      "___________________________________________-\n",
      "Cantidad de ejemplos por categoría en dataset de prueba\n",
      "clase: 0\n",
      "5099\n",
      "___________________________________________-\n",
      "clase: 1\n",
      "4149\n",
      "___________________________________________-\n",
      "clase: 2\n",
      "2882\n",
      "___________________________________________-\n",
      "clase: 3\n",
      "2523\n",
      "___________________________________________-\n",
      "clase: 4\n",
      "2384\n",
      "___________________________________________-\n",
      "clase: 5\n",
      "1977\n",
      "___________________________________________-\n",
      "clase: 6\n",
      "2019\n",
      "___________________________________________-\n",
      "clase: 7\n",
      "1660\n",
      "___________________________________________-\n",
      "clase: 8\n",
      "1595\n",
      "___________________________________________-\n",
      "clase: 9\n",
      "1744\n",
      "___________________________________________-\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "%matplotlib inline\n",
    "\n",
    "train_data = sio.loadmat('train_32x32.mat')\n",
    "test_data = sio.loadmat('test_32x32.mat')\n",
    "X_train = train_data['X'].T\n",
    "y_train = train_data['y'] - 1\n",
    "X_test = test_data['X'].T\n",
    "y_test = test_data['y'] - 1\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "n_classes = len(np.unique(y_train))\n",
    "print(n_classes)\n",
    "occurrenses = 0\n",
    "print(\"Cantidad de ejemplos por categoría en dataset de entrenamiento\")\n",
    "for i in range(0,10):\n",
    "    print(\"clase: \" + str(i))\n",
    "    occurrenses += np.count_nonzero(y_train == [i])\n",
    "    print(np.count_nonzero(y_train == [i]))\n",
    "    print(\"___________________________________________-\")\n",
    "\n",
    "print(\"Cantidad de ejemplos por categoría en dataset de prueba\")\n",
    "for i in range(0,10):\n",
    "    print(\"clase: \" + str(i))\n",
    "    print(np.count_nonzero(y_test == [i]))\n",
    "    print(\"___________________________________________-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "39112\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGz9JREFUeJztnX+MXFd1x79n3szseHfH+8O7Wf+IEyckUNIUDFpFkUAI\nimhThBT4J4I/UKRGmD8oKhL9I0qlkv5HqwLijwrJNBGhpUDUgIiqqFWwqCKqNmQJITGxW5Lg1HZs\n7zre37uzM/Pm9I95UdfLPWdmZ3ff2r3fj2R59t6575257555M/c75xxRVRBC4qOw2wYQQnYHOj8h\nkULnJyRS6PyERAqdn5BIofMTEil0fkIihc5PSKTQ+QmJlOJWBovIPQC+DiAB8Heq+mXv+XuG+7R6\ncDDYl0jLHFeSNHx+bP+vE4uuHc1NH08hZl9L7b48aTn3gKbafS3ntVl4I4rGde40zrMxNV6bN/fe\nfHg/iPXmQ53zWWuk1cPyXrm4iLW5WlcXpmfnF5EEwN8C+AiAcwCeE5EnVfVla0z14CDu+4c/DPbt\nLdbMc02UFoLtfYXGJizujn3Jktm3vzi36eNZiw8AVlp9mz5e+5ibd7rEeaNcdux4Mw2/WQPASqu8\nrXaMF8PXGbBvAIBv43y6J9i+ktqv2Xtday3bZVbTktlXd8Y1WokxJtzuceKPn+j6uVv52H8XgFdU\n9TVVrQP4HoB7t3A8QkiObMX5DwE4u+7vc1kbIeQ6YMc3/ETkmIhMicjU6qz90Z4Qki9bcf7zAA6v\n+/vGrO0qVPW4qk6q6uSekcoWTkcI2U624vzPAbhdRG4RkTKATwJ4cnvMIoTsND3v9qtqU0T+BMC/\noi31Paqqv/LGCBQFCe/2eru5lsRWdqQ373gNtXdRPclxu+kvrPU0LsH22uipBwNq2+jJZQ0NLy1P\nLvWUEU/VHXDmca0V3oFvFexzeevDM7HXNWex2LA/KVuqQ2ETQtCWdH5VfQrAU1s5BiFkd+Av/AiJ\nFDo/IZFC5yckUuj8hEQKnZ+QSNnSbv9mEdhySC8SW0XswJ6KE/STOlFgvUhRnvTm9VWcc1Uc2cjr\ns6j1IDXtBN581NQOjPFimTypz1zhTTsYyJPlSupcF2fNeUFoVrDTpWSvOWahGZYBN+NHvPMTEil0\nfkIihc5PSKTQ+QmJFDo/IZGS625/CsGykT7JCvgB7J3S4WTFHFMtrJp9VtAJ4Ae5zDTDu6+VQt0c\nM1ywbRx2xlWdCI2S8569qOHd3uUe8wV6ispwsmz2WTvYnpqy2Aqn3AJ8hcazsQZHQbDGGMFAgK8E\nDBXtaz1eXDT7LPu9dGJLhh9tJq8l7/yERAqdn5BIofMTEil0fkIihc5PSKTQ+QmJlFylPlVxq5pY\n9BuSWK3oSTK9vTRvnCXz9JqTuN9R3/qkt/flhhHXsejIRm5AjYMnsVUL4TTtXlCSNWYrWK/Nk3S9\nqjweXjUiPwgtvL6tdQ8AfYWwZCqbUHR55yckUuj8hEQKnZ+QSKHzExIpdH5CIoXOT0ikbEnqE5Ez\nABYBpACaqjrpPV8haLTCclnBkUmsCKaZZrVLS6+m7sh5+5Ils+/W8nSw3ZN4Sk5OtWQzusw6Vlq2\nXLZsvDZPzvOi6Ty8yElrToYNiQoAhgqeVGZfs/mWLYk1dCHYfhb7zDGWjNY+nh3V58mH3jhLKvYi\nIKtJWBbdTCm37dD5P6Sql7fhOISQHOHHfkIiZavOrwB+LCI/F5Fj22EQISQftvqx//2qel5EbgDw\ntIicVtVn1j8he1M4BgD9++1c6YSQfNnSnV9Vz2f/TwP4IYC7As85rqqTqjpZGe71V/CEkO2mZ+cX\nkQERqb71GMAfADi5XYYRQnaWrXzsnwDwQ2nLVUUA/6iq/+INaClQN6S+UsGWryyZZM1JtLiYOskg\nHUnmhqItlexPwmWhak5yTK9M1mLLlrZqTvRbL+frNcrRi0azSq95eLaPwZ6rotNXU2cee4jq8/Be\ns7ceF3qQU3u9Zt3S89FV9TUA795GWwghOUKpj5BIofMTEil0fkIihc5PSKTQ+QmJlHwTeELQbG3+\n/aY3Scl+aS2n7ptXE65hKEqLjsQz50g8c87r8iQ2D0va8iLE+gthCbOTHV40oyWlzTgSbAK71l1N\n7bk6XT9o9r1WHw+2T9fDdRcBP4GnV1PSi9zzWCuEr9mV5oA5xqrjl27ifs47PyGRQucnJFLo/IRE\nCp2fkEih8xMSKbnu9hdEUU7Cu7ZlJ2+atcNacwN77PBhr2TY+bVhs+8/C28Lti8bOQY7MVEO55cD\ngAPlObPvptKbZp+1c++VwvLLTG1eaQHsXIJzrX5zzJlGeGceAF5cOWz2/ezyzWbfpflwnsd63V76\nfX1OGbI9tjIyULZzCQ6V7XyH45Vw3sjEURYsUidwaiO88xMSKXR+QiKFzk9IpND5CYkUOj8hkULn\nJyRS8pX6oNiThGUUT9awpLnX10bNMbM1W1K6vGL3La3YEmF9MRxM4ZH02xLmwX3zZt/RfefMvr1V\nWzaySmgNOIE9npxXcpQjK9AJsHMJ1oyAFAD494Xbzb6fnr/F7Fs9bcuz/ZfCL6DPVuXgqL24PGy/\n6EtVex7LI7bUOjG8GGwfrdiBTpVi2I+8wLSN8M5PSKTQ+QmJFDo/IZFC5yckUuj8hEQKnZ+QSOko\n9YnIowA+BmBaVe/M2kYBfB/AEQBnANynqrNbMWShYUtsV9bC0tyr02PmmOYFW87re9N+zxuYsaWc\nfbPhvuYeWw9b2W9LW2dvs6d/b58tDd05cN7sqxiS3nhilyHrk95yz620nJJihnzo5bk7s2RLt8tn\nhsy+A1P2a9v7wnS44007ahIHb7DtuMW2Y+mgU5rtiJ2P742bjXH28sb+gXBEqFO57Lfo5s7/LQD3\nbGh7EMAJVb0dwInsb0LIdURH51fVZwBc2dB8L4DHssePAfj4NttFCNlhev3OP6GqF7LHF9Gu2EsI\nuY7Y8oafqipgp4IRkWMiMiUiU7U5+3ssISRfenX+SyJyAACy/41dFUBVj6vqpKpOVobtTT1CSL70\n6vxPArg/e3w/gB9tjzmEkLzoRur7LoAPAhgTkXMAvgTgywAeF5EHALwO4L5uTtbUAmYN2W6xYSfB\nnFkKyySenDd4xpHzLjqy15wtX0kzPK4+bE/j2pojozkRc/1FO+xsrxG5B9hlsi6mth1eAk+PknSf\nLPIthhM7Uq1csOdeE9vGZp9thw6EP20WlmwJtpXYxys0bTtKy07for0eV+bDUatzhu0AMFgOJxL1\nStFtpKPzq+qnjK4Pd30WQsg1B3/hR0ik0PkJiRQ6PyGRQucnJFLo/IRESq4JPButBJdWBoN9tbpd\nP29pYU+wvTxvv3f1zdmyS3nelpSae+xjLk+Ep6s+bEtDqxO2rLj/oB0I+a69duTeeNGu8TeThmvT\nna3vM8fMp04EZMGuW3ekfNnsO1gMvzYrwSgAvGPvJbPvvBNNN10YMfsWjoT7ygv2GCfHKJzykBD7\nUrvjktXwmltZtqW+mWJ4DTdbTOBJCOkAnZ+QSKHzExIpdH5CIoXOT0ik0PkJiZRcpT5VoN4MR5dZ\n7QCgaVhK04IT6RVWB9vnGrLPVRuxZbv5t4fP16racli5akfnjVRs2etyIyyJAsDLtRvNPiuq70Ld\nLkD3+oqdOHPJibYcLtv2/87gxWD7bX22nPe2ipkWAq2D9nWZ6rvJ7HtjLCwRzjtRfS4N+35ZXLDX\nlVMqEWKs72bdkZ1rYftT7T7Sknd+QiKFzk9IpND5CYkUOj8hkULnJyRSct3t7xUrVZwXLNEYcHY9\nnZR1tVEnH9xIeOe+v2qnJK+UbSVgYc0O3Dg1t9/sO1O0g3QKRnTJfN2WP16/ZB8vNfLLtU9md/1i\n/FCw/a5D/2OOeXf1rNl3U9/GujH/x5WqXQqrIOGLPdffWybppRV7XCO15zhZ7uE+27LXYpoax+Nu\nPyGkE3R+QiKFzk9IpND5CYkUOj8hkULnJyRSuinX9SiAjwGYVtU7s7aHAXwGwEz2tIdU9anOxwJK\nSViKahjtACBGn3qVsBw5L7FjbbBnxitdFQ5yWRu15bCVqlP+q+L0OS9ALZkHAJbDl7Ry0Z6sidO2\nHf0XwmWhACCt2MecfUc4oObffu+d5pjTN99g9nlBUBcW9pp9K0YATGPNXvre3Lfq9msuOnKekwoR\nLeOQ4pQo6+sLH9Cz/bds6uI53wJwT6D9a6p6NPvX0fEJIdcWHZ1fVZ8BYP/CghByXbKV7/yfF5EX\nReRREbHzIBNCrkl6df5vALgVwFEAFwB8xXqiiBwTkSkRmWrO2+WZCSH50pPzq+olVU1VtQXgmwDu\ncp57XFUnVXWyOGQXhyCE5EtPzi8iB9b9+QkAJ7fHHEJIXnQj9X0XwAcBjInIOQBfAvBBETmKdnzc\nGQCf7eZkAkU5CctK0udIW0ak0nK/nYctLTs6oENl1pYc9xjbnq2iHUnVrNhT3BiwJcKmHaiGphOQ\nZsmYFUfC9OS80vSi3Ve053ioZN1X7Nc8f37C7Jt1VmpiB1XCCqr0Mvg5wXmuvFywp9GNQE2Hwmuu\n3G9r0uODy8H21x3JfCMdnV9VPxVofqTrMxBCrkn4Cz9CIoXOT0ik0PkJiRQ6PyGRQucnJFJyTeCZ\niGKwHNZDKokd9tTqD79HvVyz9ZP6ov2DotKyLc1V5swuDPxmKdheOPOGOSadnTX7kvFxs6/59nAC\nTAC4coetRa0Z5cZq4/Zrnq7ac5XU7HMV7UA7FGthaXHojH2dKz+zNbuC8+tQadi1sDQJrx3tt8uQ\nrR6q2n1jnnRrdmH1BicZ50BYnjs4FF5vAHDb3plg+0nHjzbCOz8hkULnJyRS6PyERAqdn5BIofMT\nEil0fkIiJVepT0RNSW+kbOtGY31hyaOp9nvXK8mY2Te/15av6kP2lKyOhBNFDg/ZslFp1q65V5uw\ntaG5t9ky5tw7ncitvWHZy0qC2u5zkj46td/SmpPMciZs/8B5+3UNWUUZAVQSp4aiMw6FcF99r5N0\ndcJZA2P2ueoj9jzW99lyZP94OELv4OC8OWa8HPaJolGrMQTv/IRECp2fkEih8xMSKXR+QiKFzk9I\npOS72w9FsRDejRwo2gnQbiyHg2Oqo3YgyE0DdkDNxf124MYbR8JlpgDg8txgsH3uDjupXlKzlYD6\niF0mq3rQjjD60P6zZt9oObxz3HCSz40U7aCZxNk9Th215bnZm4Ptp8/b6sfKAXse+4y5BwDHDFOs\nSJ08iN51sdQUAKgM2mt4ot9eq2P94Wu2v7JgjqkaiQsTcLefENIBOj8hkULnJyRS6PyERAqdn5BI\nofMTEindlOs6DODbACbQLs91XFW/LiKjAL4P4AjaJbvuU1VbX+tA05Giaho2c6xol5K6pS+c4wwA\nkr22HDI3ZuezO1cfDbafv3XYHDNftzWl8Yqdo+1wxZ7KG0q2BFQWW4qy2F+0A0iqBTvgqqZ2cMyB\ncliqvK1qX5fnxw6bfbMrdjCWE9YDbYV7SwU7CGekYkt2IxV7PsoFWyIsJ/Z1qRoytyfB9hfCpbwK\n4gRpbXxuF89pAviiqt4B4G4AnxOROwA8COCEqt4O4ET2NyHkOqGj86vqBVV9Pnu8COAUgEMA7gXw\nWPa0xwB8fKeMJIRsP5v6zi8iRwC8B8CzACZU9ULWdRHtrwWEkOuErp1fRAYBPAHgC6p61ZdOVVW0\n9wNC446JyJSITK3NObWUCSG50pXzi0gJbcf/jqr+IGu+JCIHsv4DAKZDY1X1uKpOqupk37Dzg2pC\nSK50dH4REQCPADilql9d1/UkgPuzx/cD+NH2m0cI2Sm6iep7H4BPA3hJRF7I2h4C8GUAj4vIAwBe\nB3BfpwOJ2FJE6uSKW0nDkXHVgv01YjgJR0oBwHhiS4QVseWaxp7fBNvTIU9ssvEi7VLnfXmlZUcK\nWpQcCXB/YkuO3nx4WMf83b5z5pi7B181+y427WhLbx7nm2HpdqHpRBAW7Lny+tZaTikvx8aSMceV\nQvelt3qho/Or6k9hS6kf3l5zCCF5wV/4ERIpdH5CIoXOT0ik0PkJiRQ6PyGRkmsCzwIUA0k4Gilx\nopGWDKkvETsR52aim9bjyYD7k3D01VDBlnEqYk/xldSO2prpUTaqSFgeqhpRYO0xvcl5HuNGebBb\nirZUdmf5DeeIdt9Ky7b/rLF2zjb2mWMWW7YMWGvZkYzeuDVnXC9Yka4tN8bxanjnJyRS6PyERAqd\nn5BIofMTEil0fkIihc5PSKTkK/WJmjX5vGipQUNiGzTqlQF+IssknHcEgJ+U8qKhKDXUTvg46iSK\n7DVmy4tmtGQ7T85LHXmo4RTCqzmSIxCWFvsS246hgp2k08dOqjnQCq+DgYJ9zSqOLOqxmNr2e+vK\nkg896dCKdG050bEb4Z2fkEih8xMSKXR+QiKFzk9IpND5CYmU3AN7rF39oaK9Y2uV5ao6u/3ebm6/\n0+flxzubhoNBZnrMF+ipDh7ezr3V5yscXqk0u29Zy2ZfapTJSg0VAABaagc6eVxpOeXXWuEcft7u\nuxUcBfhrx1NhvHU1l4ZtdHf7W+G5b23ifs47PyGRQucnJFLo/IRECp2fkEih8xMSKXR+QiKlo9Qn\nIocBfBvtEtwK4Liqfl1EHgbwGQAz2VMfUtWnvGMp7PxzVskiABhOwhJQr+WMPNnFk4AsGkY+tU7H\nS2BLVN58wAm2sSS9xDneaMHL4ef12bKddWUajrrZa6CTF5hkzeOoU6LMw8/vZ0ufXt5Fa41YwTsA\nsJqGx2wmsKcbnb8J4Iuq+ryIVAH8XESezvq+pqp/0/XZCCHXDN3U6rsA4EL2eFFETgE4tNOGEUJ2\nlk195xeRIwDeA+DZrOnzIvKiiDwqIiPbbBshZAfp2vlFZBDAEwC+oKoLAL4B4FYAR9H+ZPAVY9wx\nEZkSkanVWfunkYSQfOnK+UWkhLbjf0dVfwAAqnpJVVNVbQH4JoC7QmNV9biqTqrq5J6RzdeVJ4Ts\nDB2dX0QEwCMATqnqV9e1H1j3tE8AOLn95hFCdopudvvfB+DTAF4SkReytocAfEpEjqKt4J0B8NlO\nB2pBTInCK4NkkTqSlyetpNLbzxs8ac7CkxWrBTuS0cOTtqwovJLYtieOOtTnzFUJ9hyvaTh684ra\ndqTaW5Sjd60t6XNAbJnSi1b0ZF1PBqw74+xzOfNrlHPbzAx2s9v/UyC42lxNnxBybcNf+BESKXR+\nQiKFzk9IpND5CYkUOj8hkZJrAs+WFrDcDEtfs0k4iSFgJzgsiC1seOW6PNxoum0m9d57e1O97Mg4\nR6WsOLKXN3BF7blaMez3S3z1Nvee9NkLnqTrJff01s52S33NVrhPWa6LENIJOj8hkULnJyRS6PyE\nRAqdn5BIofMTEik5S31A3ZAo6kaUEmAnOEy8KDDnbc2TaxIn+q1SCEtiXo02XzbqTY706EX2qjny\nUKPHSDtL0vOkPs/2hhPB6UfhedJiGC/pqpekc9mJ4FxzolZrhgy42LSjBGfre4LtTWeeNsI7PyGR\nQucnJFLo/IRECp2fkEih8xMSKXR+QiIlX6kPBaw0w1LJatGOLFtKw5LHYGJLbF7NMjeazmG4EK4Z\nWDUkQMBOIAn4STU9acuPjAvjyWhzjnzVK5bE5slovdY89CS2nqQ+V5az+1w5z+lbMmryXVytmmMu\nrw4G25stSn2EkA7Q+QmJFDo/IZFC5yckUuj8hERKx91+EakAeAZAX/b8f1LVL4nIKIDvAziCdrmu\n+1R11jtWMy3g0lJ4B3O1ae+GFowd89HysjlmMLErAntllcaLi2afFRDk7QB7ed28nWjvmItpOKgD\nABZa4b4VZ0ffs6PVa0k0Y5w3xrfDVius0lUA0DSOmTrHs/LjdTqXFbQGAPXUHmet/QuL9m5/bTV8\nPZtp9+pGN3f+NQC/r6rvRrsc9z0icjeABwGcUNXbAZzI/iaEXCd0dH5ts5T9Wcr+KYB7ATyWtT8G\n4OM7YiEhZEfo6ju/iCRZhd5pAE+r6rMAJlT1QvaUiwAmdshGQsgO0JXzq2qqqkcB3AjgLhG5c0O/\nwsg0LyLHRGRKRKaa8+FfyBFC8mdTu/2qOgfgJwDuAXBJRA4AQPb/tDHmuKpOqupkccguzEEIyZeO\nzi8i4yIynD3eA+AjAE4DeBLA/dnT7gfwo50ykhCy/XQT2HMAwGMikqD9ZvG4qv6ziPwHgMdF5AEA\nrwO4r9OBUhUs1zYfRPJmaWDTYxql3iQlL6+enavPDizx6DVYZd6R+uab4U9XC04+OEsOA4DV1AnE\ncaQtK5ecJ9n1iiej9WJH6gTHeHKeO86R4NYaYftXlu1rlq4a5brS7ue3o/Or6osA3hNofxPAh7s+\nEyHkmoK/8CMkUuj8hEQKnZ+QSKHzExIpdH5CIkW0x3JMPZ1MZAZtWRAAxgBczu3kNrTjamjH1Vxv\ndtysquPdHDBX57/qxCJTqjq5KyenHbSDdvBjPyGxQucnJFJ20/mP7+K510M7roZ2XM3/Wzt27Ts/\nIWR34cd+QiJlV5xfRO4Rkf8SkVdEZNdy/4nIGRF5SUReEJGpHM/7qIhMi8jJdW2jIvK0iPw6+39k\nl+x4WETOZ3Pygoh8NAc7DovIT0TkZRH5lYj8adae65w4duQ6JyJSEZGficgvMzv+Mmvf3vlQ1Vz/\nAUgAvArgVgBlAL8EcEfedmS2nAEwtgvn/QCA9wI4ua7trwE8mD1+EMBf7ZIdDwP4s5zn4wCA92aP\nqwD+G8Adec+JY0eucwJAAAxmj0sAngVw93bPx27c+e8C8IqqvqaqdQDfQzsZaDSo6jMArmxozj0h\nqmFH7qjqBVV9Pnu8COAUgEPIeU4cO3JF2+x40tzdcP5DAM6u+/scdmGCMxTAj0Xk5yJybJdseItr\nKSHq50XkxexrwY5//ViPiBxBO3/EriaJ3WAHkPOc5JE0N/YNv/drOzHpHwH4nIh8YLcNAvyEqDnw\nDbS/kh0FcAHAV/I6sYgMAngCwBdUdWF9X55zErAj9znRLSTN7ZbdcP7zAA6v+/vGrC13VPV89v80\ngB+i/ZVkt+gqIepOo6qXsoXXAvBN5DQnIlJC2+G+o6o/yJpzn5OQHbs1J9m5N500t1t2w/mfA3C7\niNwiImUAn0Q7GWiuiMiAiFTfegzgDwCc9EftKNdEQtS3FlfGJ5DDnIiIAHgEwClV/eq6rlznxLIj\n7znJLWluXjuYG3YzP4r2TuqrAP58l2y4FW2l4ZcAfpWnHQC+i/bHxwbaex4PANiHdtmzXwP4MYDR\nXbLj7wG8BODFbLEdyMGO96P9EfZFAC9k/z6a95w4duQ6JwDeBeAX2flOAviLrH1b54O/8CMkUmLf\n8CMkWuj8hEQKnZ+QSKHzExIpdH5CIoXOT0ik0PkJiRQ6PyGR8r+rSbNDyN8/mQAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f234cf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69034\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+IXIlP8H19lScdhCKH8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f7e263c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18206\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmaLgFyJTFPxCZIqCX4hMUfALkSn/ApI7hI4HnMV5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f8eb4da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57599\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f8f3d128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39324\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f8fb8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = X_train.shape[0]\n",
    "image_in_each_matrix = X_train.shape[1]\n",
    "print(image_in_each_matrix)\n",
    "\n",
    "for i in range(1,6):\n",
    "    \n",
    "    first = randint(0, total_images - 1)\n",
    "    second = randint(0, image_in_each_matrix)\n",
    "    print(first)\n",
    "    A = X_train[first][second]\n",
    "    figure(1)\n",
    "    imshow(A, interpolation='nearest')\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "905\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f9019908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14121\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5f70qAksAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f90a1e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5137\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhUgUiV+IRJH4hUiU/wGOVKo9XyW91QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f9109ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19900\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF/JJREFUeJzt3V2MXOV5B/D/M2e+9svGxhQccOPQ0guEGidaoUhBEW2U\niEaRgBsULiIuUJyLFBUpVYWo1NC7tGoS5aKK5BQUp6IkqEkUVKFGBEVCkSKSDeXDhKQBZAKu8doY\ne3c9O1/nPL2Yg7R2zvPMzLszZ9Z5/z/J8u5558x59+w8OzPnP+/7iqqCiOJTmXUHiGg2WPxEkWLx\nE0WKxU8UKRY/UaRY/ESRYvETRYrFTxQpFj9RpKrb2VlEbgPwdQAJgH9T1S97t9+3N9GDB2pjHydF\nNvY+oZ9bnPQnHkXEbnP2q7ittsz4yafxOc6Qc1X250lDzuI0+thznmf7WtymAb1fPdHF2tn+SDsG\nF7+IJAD+FcAnALwF4Bci8oSq/sra5+CBGn7+owNjH2sja4+9T0/H/4MBAO3A/SxNsX/pNaetIeP/\nkQSAjvYKt4eeD0/IuSru3fSEnMVp9PF0Wnfalgq3t3X83v/dHb8Z+bbbedl/M4BXVfV1Ve0C+A6A\n27dxf0RUou0U/7UA3tzy/Vv5NiK6DEz9gp+IHBaRFRFZOf1OOu3DEdGItlP8JwBsfQN/Xb7tIqp6\nRFWXVXX5qiuTbRyOiCZpO8X/CwA3iMgHRKQO4DMAnphMt4ho2oKv9qtqX0T+GsCPMIj6HlHVlyfW\nsyma9BX96Rwr7JpzyFX9Ms9HqLaGRZ+Qy3eymqaM/xiQMYLKbeX8qvokgCe3cx9ENBv8hB9RpFj8\nRJFi8RNFisVPFCkWP1GktnW1f1wKRU/H/5RfSHy1Hjywx46UmpOOjUocbOOFRsExWonaGvYBsfYO\nSfraapdaSKRnqYwR9fGZnyhSLH6iSLH4iSLF4ieKFIufKFKlXu3PoEHTTFlX7v2r1HZb6JXj9Wz8\nv5U1Cbui35TQuQ/Gv3Ifej4uZz1j3ryY8AwQRYrFTxQpFj9RpFj8RJFi8RNFisVPFKlSo75UFWez\nfmGbF9utZ8WrnXiDJUL1nPsMWUFlkoM2RlGT4vPbNLaXzR/gYvcxNDK1Ir3Qx473+Ji0kMdbNkbU\ny2d+okix+IkixeInihSLnyhSLH6iSLH4iSK1rdxCRI4DWAeQAuir6rJ3+wxiRnreyDIrlplG7OLF\nKyHRS9msqG8asWiZ8WHoKLxp/NyTZj2uQh5vOkbUN4kz8xeqemYC90NEJeLLfqJIbbf4FcCPReSX\nInJ4Eh0ionJs92X/Lap6QkT+CMBTIvJrVX1m6w3yPwqHAeB918Y3YwzRTrWtZ35VPZH/vwrgBwBu\nLrjNEVVdVtXlPXv5LoNopwiuRhFZEJGl974G8EkAxybVMSKaru287L8awA9E5L37+Q9V/W9vhxSC\n9Wz8+MKK9EJjuW6JE1Z6x6o7k3SGjgYsc9TZpJU9SrNMIY/VkMfpOKP6gs+Mqr4O4IOh+xPRbPFN\nOFGkWPxEkWLxE0WKxU8UKRY/UaTKXatPBetZs7DNi0LWjH3W0zlzH+s421ELWD/Pi+ysEXgA0K2E\nRYTWeSx7ItFJ885VmUJHdoZEz+2AWFzd9Ssvxmd+okix+IkixeInihSLnyhSLH6iSJV6tb+rVfyu\nt7ewzRuAcaa/VLy9t2jucyFtjNe5EdQrxVecvRRgMemYbburLbOtloUN+rGuinvpQai2THZOw9Bk\nZNJLrHlCB4yFXLmfNj7zE0WKxU8UKRY/UaRY/ESRYvETRYrFTxSpUqO+zayOl1oHCtt6Tkyy3ise\npHPe2A4ArX7dbOtlYbFXrVIcv1UrmbnPfLVrti0kdttSrW22efHh/vo5s23inKcOb/CRxY3l7FPs\nsgaFhdqJkV0oPvMTRYrFTxQpFj9RpFj8RJFi8RNFisVPFKmhUZ+IPALg0wBWVfWmfNteAN8FcBDA\ncQB3qeq7w+5rM63h5fP7C9u8+O3cZvFcfa2OHbtsbtij+jQN+5uXNItHllWrdqy1NG/Hco2qPVJt\nV8OO+nbV7TZrhGGzbo+Y8yI2L4L1Rh5ayp5LMGRUn9fH0FGCIT/3tOctHKUKvgXgtku2PQDgaVW9\nAcDT+fdEdBkZWvyq+gyAs5dsvh3A0fzrowDumHC/iGjKQt/zX62qJ/Ov38ZgxV4iuoxs+4KfqioA\ntdpF5LCIrIjISvf85nYPR0QTElr8p0RkPwDk/69aN1TVI6q6rKrL9d32IhtEVK7Q4n8CwD351/cA\n+OFkukNEZRkl6nsMwK0A9onIWwC+BODLAB4XkXsBvAHgrlEO1k0TvHnuisK2lhPNJW8Wj8yaW7WX\nJrrqrPlOBJLabWndvs/O3uKRgp099v2d2We/2kl22aP6zs/Zo9GuXLAn/rymuWa2Wbw472xvwWzb\ncEZOdrPih5Y1CSoA1MQeutdI7P0WnFGOFm9kpKdZCYsqz2TFk9CGHqthtPXHeD4fWvyqerfR9PGR\nj0JEOw4/4UcUKRY/UaRY/ESRYvETRYrFTxSpUifwzDKxI7137Khv12vF2694zR7d1jh+xu5Ix47Y\ndMmOtjrXFceUa++3I69Wyz7Fm9fYseLGor1fr2dHcy8nxaMmd9ftT1d6k52e3Zw32y50nElSjT4m\niR3nVSp2ZLrQsH9nTWd05HyteD9vZGRoHOnp6fjPsyHR5zgTjPKZnyhSLH6iSLH4iSLF4ieKFIuf\nKFIsfqJIlRr1QQWZEQFV23bsVWsVR0C1c3Zck548ZXejY4/oqpy3o61m35qw8kpzH8COw8SZtLS3\naP9d7jkx4KtrRmTq/Znv2edeOvaOFa/NGJDmzZ2qNTvqu9C027RhTySazI8/6WqtZrd56zJ6+pn9\ngzdrxX30Jni1+tHuM+ojoiFY/ESRYvETRYrFTxQpFj9RpMq92g9AjIEdzjRyyGrFV6PTOfvKZnXX\nLvv+1ux57qTqnJKsuO/Jpn1VtrFu319Ws//2Jk76kW7YbVqx0wWLNy2d15a0nXkSjYvi3viWrG43\npva4L6QN+8HTny9+jHgXxXtO6qDO4KNQG/Xi+5Q5+3ElRj+6faeQLsFnfqJIsfiJIsXiJ4oUi58o\nUix+okix+IkiNcpyXY8A+DSAVVW9Kd/2EIDPATid3+xBVX1y6H1VFNVGcXzRW3LmuruquK3atpfC\nmmscMNuSlp1f2UM6gNSKjRZGj1e2ql2wYyMvYtN1u61qxG/e1HNJz+uH09afbOyVVZ14s2E/T/Wb\n9n59Y9UzL3LUZPLPiam9+poZY6YNLyY2fs/d0fs+yi2/BeC2gu1fU9VD+b+hhU9EO8vQ4lfVZwCc\nLaEvRFSi7by+uU9EXhSRR0Rkz8R6RESlCC3+bwC4HsAhACcBfMW6oYgcFpEVEVlJ1y4EHo6IJi2o\n+FX1lKqmqpoB+CaAm53bHlHVZVVdTnbZC2IQUbmCil9Eti4LcyeAY5PpDhGVZZSo7zEAtwLYJyJv\nAfgSgFtF5BAABXAcwOdHOVhSybB7sXjevfPOfq33Fc+r15+z/3ZduMbOVipde4iYF4lZ8ZAmzhx4\naVgc5kV9IStGVbwM8zIgWVgs6kzH5xh/tOIwXhxprbDVW/L2KW6TMX7eocWvqncXbH549EMQ0U7E\nT/gRRYrFTxQpFj9RpFj8RJFi8RNFqtQJPBvVPv5kz5nCtnfm7A8AvW2MBOx0nJGALWciy8C4BqkR\nr/Ttv6HiLIWVtJwlypxJOr1oC7D3M/dwzocbOQbEaF706UWm3gSvIdzJR52fK3HP/WR5/RDrITdG\nssxnfqJIsfiJIsXiJ4oUi58oUix+okix+IkiVWrUV5MU1zSL18mrV+x1yWrGkLS1jj1y78KcHfX1\ns8n+zev1nLXinLXTem379Pc37f0qzjp+ViTmrTEnmTcq0WwCnP2C7m+cnGpE1sPKWwvReSgiKR6U\nui3WqD5vfUJrn3EiUT7zE0WKxU8UKRY/UaRY/ESRYvETRarUq/2JZNhTbRW2dTO7K8VDgfyr9l3v\nKrtzdb7iXBVPkuIRMDVnori5hjMSxJnM2Ot/mto/t9VHj3d/npBjeefekzm/azUGXAFA3zqes6yV\ndJ0koOPs5yQZ3lV4NZbeyprO+a0Wt1n3VYTP/ESRYvETRYrFTxQpFj9RpFj8RJFi8RNFapTlug4A\n+DaAqzEYeXFEVb8uInsBfBfAQQyW7LpLVd+dRidbveJBOuc25sx9OhvOqAhnXj2XEaMkTXskSMOJ\n+upVOxua9yJCR1IJiPqcGM27v2pg26R5kW+nX/wQb/fsh74XR/aceSMzZ6BTpWafj5rxOPAiZOv8\nnnaO83t9GuE2fQBfVNUbAXwEwBdE5EYADwB4WlVvAPB0/j0RXSaGFr+qnlTV5/Kv1wG8AuBaALcD\nOJrf7CiAO6bVSSKavLHe84vIQQAfAvAsgKtV9WTe9DYGbwuI6DIxcvGLyCKA7wG4X1UvmpFDVRXG\nTAwiclhEVkRkpfVud1udJaLJGan4RaSGQeE/qqrfzzefEpH9eft+AKtF+6rqEVVdVtXl+T3OQhpE\nVKqhxS8iAuBhAK+o6le3ND0B4J7863sA/HDy3SOiaRllVN9HAXwWwEsi8ny+7UEAXwbwuIjcC+AN\nAHcNu6NMK9gwJibb6NuvCk6vFw9/65y1o77aWTuuSTphUV/aKI760nn7NLYW7bbOfFhE6I08tLix\nXMDovGFqiRFfGfMxAn4fvTkeQ7Scx5sVLQNA24gOh2lWnTkqjXM1Vx0/7n0jGf08Df1JVPWnsBeA\n+/jIRyKiHYWf8COKFIufKFIsfqJIsfiJIsXiJ4pUqRN49rSCU52lwrbfre8192u9Wxzp1c/YcV7z\njB3nJe2wZaHSZvF99uftY/VaxrpKGBIRNp0PRBmTNwJAbdH4FGUjLCrre5N71uz73OwV/9y9iv07\nsyIvAO4j1YsBrfhwV91ed2u+OvlPonoT1FqjEkPizXFCbD7zE0WKxU8UKRY/UaRY/ESRYvETRYrF\nTxSpcqO+LMHJ1u7CNmvkHgAk7xbHRl6cN3/KicNadlulb8eA/bniv5W9eWcCySucdeSciDCr2vfZ\nn7f7GDLt56YzAaa7nzMBqcVb388brdh0YsWGM2JurlZ8RsoeXbjWaZpt1kjBtYq9j9VHbzLTS/GZ\nnyhSLH6iSLH4iSLF4ieKFIufKFKlXu1PswrObRYP0mmdt+fjW3in+Kr44v/ZV2UXj18w25JzLbNN\n+vZV4GzXfOH23hX2VdmaM7Cnvdu5or9gJwHiLAulleLjpRv2r1rGv2gPAHBWtbL3CTsULhhLpQGA\nNuwfQIzlq7zls6pOiuEtoeXxlgDr94vbvH5YqQmv9hPRUCx+okix+IkixeInihSLnyhSLH6iSA2N\n+kTkAIBvY7AEtwI4oqpfF5GHAHwOwOn8pg+q6pPefWUqaPeMQ/bs+KpqJHONc87cbavn7X6snrHb\nUjteqazvKtxe7+4x99FK8ZyFgzY7BlQnRtOKMz+hsRSZt48ErtYVEhGGHiuzTxWymn2yjNXhkDnR\nYepURd+LHAOiT0/XPVZxW9Yb/fl8lJy/D+CLqvqciCwB+KWIPJW3fU1V/2XkoxHRjjHKWn0nAZzM\nv14XkVcAXDvtjhHRdI31nl9EDgL4EIBn8033iciLIvKIiNivfYloxxm5+EVkEcD3ANyvqmsAvgHg\negCHMHhl8BVjv8MisiIiK+ma/bFaIirXSMUvIjUMCv9RVf0+AKjqKVVNVTUD8E0ANxftq6pHVHVZ\nVZcT47PxRFS+ocUvIgLgYQCvqOpXt2zfv+VmdwI4NvnuEdG0jHK1/6MAPgvgJRF5Pt/2IIC7ReQQ\nBvHfcQCfH3ZHSSXD7rniZZI2l+zlqTp7inOezSvt7tevtCM2L5HRC5tmmywUjzxM5+wcKquHfZSi\n4gx/q7bsCMjaz5myDpLa9+dFc0nP60dxW9IJXCqt4cx3WPNGORa3ZW6U6rQl9rFSN4502urF99lb\nHP/+xInMLzXK1f6fongJMDfTJ6KdjZ/wI4oUi58oUix+okix+IkixeInilSpE3jWkxTXLZ0rbPMm\nHjy1vzgGXLNGCAIA7Khv7h17stCkZY8UzOrF+VBvyc6Nuov2z5UaEc8wbgzYNiI2J5bzRuclXTvr\nSza9tuLzWOna51cT+1xZ5x4A1FnazIr6JAuLHL3RkV6s60WVaaN4v7az1FvaLG7zIt3fu+3oNyWi\nPyQsfqJIsfiJIsXiJ4oUi58oUix+okiVGvU1Kn386cLpwrZ6xY6AUiMGPFMtnlATAPrz9jCq2oY9\ngjBp220Wb8SWxx0xVzz4MW+zY6q+EQF5I9W8fnjxm0erxSdFK/b59SbATJ0YzRvVZ/GiPi/69I7l\njepzz7Hxo3m/s0ngMz9RpFj8RJFi8RNFisVPFCkWP1GkWPxEkSo16mtKD3/WPFnYtuRlW4bXq3Y8\n+Pa8HQN2W95ibPbfQ0nHj5QqbXufirGuHgDUNpy1CwOirYozsaM3gae3X9UZqWZFW1YUCQxbjy9s\nBKQldNLS/nxY/631Jj1p026zjjVOPMhnfqJIsfiJIsXiJ4oUi58oUix+okgNvdovIk0AzwBo5Lf/\nT1X9kojsBfBdAAcxWK7rLlV917uvuvTxx7WzhW1NZ2K6deOyZzcLCysuLNiDS7p9bzGv8fV69v31\nWnY/+hv2fknLSSSMK9XeVXu3zQ5UkDhJhsW7Ip42wvbzrs5bg3QkC0sP+vN2SuA9HJPF8ZORtGEf\ny7qq7y1DdqlRnvk7AP5SVT+IwXLct4nIRwA8AOBpVb0BwNP590R0mRha/DqwkX9by/8pgNsBHM23\nHwVwx1R6SERTMdJ7fhFJ8hV6VwE8parPArhaVd/7xM7bAK6eUh+JaApGKn5VTVX1EIDrANwsIjdd\n0q4YvBr4PSJyWERWRGTl3NkxJhUnoqka62q/qp4D8BMAtwE4JSL7ASD/f9XY54iqLqvq8hV7J3sx\njYjCDS1+EblKRK7Iv54D8AkAvwbwBIB78pvdA+CH0+okEU3eKFnZfgBHRSTB4I/F46r6XyLyMwCP\ni8i9AN4AcNewO0okw95k/BEO+2rrhdvP1hfMfVpzdoxWrdjZULtvnxJvP0vHub/1ih3ldGDnXqn3\nAsp4Z+XGec4AI2+/kDnm/KjPidG8qM95N2nFaF486EnnvD460ZwzF6LV/6zpRX1G2xi/k6HFr6ov\nAvhQwfZ3AHx89EMR0U7CT/gRRYrFTxQpFj9RpFj8RJFi8RNFSgYfzivpYCKnMYgFAWAfgDOlHdzG\nflyM/bjY5daP96vqVaPcYanFf9GBRVZUdXkmB2c/2A/2gy/7iWLF4ieK1CyL/8gMj70V+3Ex9uNi\nf7D9mNl7fiKaLb7sJ4rUTIpfRG4Tkd+IyKsiMrO5/0TkuIi8JCLPi8hKicd9RERWReTYlm17ReQp\nEflt/v+eGfXjIRE5kZ+T50XkUyX044CI/EREfiUiL4vI3+TbSz0nTj9KPSci0hSRn4vIC3k//jHf\nPtnzoaql/gOQAHgNwPUA6gBeAHBj2f3I+3IcwL4ZHPdjAD4M4NiWbf8M4IH86wcA/NOM+vEQgL8t\n+XzsB/Dh/OslAP8L4Mayz4nTj1LPCQABsJh/XQPwLICPTPp8zOKZ/2YAr6rq66raBfAdDCYDjYaq\nPgPg0jnMS58Q1ehH6VT1pKo+l3+9DuAVANei5HPi9KNUOjD1SXNnUfzXAnhzy/dvYQYnOKcAfiwi\nvxSRwzPqw3t20oSo94nIi/nbgqm//dhKRA5iMH/ETCeJvaQfQMnnpIxJc2O/4HeLDiYm/SsAXxCR\nj826Q4A/IWoJvoHBW7JDAE4C+EpZBxaRRQDfA3C/qq5tbSvznBT0o/RzotuYNHdUsyj+EwAObPn+\nunxb6VT1RP7/KoAfYPCWZFZGmhB12lT1VP7AywB8EyWdExGpYVBwj6rq9/PNpZ+Ton7M6pzkxx57\n0txRzaL4fwHgBhH5gIjUAXwGg8lASyUiCyKy9N7XAD4J4Ji/11TtiAlR33tw5e5ECedERATAwwBe\nUdWvbmkq9ZxY/Sj7nJQ2aW5ZVzAvuZr5KQyupL4G4O9n1IfrMUgaXgDwcpn9APAYBi8fexhc87gX\nwJUYLHv2WwA/BrB3Rv34dwAvAXgxf7DtL6Eft2DwEvZFAM/n/z5V9jlx+lHqOQHw5wD+Jz/eMQD/\nkG+f6PngJ/yIIhX7BT+iaLH4iSLF4ieKFIufKFIsfqJIsfiJIsXiJ4oUi58oUv8P8TGTw54SjOUA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f8ffefd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15834\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGMVJREFUeJzt3W2IXNd5B/D/c++87a60u1rZkhVZqWIwLcY0SlhMSkJw\nGxLcELD9xcQfgj+YKB/S0ED6wbjQuN/SUifNhxBQahOluIlNnRBTTItjAiahuF67flEiOy+OXEuR\ntHrbt9mdnZf79MNcw0rc55mZuzN3Vjn/HwjN3jP33jNn5pk7c54554iqgojCE427AkQ0Hgx+okAx\n+IkCxeAnChSDnyhQDH6iQDH4iQLF4CcKFIOfKFCl7ewsIncB+CaAGMC/qOrXvPvPzMW6/2A5sywS\n+5eGZSQD1y2B5CrrqF3W1OzmWuvUzH022tmPFwDardgsk5ZdD+mYRXCa0eQ8ZDhNBTjnEuMps7b3\nKvPO5dZx2HLWI3EiLTFeIupdmo1ztS9fRmet3leL5A5+EYkBfAvAJwGcBvCSiDyjqr+09tl/sIxv\nPXM4s6wmLfNcN8XrA9evoXZgeWVLyYRZ9m5rb+b2ny/fau7z6oWDZtnFc9NmWeWc/aZRvWI/t5Hd\njCbrxQcATlO5b0JxI3t7ec15k9+wy6KWXeYGibVPnO8dI2896jfZDbmxL3t7e8p+N7Sel98/+s92\nJa6xnY/9dwD4jaq+rapNAD8AcPc2jkdEBdpO8B8E8O6Wv0+n24joOjDyDj8ROSoiCyKysHzZ+ZxI\nRIXaTvCfAXBoy983p9uuoqrHVHVeVedn5pwvkERUqO0E/0sAbhWRD4hIBcBnATwznGoR0ajl7u1X\n1baI/BWA/0I31fe4qv5iaDXrQ9XtsM33FWM22jDLVo2sw821K+Y+707O2sebrpplzXq+p0Y62Y2i\nsd1L7fX2u+dyUnPxRnY9OjX7SWs17LK4adffyzpYvCyGJ2radfSO2bQTO2avfsfp7UdktIe1PcO2\n8vyq+iyAZ7dzDCIaD/7CjyhQDH6iQDH4iQLF4CcKFIOfKFDb6u0fVCRqDuCpSXvg49XETrvMRt7A\nHjs35JUBl7M3T9p7bDrDuXaVN82yt8rGaA8AG+t2ijCKstNDUWynjWrlfGnRJLHbf6OeXcfGhjOS\nsWGXxU4a0BvMlCf1ae3TLbPPlVTsY7ZmnR2nsx/A5JT9+oiMlF5U6n8ELK/8RIFi8BMFisFPFCgG\nP1GgGPxEgSq0t3+nqIk3qsPula0ZXb1lJ1Oxr7Jilq3U7CnD1mbsHv3lmj1n4GQ5u+e4HNmPqxIP\nf56F+kwlc/t6yx5F1GjaZRub3lyI9ss4cXruh63kZE2manZKolbOfv3snayb+1jP2ZlS/1kzXvmJ\nAsXgJwoUg58oUAx+okAx+IkCxeAnClShqT5BvgE85go7iTNAJ8d5AGDTmQJtKclOv7WMZbx6qToj\nUuaq9ipFXmpuVyl7MEg1ttuj5K6TZatGg7ex95hX2nbqs97OTh0CwFrbTosOW8V5zBUnneq1v2Uq\nbtrHM9rxpQGeE175iQLF4CcKFIOfKFAMfqJAMfiJAsXgJwrUtlJ9InIKwCq6Q+Haqjo/jEoNgze/\nn8/O9VlpSm9UnzVnIQBMlxp2NeyBe9h01teyUkBeWm6mZC9R5vHSduZcjc4+DedxLXfsiRIXm7vN\nMqutvLkVp5328NrRK/POl+d4VjuWB1i7bBh5/j9X1YtDOA4RFYgf+4kCtd3gVwA/EZGXReToMCpE\nRMXY7sf+j6nqGRHZB+A5EXlTVV/Yeof0TeEoANx0MOe6yEQ0dNu68qvqmfT/RQA/AnBHxn2Oqeq8\nqs7PzjH4iXaK3MEvIlMisvu92wA+BeDEsCpGRKO1nY/9+wH8SLoptRKAf1PV/xxKra5hjupzzMBZ\nnirnBJ6zUfYoq6XESXl56TBvnSn3mbFThNYx86TlepmN7ZGH7mMzNCRfqs9jpdjaal/3vNGFXhqw\nSFZaNEH/Ke7cwa+qbwP4YN79iWi8mOojChSDnyhQDH6iQDH4iQLF4CcKVKETeLY1wrnO9MD7mSPE\nnBRVTew0VN7JPa2U42pip4aWnBSVN4otzyiwvDbhrJ+Xc6RdnuOttO2hjO+sz5llZ9ZmzLIrq9l1\nbDbsenii2B71Wa3ZE24e2rNkl03ZZYNqDvC64ZWfKFAMfqJAMfiJAsXgJwoUg58oUIX29rc0xoX2\nEHv7cwwe6R4vX2+/lak41bzB3Ofdht1LnXd5Kk+eZaFWW/ZyV17vcbMz+ICrVmLvc7luZw9W1+y2\nSi7a9a9eyr6+7aqbu8BbvcybIq+5267jr/7YrmPrfcMb6j5IlohXfqJAMfiJAsXgJwoUg58oUAx+\nokAx+IkCVWiqr4PIHegyKG9gj7eElqeldpO8tv7+zO1vre039zm9NmuWrTacFFvbTv9sNuw0YGct\nu/7SyJlOcgayeNTaL7KPF63bdSyv2Nep2iW7HhMXsvN2lTU7nycdu45eWWPOSYvusZ+zc1PZy41V\nSv0vvfWeTtL/9ZxXfqJAMfiJAsXgJwoUg58oUAx+okAx+IkC1TPVJyKPA/gMgEVVvT3dNgfgSQCH\nAZwCcJ+qXhlVJTeted+ct67VTr559c637Png3lh5X+b2ty7uM/dZu2SfS+p287sredXtB757MXt7\nddlObSWxvcSTOmWdHAMPOxP28YzV0AAAcdOZO2/ZLqssZ6d8o3a+dJ6n1LD3K9XtNOb6lezX6uZE\njlRfp//luvq58n8XwF3XbHsIwPOqeiuA59O/ieg60jP4VfUFAJev2Xw3gOPp7eMA7hlyvYhoxPJ+\n59+vqmfT2+fQXbGXiK4j2+7wU1UFYH7ZEZGjIrIgIgv1y/lm3iGi4csb/OdF5AAApP8b3UyAqh5T\n1XlVnZ+ay7dQAhENX97gfwbAA+ntBwD8eDjVIaKi9JPq+z6AOwHcICKnAXwVwNcAPCUiDwJ4B8B9\n/Zwsgroj8Sy1ePB9vHTerzfsLgpvWaiTi9n7bZzOHpUFADVjAkkAqKyaRW7aq7Thpb2yU3rlNTtt\nVKrbIyDjut32UXPwkZPJpJ0fbE3bZY299qdGL+W4+v7s/Zx5RHNLKv2n2baKVrPDMHFSwaZ2/9fz\nnkdX1fuNok/0fRYi2nH4Cz+iQDH4iQLF4CcKFIOfKFAMfqJAFTqBp8dbd89KDzbUTv94qT5vjbzF\ndTttt75Sy9xeWbVTPNUlswgVZzRa5Iwsi500YB5eOi++YD+A9u/P2QdNslOL0dSUuUv15gNmWXN6\nr1m2fqOdt9swBly2pp1RjjVnsT5nAlIkzojFhn2djZr5UoTZdej/rrzyEwWKwU8UKAY/UaAY/ESB\nYvATBYrBTxSoHZPq85hpQCetMRuvm2XTpQ17v5pddq6WPYqtU7NTjq0pL42TbzJLT1LJfj+Pms6k\nn8Y+ADBRssviRsOux+pa5nYpOy+5kp2yU+cy1cnOwAIAmnuzU44TB7LrBwCH9tjpzUNTTurTqeTP\nf3eLWdY5Z6eeBzbA5ZxXfqJAMfiJAsXgJwoUg58oUAx+okBdF739Fm8wkJcJ2OdMnne56gw8qWWf\nb22qau7TdJZPSspOb38r32APq+fbGzwSN+1e9qhpP7aJlT32MUvGS8vaDqDjzO/XcebHazu9/TqV\nnaHxevT/bO/vzLL5qbfNMm+JuBMX7UFLlxZz9PbHxgCjAV42vPITBYrBTxQoBj9RoBj8RIFi8BMF\nisFPFKh+lut6HMBnACyq6u3ptkcAfB7AhfRuD6vqs6OqZFkGXxbKe1ubcQb9zJXrZtmeyexBP/Up\nO9fUcVJ9XiVlgLnY+uGl+rzlrtqTdhowmbZTVHmuKp2qfS4v1afO0ltipcQcbgrZ4c0p6cpRx2Ho\n5zn6LoC7MrZ/Q1WPpP9GFvhENBo9g19VXwBwuYC6EFGBtvOd/0si8rqIPC4i9k+9iGhHyhv83wZw\nC4AjAM4CeNS6o4gcFZEFEVlYu5LvuxQRDV+u4FfV86raUdUEwHcA3OHc95iqzqvq/K49OTtEiGjo\ncgW/iGwdpXAvgBPDqQ4RFaWfVN/3AdwJ4AYROQ3gqwDuFJEjABTAKQBf6OdkCcROhziprZZmVzNX\nChB+Kqca2ccsx9nzwZXK2dsBoFV20lc1O33lpfq8tJ310Nzj2dV3Jc78flLNfs7EWYZMneN5xKm/\nbmS3/6V1e/Tmm/WbzLJGYn96XXGGF65vOp963XTwgAbIGvYMflW9P2PzYwNUh4h2IP7CjyhQDH6i\nQDH4iQLF4CcKFIOfKFCFTuCpKti0UiXO25CVXmnATp94I6y8dM1mYjdJq5OdNkoSp/JJvnSel77y\n9os3ss/nDFZ0lwaL2nbuKGrbFfFSennETft4pYbdxqWl7OfzYmW3uc9Cy34NvFnbZ5atNezJTtev\n2CMg42H+8HWAZueVnyhQDH6iQDH4iQLF4CcKFIOfKFAMfqJAFZrq6yDCcjs75VGN7PRbTbJzIbPO\nRJye5c6kWWamIgG0jJRe4ozKErfMLMq9n5U2ipxUmZdGi5pOXtFJ9aFtVLJjV75Ut2cSraw5KdgV\nbx2/7DKN7ed5rWlfE+sT9sg99Z6zul1/a11G73k2zzPAxK+88hMFisFPFCgGP1GgGPxEgWLwEwWq\n0N7+RMUdOGOxBunkHbzjldWdtasazez91Okdtnpye5V5Pb3eHH55eHP4eQN7xOm5l01jtFBj0z6X\nXQ1U3KW87AE1nYq1n322ltNr33YGaiFysibO4B1rMFae3n4O7CGinhj8RIFi8BMFisFPFCgGP1Gg\nGPxEgepnua5DAL4HYD+6iYRjqvpNEZkD8CSAw+gu2XWfql4ZXVXpPWpnvZAYZUnFTlF1KnZ+qFO1\nrw9atVOmWsquiJTyZZfjTWdA0IY9mqW8ll1/jb10qZeetRvfe16GnZ4dhn6u/G0AX1HV2wB8BMAX\nReQ2AA8BeF5VbwXwfPo3EV0nega/qp5V1VfS26sATgI4COBuAMfTux0HcM+oKklEwzfQd34ROQzg\nQwBeBLBfVc+mRefQ/VpARNeJvoNfRHYBeBrAl1V1ZWuZqiqMHxaKyFERWRCRhY0r9k87iahYfQW/\niJTRDfwnVPWH6ebzInIgLT8AYDFrX1U9pqrzqjo/scf+DTYRFatn8IuIAHgMwElV/fqWomcAPJDe\nfgDAj4dfPSIalX7yLh8F8DkAb4jIq+m2hwF8DcBTIvIggHcA3NfrQJEoqlE7s8zaDthz+Fnbuyez\ni2ZK9tx/U7G9jNPuWvbXltUJeymmTsuuiHScssgZ8Rd7Q7cGTym59XDnGbQ/yZVL2ceMGvbz7Ok4\no/q8pcGs+QlLG/a5vPkTddUpc1J9bXvqP3SMMu94pgGe/p7Br6o/cw75if5PRUQ7CX/hRxQoBj9R\noBj8RIFi8BMFisFPFKhCJ/CMkWDGyLFUnRkOrWW5dsd2vmY37DIvRbhcsZfy2je5mrn9yqSd6qs7\naaNO5A3Py7cEGIyyuGHvk5SdOtbs68PmjD2qr7SRXRY5aTkvrZgr7QWgPZH92KzRj9162HUsr9n7\neXVsTTqTgk4NsMZWynoN6ACpPl75iQLF4CcKFIOfKFAMfqJAMfiJAsXgJwpUoam+knSwv7ycWeal\n36yU3k3xSub27vHsvFHD2c+zYgzNWpuxR7edj+w0TnPKWX/OGWmXOGVtY4LJ9oZ9rva0nR/adNch\nNIty8VKYXqrSH3loHM9bO69hl6kxShDokeqbdiZJ3WtUxml7MxXsjvi8Gq/8RIFi8BMFisFPFCgG\nP1GgGPxEgSq4tz8xB+mUxZ7bzerV3x83zX1mI/uhNdTrpj5vlixNZA/6udyaco5nW29Vcu3XSuz3\n7LVGduYhmcq3XFTerEMUZ2c5YmN7r3O1W/bz2fF6xY0MgjTsrvm4bh+vVM/Xju299mt1cjo7vdBq\n2o85sV4D7O0nol4Y/ESBYvATBYrBTxQoBj9RoBj8RIHqmeoTkUMAvofuEtwK4JiqflNEHgHweQAX\n0rs+rKrPuseCmik9b2DPbJSdJvHSebsie32kqtojQfY78wIeqlzK3L48Zc/7N+WkIzeT4Wdazzey\nlxtrduzUViV2RsbkVDGWX6tE+c7VdCbdazrtaD3u5U379bGybpdt1PMtNjs7Yy8RZy0D56V028bj\nOu+kUq/Vz6uvDeArqvqKiOwG8LKIPJeWfUNV/6nvsxHRjtHPWn1nAZxNb6+KyEkAB0ddMSIarYG+\n84vIYQAfAvBiuulLIvK6iDwuInuGXDciGqG+g19EdgF4GsCXVXUFwLcB3ALgCLqfDB419jsqIgsi\nsrB8efjfLYkon76CX0TK6Ab+E6r6QwBQ1fOq2lHVBMB3ANyRta+qHlPVeVWdn5nLufICEQ1dz+AX\nEQHwGICTqvr1LdsPbLnbvQBODL96RDQq/fT2fxTA5wC8ISKvptseBnC/iBxBN/13CsAXeh1IoGZK\nz0v1WZYSeyQg4EzE5vDSh39Svpi53av77RPvmmUtzZfqayT2MllvNQ5kbrfmHwSA6ZLdVjVnoj5v\niTXzec458Z/3mBtql20a+y237SXWLjSz06UA0FZn+bKOk3L01gcz5EmL/q7kxcTV+unt/xmArHGM\nbk6fiHY2/sKPKFAMfqJAMfiJAsXgJwoUg58oUIVO4BmJnerzNMx1kOxUiJcGrIk9CWNN7JTMTJS9\n3wcr+Zb/yms5sSdptFJpSx175KE1qSoA7I7sUY7ec1kzR2/m+5Wn/RoAGk7KdCnJftxemtVLK3q8\nNv6/zb1mmZVavLGyau5TNUZNvjBAKpVXfqJAMfiJAsXgJwoUg58oUAx+okAx+IkCVWiqj0bPSs15\nayH6KTtvYtXsiSe7+2Wn9Kr5lrpDzZl0teGU2fvY6Twvvent5zkfzZhl06Xs81npPMBO6UbgWn1E\n1AODnyhQDH6iQDH4iQLF4CcKFIOfKFBM9Q1BQ/tPr4xanlGTo5A3pZfveHaqzxpd6MmbzvPMlOyR\nk3n2sZ7nWPpfq49XfqJAMfiJAsXgJwoUg58oUAx+okD17O0XkRqAFwBU0/v/u6p+VUTmADwJ4DC6\ny3Xdp6pXRlfV69NmzkSAN2edv192T7XXgz0b2b3K3uCdGWc5KW+exDy8jIqXCZiNmpnbl5xO8by9\n/d5SZN48iVaZO+DKOFd5gDkS+7nybwL4C1X9ILrLcd8lIh8B8BCA51X1VgDPp38T0XWiZ/Br11r6\nZzn9pwDuBnA83X4cwD0jqSERjURf3/lFJE5X6F0E8Jyqvghgv6qeTe9yDsD+EdWRiEagr+BX1Y6q\nHgFwM4A7ROT2a8oVyJ5FQESOisiCiCwsXer/10dENFoD9far6hKAnwK4C8B5ETkAAOn/i8Y+x1R1\nXlXnZ/cyuUC0U/SMRhG5UURm09sTAD4J4E0AzwB4IL3bAwB+PKpKEtHw9TOw5wCA4yISo/tm8ZSq\n/oeI/DeAp0TkQQDvALhvhPX8g+QvQZUv1Uc7U54BV17q0JqTUQaYw69n8Kvq6wA+lLH9EoBP9H0m\nItpR+CWcKFAMfqJAMfiJAsXgJwoUg58oUKIFzj8nIhfQTQsCwA0ALhZ2chvrcTXW42rXWz3+SFVv\n7OeAhQb/VScWWVDV+bGcnPVgPVgPfuwnChWDnyhQ4wz+Y2M891asx9VYj6v9wdZjbN/5iWi8+LGf\nKFBjCX4RuUtE3hKR34jI2Ob+E5FTIvKGiLwqIgsFnvdxEVkUkRNbts2JyHMi8uv0/z1jqscjInIm\nbZNXReTTBdTjkIj8VER+KSK/EJG/TrcX2iZOPQptExGpicj/iMhraT3+Pt0+3PZQ1UL/AYgB/BbA\nLQAqAF4DcFvR9UjrcgrADWM478cBfBjAiS3b/hHAQ+nthwD8w5jq8QiAvym4PQ4A+HB6ezeAXwG4\nreg2cepRaJsAEAC70ttlAC8C+Miw22McV/47APxGVd9W1SaAH6A7GWgwVPUFAJev2Vz4hKhGPQqn\nqmdV9ZX09iqAkwAOouA2cepRKO0a+aS54wj+gwDe3fL3aYyhgVMK4Cci8rKIHB1THd6zkyZE/ZKI\nvJ5+LRj514+tROQwuvNHjHWS2GvqARTcJkVMmht6h9/HtDsx6V8C+KKIfHzcFQL8CVEL8G10v5Id\nAXAWwKNFnVhEdgF4GsCXVXVla1mRbZJRj8LbRLcxaW6/xhH8ZwAc2vL3zem2wqnqmfT/RQA/Qvcr\nybj0NSHqqKnq+fSFlwD4DgpqExEpoxtwT6jqD9PNhbdJVj3G1SbpuQeeNLdf4wj+lwDcKiIfEJEK\ngM+iOxlooURkSkR2v3cbwKcAnPD3GqkdMSHqey+u1L0ooE1ERAA8BuCkqn59S1GhbWLVo+g2KWzS\n3KJ6MK/pzfw0uj2pvwXwt2Oqwy3oZhpeA/CLIusB4Pvofnxsodvn8SCAvegue/ZrAD8BMDemevwr\ngDcAvJ6+2A4UUI+PofsR9nUAr6b/Pl10mzj1KLRNAPwpgP9Nz3cCwN+l24faHvyFH1GgQu/wIwoW\ng58oUAx+okAx+IkCxeAnChSDnyhQDH6iQDH4iQL1/4lgkqXrNQNBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221f8ede0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = X_test.shape[0]\n",
    "image_in_each_matrix = X_test.shape[1]\n",
    "print(image_in_each_matrix)\n",
    "\n",
    "for i in range(1,6):\n",
    "    \n",
    "    first = randint(0, total_images - 1)\n",
    "    second = randint(0, image_in_each_matrix)\n",
    "    print(first)\n",
    "    A = X_test[first][second]\n",
    "    figure(1)\n",
    "    imshow(A, interpolation='nearest')\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> b) Normalice las imágenes, dividiendo las intensidades originales de pixel por 255. Represente adecuadamente\n",
    "la salida deseada de la red de modo de tener un vector de tamaño igual al número de clases </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Defina una CNN con arquitectura C ×P ×C ×P ×F ×F. Para la primera capa convolucional utilice 16\n",
    "filtros de 5×5 y para la segunda 512 filtros de 7×7. Para la capa MLP escondida use 20 neuronas. Esta\n",
    "arquitectura, con algunas diferencias, fue una de las primera CNNs entrenadas sobre SVHN y consigui´o\n",
    "una accuracy de 94.28% [11]. Genere un esquema lo m´as compacto posible que muestre los cambios\n",
    "de forma que experimenta un patr´on de entrada a medida que se ejecuta un forward-pass. Entrene la\n",
    "red anterior un m´aximo de 10 epochs. ¿Logra mejorar o al menos igualar el resultado reportado en la\n",
    "literatura?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), activation=\"relu\", input_shape=(3, 32, 32..., padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (7, 7), activation=\"relu\", padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 512, 16, 16)       401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 512, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                655380    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,058,726\n",
      "Trainable params: 1,058,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/8\n",
      "73257/73257 [==============================] - 153s - loss: 0.3903 - acc: 0.9025 - val_loss: 0.2331 - val_acc: 0.9166\n",
      "Epoch 2/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.1667 - acc: 0.9396 - val_loss: 0.1648 - val_acc: 0.9415\n",
      "Epoch 3/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.1091 - acc: 0.9621 - val_loss: 0.1065 - val_acc: 0.9639\n",
      "Epoch 4/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.0881 - acc: 0.9703 - val_loss: 0.0987 - val_acc: 0.9673\n",
      "Epoch 5/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.0785 - acc: 0.9740 - val_loss: 0.1008 - val_acc: 0.9666\n",
      "Epoch 6/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.0722 - acc: 0.9762 - val_loss: 0.0828 - val_acc: 0.9732\n",
      "Epoch 7/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.0676 - acc: 0.9779 - val_loss: 0.0869 - val_acc: 0.9714\n",
      "Epoch 8/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.0639 - acc: 0.9793 - val_loss: 0.0770 - val_acc: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133717fd4e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, 5, 5, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(512, 7, 7, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "model.fit(X_train, Y_train, batch_size=400, nb_epoch=8, verbose=1, \\\n",
    "            validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), input_shape=(3, 32, 32..., padding=\"same\", activation=\"relu\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (7, 7), padding=\"same\", activation=\"relu\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 512, 16, 16)       401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 512, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                655380    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,058,726\n",
      "Trainable params: 1,058,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/8\n",
      "73257/73257 [==============================] - 156s - loss: 2.6088 - acc: 0.1881 - val_loss: 2.2233 - val_acc: 0.1959\n",
      "Epoch 2/8\n",
      "73257/73257 [==============================] - 151s - loss: 2.2335 - acc: 0.1892 - val_loss: 2.2243 - val_acc: 0.1959\n",
      "Epoch 3/8\n",
      "73257/73257 [==============================] - 151s - loss: 2.0653 - acc: 0.2564 - val_loss: 1.7573 - val_acc: 0.3928\n",
      "Epoch 4/8\n",
      "73257/73257 [==============================] - 175s - loss: 1.5004 - acc: 0.4957 - val_loss: 1.3719 - val_acc: 0.5558\n",
      "Epoch 5/8\n",
      "73257/73257 [==============================] - 367s - loss: 1.0932 - acc: 0.6485 - val_loss: 1.1354 - val_acc: 0.6460\n",
      "Epoch 6/8\n",
      "73257/73257 [==============================] - 381s - loss: 0.8277 - acc: 0.7505 - val_loss: 1.0129 - val_acc: 0.6911\n",
      "Epoch 7/8\n",
      "73257/73257 [==============================] - 367s - loss: 0.6954 - acc: 0.7988 - val_loss: 0.7626 - val_acc: 0.7824\n",
      "Epoch 8/8\n",
      "73257/73257 [==============================] - 388s - loss: 0.6167 - acc: 0.8251 - val_loss: 0.7101 - val_acc: 0.8011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221f315b9e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, 5, 5, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(512, 7, 7, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "model.fit(X_train, Y_train, batch_size=400, nb_epoch=8, verbose=1, \\\n",
    "            validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Eval\u0013ue el efecto de modi\f",
    "car el tama~no de los \f",
    "ltros (de convoluci\u0013on y pooling) reportando la\n",
    "sensibilidad del error de pruebas a estos cambios. Presente un gr\u0013a\f",
    "co o tabla resumen. Por simplicidad\n",
    "entre durante s\u0013olo 10 epochs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (10, 10), padding=\"same\", input_shape=(3, 32, 32..., activation=\"relu\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (15, 15), padding=\"same\", activation=\"relu\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 32, 32)        4816      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 512, 6, 6)         1843712   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,858,998\n",
      "Trainable params: 1,858,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/10\n",
      "73257/73257 [==============================] - 69s - loss: 0.3263 - acc: 0.8996 - val_loss: 0.3183 - val_acc: 0.9000\n",
      "Epoch 2/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3186 - acc: 0.9000 - val_loss: 0.3172 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3180 - acc: 0.9000 - val_loss: 0.3167 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3178 - acc: 0.9000 - val_loss: 0.3165 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3177 - acc: 0.9000 - val_loss: 0.3164 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3177 - acc: 0.9000 - val_loss: 0.3163 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3177 - acc: 0.9000 - val_loss: 0.3163 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3177 - acc: 0.9000 - val_loss: 0.3163 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3176 - acc: 0.9000 - val_loss: 0.3163 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "73257/73257 [==============================] - 64s - loss: 0.3176 - acc: 0.9000 - val_loss: 0.3162 - val_acc: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25a640792e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, 10, 10, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "model.add(Convolution2D(512, 15, 15, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "model.fit(X_train, Y_train, batch_size=400, nb_epoch=10, verbose=1, \\\n",
    "            validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (2, 2), input_shape=(3, 32, 32..., activation=\"relu\", padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (2, 2), activation=\"relu\", padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 32, 32)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 512, 6, 6)         33280     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 512, 6, 6)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                368660    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 402,358\n",
      "Trainable params: 402,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/10\n",
      "73257/73257 [==============================] - 31s - loss: 0.3180 - acc: 0.8997 - val_loss: 0.3042 - val_acc: 0.9010\n",
      "Epoch 2/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.2869 - acc: 0.9035 - val_loss: 0.2729 - val_acc: 0.9070\n",
      "Epoch 3/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.2553 - acc: 0.9125 - val_loss: 0.2559 - val_acc: 0.9117\n",
      "Epoch 4/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.2302 - acc: 0.9197 - val_loss: 0.2269 - val_acc: 0.9187\n",
      "Epoch 5/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.2113 - acc: 0.9251 - val_loss: 0.2122 - val_acc: 0.9242\n",
      "Epoch 6/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1985 - acc: 0.9287 - val_loss: 0.1995 - val_acc: 0.9285\n",
      "Epoch 7/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1892 - acc: 0.9315 - val_loss: 0.1946 - val_acc: 0.9317\n",
      "Epoch 8/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1816 - acc: 0.9338 - val_loss: 0.1886 - val_acc: 0.9318\n",
      "Epoch 9/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1756 - acc: 0.9359 - val_loss: 0.1915 - val_acc: 0.9306\n",
      "Epoch 10/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1704 - acc: 0.9374 - val_loss: 0.1784 - val_acc: 0.9350\n",
      "Tamaño filtros primera capa convolucional: 2X2\n",
      "Tamaño filtros segunda capa convolucional: 2X2\n",
      "Tamaño filtros primera capa de pooling: 5X5\n",
      "Tamaño filtros segunda capa de pooling: 1X1\n",
      "Acc: 93.5045433103%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 512, 8, 8)         8704      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 2, 2)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                40980     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 51,110\n",
      "Trainable params: 51,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), input_shape=(3, 32, 32..., activation=\"relu\", padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/10\n",
      "73257/73257 [==============================] - 31s - loss: 0.3084 - acc: 0.9006 - val_loss: 0.2788 - val_acc: 0.9070\n",
      "Epoch 2/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.2645 - acc: 0.9090 - val_loss: 0.2553 - val_acc: 0.9099\n",
      "Epoch 3/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.2313 - acc: 0.9156 - val_loss: 0.2243 - val_acc: 0.9194\n",
      "Epoch 4/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.2108 - acc: 0.9227 - val_loss: 0.2073 - val_acc: 0.9237\n",
      "Epoch 5/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.1985 - acc: 0.9271 - val_loss: 0.1951 - val_acc: 0.9279\n",
      "Epoch 6/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.1887 - acc: 0.9306 - val_loss: 0.1867 - val_acc: 0.9304\n",
      "Epoch 7/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.1809 - acc: 0.9333 - val_loss: 0.1770 - val_acc: 0.9343\n",
      "Epoch 8/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.1745 - acc: 0.9359 - val_loss: 0.1726 - val_acc: 0.9362\n",
      "Epoch 9/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.1688 - acc: 0.9379 - val_loss: 0.1735 - val_acc: 0.9375\n",
      "Epoch 10/10\n",
      "73257/73257 [==============================] - 30s - loss: 0.1641 - acc: 0.9397 - val_loss: 0.1862 - val_acc: 0.9295\n",
      "Tamaño filtros primera capa convolucional: 5X5\n",
      "Tamaño filtros segunda capa convolucional: 1X1\n",
      "Tamaño filtros primera capa de pooling: 4X4\n",
      "Tamaño filtros segunda capa de pooling: 3X3\n",
      "Acc: 92.9463839084%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 16, 32, 32)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 512, 32, 32)       74240     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 512, 10, 10)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                1024020   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,098,678\n",
      "Trainable params: 1,098,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[400,512,32,32]\n\t [[Node: gradients_2/max_pooling2d_6/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_6/MaxPool\"], data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 3, 3, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](max_pooling2d_6/transpose, max_pooling2d_6/MaxPool, gradients_2/max_pooling2d_6/transpose_1_grad/transpose)]]\n\nCaused by op 'gradients_2/max_pooling2d_6/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-56ab9a8b81ec>\", line 19, in <module>\n    model.fit(X_train, Y_train, batch_size=400, nb_epoch=10, verbose=1,                 validation_data=(X_test, Y_test))\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 856, in fit\n    initial_epoch=initial_epoch)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1481, in fit\n    self._make_train_function()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1013, in _make_train_function\n    self.total_loss)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py\", line 254, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py\", line 47, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2264, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 482, in gradients\n    in_grads = grad_fn(op, *out_grads)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 421, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1697, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'max_pooling2d_6/MaxPool', defined at:\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-56ab9a8b81ec>\", line 12, in <module>\n    model.add(MaxPooling2D(pool_size=(second_max_pool_fil_size, second_max_pool_fil_size)))\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 466, in add\n    output_tensor = layer(self.outputs[0])\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3245, in pool2d\n    x = tf.nn.max_pool(x, pool_size, strides, padding=padding)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1793, in max_pool\n    name=name)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1598, in _max_pool\n    data_format=data_format, name=name)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[400,512,32,32]\n\t [[Node: gradients_2/max_pooling2d_6/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_6/MaxPool\"], data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 3, 3, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](max_pooling2d_6/transpose, max_pooling2d_6/MaxPool, gradients_2/max_pooling2d_6/transpose_1_grad/transpose)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[400,512,32,32]\n\t [[Node: gradients_2/max_pooling2d_6/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_6/MaxPool\"], data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 3, 3, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](max_pooling2d_6/transpose, max_pooling2d_6/MaxPool, gradients_2/max_pooling2d_6/transpose_1_grad/transpose)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-56ab9a8b81ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adagrad'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0madagrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdagrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tamaño filtros primera capa convolucional: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_convolution_filter_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'X'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_convolution_filter_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[400,512,32,32]\n\t [[Node: gradients_2/max_pooling2d_6/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_6/MaxPool\"], data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 3, 3, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](max_pooling2d_6/transpose, max_pooling2d_6/MaxPool, gradients_2/max_pooling2d_6/transpose_1_grad/transpose)]]\n\nCaused by op 'gradients_2/max_pooling2d_6/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-56ab9a8b81ec>\", line 19, in <module>\n    model.fit(X_train, Y_train, batch_size=400, nb_epoch=10, verbose=1,                 validation_data=(X_test, Y_test))\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 856, in fit\n    initial_epoch=initial_epoch)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1481, in fit\n    self._make_train_function()\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1013, in _make_train_function\n    self.total_loss)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py\", line 254, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py\", line 47, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2264, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 482, in gradients\n    in_grads = grad_fn(op, *out_grads)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 421, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1697, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'max_pooling2d_6/MaxPool', defined at:\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-56ab9a8b81ec>\", line 12, in <module>\n    model.add(MaxPooling2D(pool_size=(second_max_pool_fil_size, second_max_pool_fil_size)))\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 466, in add\n    output_tensor = layer(self.outputs[0])\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3245, in pool2d\n    x = tf.nn.max_pool(x, pool_size, strides, padding=padding)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1793, in max_pool\n    name=name)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1598, in _max_pool\n    data_format=data_format, name=name)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[400,512,32,32]\n\t [[Node: gradients_2/max_pooling2d_6/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_6/MaxPool\"], data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 3, 3, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](max_pooling2d_6/transpose, max_pooling2d_6/MaxPool, gradients_2/max_pooling2d_6/transpose_1_grad/transpose)]]\n"
     ]
    }
   ],
   "source": [
    "array = []\n",
    "for i in range(0,5):\n",
    "    first_max_pool_fil_size = randint(1,6)\n",
    "    second_max_pool_fil_size = randint(1,6)\n",
    "    first_convolution_filter_size = randint(1,6)\n",
    "    second_convolution_filter_size = randint(1,6)\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, first_convolution_filter_size, first_convolution_filter_size, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "    model.add(MaxPooling2D(pool_size=(first_max_pool_fil_size, first_max_pool_fil_size)))\n",
    "    model.add(Convolution2D(512, second_convolution_filter_size, second_convolution_filter_size, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(second_max_pool_fil_size, second_max_pool_fil_size)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "    adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "    model.fit(X_train, Y_train, batch_size=400, nb_epoch=10, verbose=1, \\\n",
    "                validation_data=(X_test, Y_test))\n",
    "    scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Tamaño filtros primera capa convolucional: \" + str(first_convolution_filter_size) + 'X' + str(first_convolution_filter_size) )\n",
    "    print(\"Tamaño filtros segunda capa convolucional: \" + str(second_convolution_filter_size) + 'X' + str(second_convolution_filter_size) )\n",
    "    print(\"Tamaño filtros primera capa de pooling: \" + str(first_max_pool_fil_size) + 'X' + str(first_max_pool_fil_size) )\n",
    "    print(\"Tamaño filtros segunda capa de pooling: \" + str(second_max_pool_fil_size) + 'X' + str(second_max_pool_fil_size) ) \n",
    "    print(\"Acc: \" + str(scores[1] * 100) + \"%\")\n",
    "    array.append([scores[1],first_max_pool_fil_size ,second_max_pool_fil_size, first_convolution_filter_size, second_convolution_filter_size] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), input_shape=(3, 32, 32..., activation=\"relu\", padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (5, 5), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 16, 32, 32)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 10, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 512, 10, 10)       205312    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 512, 3, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                92180     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 297,766\n",
      "Trainable params: 297,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/10\n",
      "73257/73257 [==============================] - 61s - loss: 0.3245 - acc: 0.8994 - val_loss: 0.3117 - val_acc: 0.9003\n",
      "Epoch 2/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.3007 - acc: 0.9031 - val_loss: 0.3058 - val_acc: 0.9036\n",
      "Epoch 3/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.2870 - acc: 0.9064 - val_loss: 0.2833 - val_acc: 0.9098\n",
      "Epoch 4/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.2743 - acc: 0.9098 - val_loss: 0.2723 - val_acc: 0.9090\n",
      "Epoch 5/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.2612 - acc: 0.9135 - val_loss: 0.2541 - val_acc: 0.9155\n",
      "Epoch 6/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.2509 - acc: 0.9164 - val_loss: 0.2455 - val_acc: 0.9177\n",
      "Epoch 7/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.2429 - acc: 0.9180 - val_loss: 0.2423 - val_acc: 0.9179\n",
      "Epoch 8/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.2358 - acc: 0.9196 - val_loss: 0.2332 - val_acc: 0.9197\n",
      "Epoch 9/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.2300 - acc: 0.9203 - val_loss: 0.2385 - val_acc: 0.9185\n",
      "Epoch 10/10\n",
      "73257/73257 [==============================] - 60s - loss: 0.2248 - acc: 0.9211 - val_loss: 0.2200 - val_acc: 0.9217\n",
      "Tamaño filtros primera capa convolucional: 1X1\n",
      "Tamaño filtros segunda capa convolucional: 5X5\n",
      "Tamaño filtros primera capa de pooling: 3X3\n",
      "Tamaño filtros segunda capa de pooling: 3X3\n",
      "Acc: 92.1731040752%\n"
     ]
    }
   ],
   "source": [
    "first_max_pool_fil_size = randint(1,6)\n",
    "second_max_pool_fil_size = first_max_pool_fil_size\n",
    "first_convolution_filter_size = randint(1,6)\n",
    "second_convolution_filter_size = randint(1,6)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, first_convolution_filter_size, first_convolution_filter_size, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(first_max_pool_fil_size, first_max_pool_fil_size)))\n",
    "model.add(Convolution2D(512, second_convolution_filter_size, second_convolution_filter_size, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(second_max_pool_fil_size, second_max_pool_fil_size)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "model.fit(X_train, Y_train, batch_size=400, nb_epoch=10, verbose=1, \\\n",
    "            validation_data=(X_test, Y_test))\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Tamaño filtros primera capa convolucional: \" + str(first_convolution_filter_size) + 'X' + str(first_convolution_filter_size) )\n",
    "print(\"Tamaño filtros segunda capa convolucional: \" + str(second_convolution_filter_size) + 'X' + str(second_convolution_filter_size) )\n",
    "print(\"Tamaño filtros primera capa de pooling: \" + str(first_max_pool_fil_size) + 'X' + str(first_max_pool_fil_size) )\n",
    "print(\"Tamaño filtros segunda capa de pooling: \" + str(second_max_pool_fil_size) + 'X' + str(second_max_pool_fil_size) ) \n",
    "print(\"Acc: \" + str(scores[1] * 100) + \"%\")\n",
    "array.append([scores[1],first_max_pool_fil_size ,second_max_pool_fil_size, first_convolution_filter_size, second_convolution_filter_size] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<p>Eval\u0013ue el efecto de modi\f",
    "car el n\u0013umero de \f",
    "ltros para las capas convolucionales tanto en los tiempos\n",
    "de entrenamiento como en el desempe~no de la red. Presente un gr\u0013a\f",
    "co o tabla resumen. Por simplicidad\n",
    "entre durante s\u0013olo 10 epochs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(3, 32, 32..., padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 32, 32)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 512, 6, 6)         205312    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 216,230\n",
      "Trainable params: 216,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/10\n",
      "73257/73257 [==============================] - 32s - loss: 0.3060 - acc: 0.9016 - val_loss: 0.2952 - val_acc: 0.9016\n",
      "Epoch 2/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.2605 - acc: 0.9126 - val_loss: 0.2387 - val_acc: 0.9188\n",
      "Epoch 3/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.2223 - acc: 0.9239 - val_loss: 0.2043 - val_acc: 0.9287\n",
      "Epoch 4/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1888 - acc: 0.9317 - val_loss: 0.1842 - val_acc: 0.9330\n",
      "Epoch 5/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1652 - acc: 0.9385 - val_loss: 0.1671 - val_acc: 0.9384\n",
      "Epoch 6/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1516 - acc: 0.9432 - val_loss: 0.1537 - val_acc: 0.9424\n",
      "Epoch 7/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1424 - acc: 0.9466 - val_loss: 0.1459 - val_acc: 0.9463\n",
      "Epoch 8/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1355 - acc: 0.9496 - val_loss: 0.1362 - val_acc: 0.9499\n",
      "Epoch 9/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1297 - acc: 0.9522 - val_loss: 0.1325 - val_acc: 0.9521\n",
      "Epoch 10/10\n",
      "73257/73257 [==============================] - 28s - loss: 0.1248 - acc: 0.9543 - val_loss: 0.1305 - val_acc: 0.9529\n",
      "Tamaño filtros primera capa convolucional: 3X3\n",
      "Tamaño filtros segunda capa convolucional: 5X5\n",
      "Acc: 95.2942641586%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 16, 32, 32)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 512, 6, 6)         33280     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 44,198\n",
      "Trainable params: 44,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (2, 2), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.3152 - acc: 0.9000 - val_loss: 0.3048 - val_acc: 0.9000\n",
      "Epoch 2/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2972 - acc: 0.9022 - val_loss: 0.2836 - val_acc: 0.9061\n",
      "Epoch 3/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2819 - acc: 0.9047 - val_loss: 0.2733 - val_acc: 0.9076\n",
      "Epoch 4/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2700 - acc: 0.9062 - val_loss: 0.2585 - val_acc: 0.9084\n",
      "Epoch 5/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2604 - acc: 0.9076 - val_loss: 0.2516 - val_acc: 0.9113\n",
      "Epoch 6/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2520 - acc: 0.9096 - val_loss: 0.2452 - val_acc: 0.9115\n",
      "Epoch 7/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2449 - acc: 0.9114 - val_loss: 0.2370 - val_acc: 0.9138\n",
      "Epoch 8/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2386 - acc: 0.9134 - val_loss: 0.2441 - val_acc: 0.9109\n",
      "Epoch 9/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2330 - acc: 0.9150 - val_loss: 0.2245 - val_acc: 0.9185\n",
      "Epoch 10/10\n",
      "73257/73257 [==============================] - 24s - loss: 0.2281 - acc: 0.9166 - val_loss: 0.2192 - val_acc: 0.9192\n",
      "Tamaño filtros primera capa convolucional: 3X3\n",
      "Tamaño filtros segunda capa convolucional: 2X2\n",
      "Acc: 91.9214904418%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 512, 6, 6)         205312    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 216,998\n",
      "Trainable params: 216,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), activation=\"relu\", input_shape=(3, 32, 32..., padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.3152 - acc: 0.8999 - val_loss: 0.2979 - val_acc: 0.9000\n",
      "Epoch 2/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.2795 - acc: 0.9012 - val_loss: 0.2600 - val_acc: 0.9029\n",
      "Epoch 3/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.2401 - acc: 0.9142 - val_loss: 0.2199 - val_acc: 0.9223\n",
      "Epoch 4/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.2129 - acc: 0.9229 - val_loss: 0.1996 - val_acc: 0.9275\n",
      "Epoch 5/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.1953 - acc: 0.9279 - val_loss: 0.1925 - val_acc: 0.9303\n",
      "Epoch 6/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.1754 - acc: 0.9362 - val_loss: 0.1710 - val_acc: 0.9392\n",
      "Epoch 7/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.1591 - acc: 0.9432 - val_loss: 0.1584 - val_acc: 0.9438\n",
      "Epoch 8/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.1466 - acc: 0.9469 - val_loss: 0.1430 - val_acc: 0.9493\n",
      "Epoch 9/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.1360 - acc: 0.9507 - val_loss: 0.1378 - val_acc: 0.9508\n",
      "Epoch 10/10\n",
      "73257/73257 [==============================] - 29s - loss: 0.1267 - acc: 0.9547 - val_loss: 0.1290 - val_acc: 0.9553\n",
      "Tamaño filtros primera capa convolucional: 5X5\n",
      "Tamaño filtros segunda capa convolucional: 5X5\n",
      "Acc: 95.5332010417%\n"
     ]
    }
   ],
   "source": [
    "array = []\n",
    "for i in range(0,3):\n",
    "    first_max_pool_fil_size = randint(1,6)\n",
    "    second_max_pool_fil_size = randint(1,6)\n",
    "    first_convolution_filter_size = randint(1,6)\n",
    "    second_convolution_filter_size = randint(1,6)\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, first_convolution_filter_size, first_convolution_filter_size, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "    model.add(Convolution2D(512, second_convolution_filter_size, second_convolution_filter_size, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "    adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "    model.fit(X_train, Y_train, batch_size=200, nb_epoch=10, verbose=1, \\\n",
    "                validation_data=(X_test, Y_test))\n",
    "    scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Tamaño filtros primera capa convolucional: \" + str(first_convolution_filter_size) + 'X' + str(first_convolution_filter_size) )\n",
    "    print(\"Tamaño filtros segunda capa convolucional: \" + str(second_convolution_filter_size) + 'X' + str(second_convolution_filter_size) )\n",
    "    print(\"Acc: \" + str(scores[1] * 100) + \"%\")\n",
    "    array.append([scores[1],first_max_pool_fil_size ,second_max_pool_fil_size, first_convolution_filter_size, second_convolution_filter_size] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Proponga una mejora sobre la red de\f",
    "nida en (c) que mejore el error de pruebas. Recuerde que debe\n",
    "de\f",
    "nir un subconjunto de validaci\u0013on si necesita elegir entre arquitecturas.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), input_shape=(3, 32, 32..., padding=\"same\", activation=\"relu\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (7, 7), padding=\"same\", activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 512, 6, 6)         401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 413,606\n",
      "Trainable params: 413,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8571 samples, validate on 1429 samples\n",
      "Epoch 1/10\n",
      "8571/8571 [==============================] - 14s - loss: 2.2545 - acc: 0.1834 - val_loss: 2.1989 - val_acc: 0.2064\n",
      "Epoch 2/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.1937 - acc: 0.1925 - val_loss: 2.1385 - val_acc: 0.2148\n",
      "Epoch 3/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.1402 - acc: 0.2091 - val_loss: 2.1321 - val_acc: 0.2099\n",
      "Epoch 4/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.0837 - acc: 0.2403 - val_loss: 2.0373 - val_acc: 0.2778\n",
      "Epoch 5/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.0144 - acc: 0.2801 - val_loss: 1.9633 - val_acc: 0.3359\n",
      "Epoch 6/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.9343 - acc: 0.3129 - val_loss: 1.8581 - val_acc: 0.3352\n",
      "Epoch 7/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.8524 - acc: 0.3400 - val_loss: 1.8027 - val_acc: 0.3527\n",
      "Epoch 8/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.7735 - acc: 0.3601 - val_loss: 1.7245 - val_acc: 0.3688\n",
      "Epoch 9/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.7078 - acc: 0.3693 - val_loss: 1.6917 - val_acc: 0.3793\n",
      "Epoch 10/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.6683 - acc: 0.3798 - val_loss: 1.6482 - val_acc: 0.3982\n",
      "Acc: 39.8180546378%\n",
      "acc: 39.82%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 512, 6, 6)         401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 413,606\n",
      "Trainable params: 413,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8571 samples, validate on 1429 samples\n",
      "Epoch 1/10\n",
      "8571/8571 [==============================] - 13s - loss: 2.2505 - acc: 0.1891 - val_loss: 2.2202 - val_acc: 0.1875\n",
      "Epoch 2/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.1757 - acc: 0.2052 - val_loss: 2.0989 - val_acc: 0.2071\n",
      "Epoch 3/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.0657 - acc: 0.2415 - val_loss: 1.9898 - val_acc: 0.3289\n",
      "Epoch 4/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.9268 - acc: 0.3068 - val_loss: 1.8597 - val_acc: 0.2736\n",
      "Epoch 5/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.8008 - acc: 0.3490 - val_loss: 1.7920 - val_acc: 0.3387\n",
      "Epoch 6/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.6757 - acc: 0.3916 - val_loss: 1.6002 - val_acc: 0.4108\n",
      "Epoch 7/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.5775 - acc: 0.4446 - val_loss: 1.5018 - val_acc: 0.4619\n",
      "Epoch 8/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.4679 - acc: 0.4971 - val_loss: 1.4214 - val_acc: 0.5206\n",
      "Epoch 9/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.3884 - acc: 0.5316 - val_loss: 1.3626 - val_acc: 0.5423\n",
      "Epoch 10/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.3054 - acc: 0.5726 - val_loss: 1.2916 - val_acc: 0.5717\n",
      "Acc: 57.1728481706%\n",
      "acc: 57.17%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 512, 6, 6)         401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 413,606\n",
      "Trainable params: 413,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8571 samples, validate on 1429 samples\n",
      "Epoch 1/10\n",
      "8571/8571 [==============================] - 13s - loss: 2.3027 - acc: 0.1351 - val_loss: 2.2456 - val_acc: 0.1973\n",
      "Epoch 2/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.2056 - acc: 0.2144 - val_loss: 2.1844 - val_acc: 0.1847\n",
      "Epoch 3/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.0904 - acc: 0.2499 - val_loss: 2.1211 - val_acc: 0.1875\n",
      "Epoch 4/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.9828 - acc: 0.2897 - val_loss: 1.9266 - val_acc: 0.2911\n",
      "Epoch 5/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.8531 - acc: 0.3493 - val_loss: 1.9274 - val_acc: 0.3380\n",
      "Epoch 6/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.7312 - acc: 0.3974 - val_loss: 1.6813 - val_acc: 0.4024\n",
      "Epoch 7/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.6414 - acc: 0.4292 - val_loss: 1.6959 - val_acc: 0.4108\n",
      "Epoch 8/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.5100 - acc: 0.4834 - val_loss: 1.4766 - val_acc: 0.5101\n",
      "Epoch 9/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.4223 - acc: 0.5310 - val_loss: 1.3699 - val_acc: 0.5388\n",
      "Epoch 10/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.3367 - acc: 0.5664 - val_loss: 1.2711 - val_acc: 0.5969\n",
      "Acc: 59.6920924057%\n",
      "acc: 59.69%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 512, 6, 6)         401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 413,606\n",
      "Trainable params: 413,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8571 samples, validate on 1429 samples\n",
      "Epoch 1/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.2657 - acc: 0.1871 - val_loss: 2.2278 - val_acc: 0.2008\n",
      "Epoch 2/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.2065 - acc: 0.2065 - val_loss: 2.1989 - val_acc: 0.2022\n",
      "Epoch 3/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.1453 - acc: 0.2340 - val_loss: 2.0921 - val_acc: 0.2253\n",
      "Epoch 4/10\n",
      "8571/8571 [==============================] - 12s - loss: 2.0633 - acc: 0.2610 - val_loss: 2.0508 - val_acc: 0.2351\n",
      "Epoch 5/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.9990 - acc: 0.2746 - val_loss: 2.0185 - val_acc: 0.2456\n",
      "Epoch 6/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.9384 - acc: 0.2995 - val_loss: 1.9115 - val_acc: 0.3527\n",
      "Epoch 7/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.8517 - acc: 0.3324 - val_loss: 1.7965 - val_acc: 0.3443\n",
      "Epoch 8/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.7683 - acc: 0.3856 - val_loss: 1.7020 - val_acc: 0.4115\n",
      "Epoch 9/10\n",
      "8571/8571 [==============================] - 11s - loss: 1.6925 - acc: 0.4221 - val_loss: 1.6060 - val_acc: 0.4647\n",
      "Epoch 10/10\n",
      "8571/8571 [==============================] - 12s - loss: 1.5986 - acc: 0.4547 - val_loss: 1.6156 - val_acc: 0.4479\n",
      "Acc: 44.7865640412%\n",
      "acc: 44.79%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 512, 6, 6)         401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 413,606\n",
      "Trainable params: 413,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8572 samples, validate on 1428 samples\n",
      "Epoch 1/10\n",
      "8572/8572 [==============================] - 13s - loss: 2.3010 - acc: 0.1891 - val_loss: 2.2260 - val_acc: 0.1933\n",
      "Epoch 2/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.2051 - acc: 0.1949 - val_loss: 2.1588 - val_acc: 0.1912\n",
      "Epoch 3/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.1374 - acc: 0.2117 - val_loss: 2.0997 - val_acc: 0.2486\n",
      "Epoch 4/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.0738 - acc: 0.2319 - val_loss: 2.0462 - val_acc: 0.2682\n",
      "Epoch 5/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.0085 - acc: 0.2645 - val_loss: 1.9735 - val_acc: 0.2906\n",
      "Epoch 6/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.9284 - acc: 0.3041 - val_loss: 1.8853 - val_acc: 0.2857\n",
      "Epoch 7/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.8491 - acc: 0.3422 - val_loss: 1.8038 - val_acc: 0.3817\n",
      "Epoch 8/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.7486 - acc: 0.3857 - val_loss: 1.7151 - val_acc: 0.4146\n",
      "Epoch 9/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.6623 - acc: 0.4232 - val_loss: 1.6498 - val_acc: 0.4426\n",
      "Epoch 10/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.5613 - acc: 0.4666 - val_loss: 1.5370 - val_acc: 0.4860\n",
      "Acc: 48.599439801%\n",
      "acc: 48.60%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 512, 6, 6)         401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 413,606\n",
      "Trainable params: 413,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8572 samples, validate on 1428 samples\n",
      "Epoch 1/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.3550 - acc: 0.1758 - val_loss: 2.2525 - val_acc: 0.1863\n",
      "Epoch 2/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.2377 - acc: 0.1939 - val_loss: 2.2499 - val_acc: 0.1863\n",
      "Epoch 3/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.2318 - acc: 0.1937 - val_loss: 2.2432 - val_acc: 0.1863\n",
      "Epoch 4/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.2228 - acc: 0.1954 - val_loss: 2.2331 - val_acc: 0.1856\n",
      "Epoch 5/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.2088 - acc: 0.1988 - val_loss: 2.2114 - val_acc: 0.1996\n",
      "Epoch 6/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.1695 - acc: 0.2234 - val_loss: 2.1609 - val_acc: 0.2486\n",
      "Epoch 7/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.1157 - acc: 0.2549 - val_loss: 2.1376 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.0643 - acc: 0.2801 - val_loss: 2.0719 - val_acc: 0.2626\n",
      "Epoch 9/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.0167 - acc: 0.3066 - val_loss: 2.0479 - val_acc: 0.2871\n",
      "Epoch 10/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.9702 - acc: 0.3250 - val_loss: 2.0015 - val_acc: 0.2955\n",
      "Acc: 29.5518207283%\n",
      "acc: 29.55%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 16, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 512, 6, 6)         401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 413,606\n",
      "Trainable params: 413,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8572 samples, validate on 1428 samples\n",
      "Epoch 1/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.3193 - acc: 0.1695 - val_loss: 2.2372 - val_acc: 0.1947\n",
      "Epoch 2/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.2345 - acc: 0.1921 - val_loss: 2.2304 - val_acc: 0.1947\n",
      "Epoch 3/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.2220 - acc: 0.1923 - val_loss: 2.2092 - val_acc: 0.1968\n",
      "Epoch 4/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.1948 - acc: 0.1988 - val_loss: 2.1657 - val_acc: 0.1996\n",
      "Epoch 5/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.1375 - acc: 0.2245 - val_loss: 2.1150 - val_acc: 0.2066\n",
      "Epoch 6/10\n",
      "8572/8572 [==============================] - 12s - loss: 2.0130 - acc: 0.2852 - val_loss: 1.9305 - val_acc: 0.3326\n",
      "Epoch 7/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.8629 - acc: 0.3423 - val_loss: 1.7919 - val_acc: 0.3852\n",
      "Epoch 8/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.7228 - acc: 0.3994 - val_loss: 1.6871 - val_acc: 0.4090\n",
      "Epoch 9/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.6174 - acc: 0.4403 - val_loss: 1.5833 - val_acc: 0.4699\n",
      "Epoch 10/10\n",
      "8572/8572 [==============================] - 12s - loss: 1.5260 - acc: 0.4727 - val_loss: 1.5322 - val_acc: 0.4503\n",
      "Acc: 45.0280112045%\n",
      "acc: 45.03%\n",
      "46.38% (+/- 9.48%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "kfold = KFold(10000, n_folds=7, shuffle=True)\n",
    "cvscores = []\n",
    "\n",
    "for i, (train, test) in enumerate(kfold):\n",
    "\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 5, 5, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "    model.add(Convolution2D(512, 7, 7, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "    adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "    model.fit(X_train[train], Y_train[train], batch_size=200, nb_epoch=10, verbose=1, \\\n",
    "                validation_data=(X_train[test], Y_train[test]))\n",
    "    scores = model.evaluate(X_train[test], Y_train[test], verbose=0)\n",
    "    print(\"Acc: \" + str(scores[1] * 100) + \"%\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Elija una de las redes entrenadas (preferentemente una con buen desempe~no) y visualice los pesos\n",
    "correspondientes a los \f",
    "ltros de la primera capa convolucional. Visualice adem\u0013as el efecto del \f",
    "ltro\n",
    "sobre algunas im\u0013agenes de entrenamiento. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), padding=\"same\", input_shape=(3, 32, 32..., activation=\"relu\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (7, 7), padding=\"same\", activation=\"relu\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 512, 16, 16)       401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 512, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                655380    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,058,726\n",
      "Trainable params: 1,058,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/8\n",
      "73257/73257 [==============================] - 152s - loss: 2.5872 - acc: 0.8382 - val_loss: 2.5781 - val_acc: 0.8392\n",
      "Epoch 2/8\n",
      "73257/73257 [==============================] - 147s - loss: 2.5994 - acc: 0.8378 - val_loss: 2.5781 - val_acc: 0.8392\n",
      "Epoch 3/8\n",
      "73257/73257 [==============================] - 147s - loss: 2.5994 - acc: 0.8378 - val_loss: 2.5781 - val_acc: 0.8392\n",
      "Epoch 4/8\n",
      "73257/73257 [==============================] - 147s - loss: 2.5994 - acc: 0.8378 - val_loss: 2.5781 - val_acc: 0.8392\n",
      "Epoch 5/8\n",
      "73257/73257 [==============================] - 147s - loss: 2.5994 - acc: 0.8378 - val_loss: 2.5781 - val_acc: 0.8392\n",
      "Epoch 6/8\n",
      "73257/73257 [==============================] - 147s - loss: 2.5994 - acc: 0.8378 - val_loss: 2.5781 - val_acc: 0.8392\n",
      "Epoch 7/8\n",
      "73257/73257 [==============================] - 147s - loss: 2.5994 - acc: 0.8378 - val_loss: 2.5781 - val_acc: 0.8392\n",
      "Epoch 8/8\n",
      "73257/73257 [==============================] - 147s - loss: 2.5994 - acc: 0.8378 - val_loss: 2.5781 - val_acc: 0.8392\n",
      "[array([[[[  5.67020960e-02,  -1.98912323e-02,   1.03031229e-02, ...,\n",
      "           -3.37935761e-02,  -6.78307712e-02,   2.95828767e-02],\n",
      "         [ -5.66116124e-02,  -8.04314092e-02,  -6.43163733e-03, ...,\n",
      "            7.00688735e-02,   1.09176546e-01,   7.57874027e-02],\n",
      "         [  5.45103811e-02,   3.23694311e-02,  -9.93931666e-02, ...,\n",
      "           -5.69957215e-03,  -5.55633083e-02,  -7.66072795e-02]],\n",
      "\n",
      "        [[  5.45517653e-02,  -8.78318772e-02,   7.29796514e-02, ...,\n",
      "           -6.01707101e-02,  -4.78799678e-02,   9.98845175e-02],\n",
      "         [ -4.55662906e-02,   3.12897600e-02,   8.74684602e-02, ...,\n",
      "           -5.04878536e-02,  -9.02085975e-02,   3.49455401e-02],\n",
      "         [  2.89231986e-02,  -3.42344008e-02,   6.34458102e-03, ...,\n",
      "            4.09677364e-02,   1.10021383e-01,   2.58098878e-02]],\n",
      "\n",
      "        [[ -1.64466444e-02,  -1.38283465e-02,  -5.31613082e-02, ...,\n",
      "           -6.83275983e-02,  -8.60814080e-02,  -2.04591304e-02],\n",
      "         [  8.59460756e-02,   5.96480742e-02,   4.91701141e-02, ...,\n",
      "            2.16125399e-02,  -7.63309374e-02,  -4.52765860e-02],\n",
      "         [ -6.15913495e-02,   8.22868571e-03,   3.03295702e-02, ...,\n",
      "           -1.00210346e-01,   4.30458225e-02,  -7.93850794e-02]],\n",
      "\n",
      "        [[ -8.56314823e-02,  -7.72220194e-02,  -7.46093765e-02, ...,\n",
      "           -9.05070379e-02,  -2.04870477e-04,   3.75500917e-02],\n",
      "         [ -7.51580745e-02,  -6.06380682e-03,   2.98018344e-02, ...,\n",
      "           -7.59624094e-02,  -2.13368554e-02,  -7.59664178e-02],\n",
      "         [  1.04564987e-01,  -4.55681831e-02,  -9.16059166e-02, ...,\n",
      "           -1.11866012e-01,  -2.22926028e-04,   7.70879537e-02]],\n",
      "\n",
      "        [[ -3.57338898e-02,   6.95541948e-02,  -3.61244380e-02, ...,\n",
      "           -7.88294151e-02,  -3.77149358e-02,  -9.57537666e-02],\n",
      "         [  5.98048829e-02,  -3.52318957e-02,   6.34786636e-02, ...,\n",
      "            2.57028975e-02,  -3.52172088e-03,   5.31575158e-02],\n",
      "         [  2.60677915e-02,   4.88599688e-02,   5.14620319e-02, ...,\n",
      "           -6.48607239e-02,   1.01886742e-01,   1.20506231e-02]]],\n",
      "\n",
      "\n",
      "       [[[  1.13825344e-01,   7.26075321e-02,   1.32705020e-02, ...,\n",
      "            6.63996190e-02,  -5.19244000e-04,  -5.74177615e-02],\n",
      "         [ -1.32473558e-02,  -9.31426957e-02,   1.19398318e-01, ...,\n",
      "           -1.19638465e-01,  -1.87408552e-02,  -4.23279628e-02],\n",
      "         [  5.68302907e-02,   7.41381869e-02,   8.53348747e-02, ...,\n",
      "           -2.24349648e-03,   1.17003053e-01,   6.88567385e-02]],\n",
      "\n",
      "        [[  2.90442556e-02,   4.87036854e-02,   5.62539026e-02, ...,\n",
      "            2.53699534e-02,  -6.46241456e-02,   7.97273219e-02],\n",
      "         [  1.31510431e-02,   7.32820407e-02,  -8.28437358e-02, ...,\n",
      "           -5.31992652e-02,  -2.59077270e-03,  -6.56759143e-02],\n",
      "         [ -9.64396670e-02,  -1.80513561e-02,  -5.34990281e-02, ...,\n",
      "            3.83305997e-02,   1.44393435e-02,   1.01810329e-01]],\n",
      "\n",
      "        [[  1.22153729e-01,  -1.05040893e-01,   9.38817933e-02, ...,\n",
      "            4.39889319e-02,  -4.72517014e-02,   3.41864415e-02],\n",
      "         [  4.58265729e-02,  -1.01672061e-01,   2.71628350e-02, ...,\n",
      "            6.86917454e-02,   1.55992080e-02,   9.96435527e-03],\n",
      "         [  8.42112303e-02,  -2.35149171e-03,   1.70916505e-03, ...,\n",
      "           -3.55328396e-02,   4.51577920e-03,  -3.86227667e-03]],\n",
      "\n",
      "        [[ -9.14884731e-05,  -8.69296119e-02,   3.67490314e-02, ...,\n",
      "            8.80744457e-02,   1.19755045e-01,   1.42929740e-02],\n",
      "         [  7.17629790e-02,  -5.26233874e-02,   5.59331104e-02, ...,\n",
      "            9.17720348e-02,  -6.64746910e-02,   1.05032481e-01],\n",
      "         [ -6.49890080e-02,  -2.29506660e-03,  -5.94134741e-02, ...,\n",
      "           -1.16475105e-01,  -9.65186581e-02,   1.12210065e-02]],\n",
      "\n",
      "        [[ -1.91084296e-02,  -7.08105490e-02,  -9.53632221e-03, ...,\n",
      "           -7.02987239e-03,   8.50964263e-02,  -1.11124553e-01],\n",
      "         [  1.11187398e-01,  -4.64703999e-02,   3.25236470e-05, ...,\n",
      "            5.01947999e-02,   6.79769926e-03,  -4.76357639e-02],\n",
      "         [  1.11768924e-01,   4.61401008e-02,  -1.08528109e-02, ...,\n",
      "           -7.50386044e-02,   5.04300557e-02,  -1.10139191e-01]]],\n",
      "\n",
      "\n",
      "       [[[  1.01002440e-01,   5.49060181e-02,  -7.07580447e-02, ...,\n",
      "            3.64197157e-02,   8.09863955e-02,  -8.52489918e-02],\n",
      "         [ -1.07664280e-01,   3.18863615e-03,   2.82405466e-02, ...,\n",
      "            8.16449523e-04,  -1.92834623e-02,   4.30366173e-02],\n",
      "         [ -1.10231221e-01,   9.19628516e-02,   3.67831774e-02, ...,\n",
      "            6.89156875e-02,  -1.08429007e-02,  -8.98820348e-03]],\n",
      "\n",
      "        [[ -1.83883831e-02,  -1.16207033e-01,  -3.58179398e-02, ...,\n",
      "            8.16640556e-02,  -7.76218995e-02,   8.79205689e-02],\n",
      "         [  1.53343016e-02,  -2.82634608e-03,   1.02438129e-01, ...,\n",
      "            1.03646414e-02,   3.40133905e-02,  -7.99442008e-02],\n",
      "         [  3.99504900e-02,  -6.87401369e-02,   3.79817560e-02, ...,\n",
      "            7.68722072e-02,   4.14291583e-02,  -6.27800599e-02]],\n",
      "\n",
      "        [[ -6.42528385e-02,  -1.05838373e-01,   9.33508389e-03, ...,\n",
      "           -4.81588468e-02,  -6.96861073e-02,   8.28886703e-02],\n",
      "         [  6.10370524e-02,   5.83155081e-02,   2.02398114e-02, ...,\n",
      "           -5.02959713e-02,  -3.14887334e-03,   7.55727887e-02],\n",
      "         [ -2.99007371e-02,  -1.40918689e-02,   1.11231804e-01, ...,\n",
      "            6.86877966e-02,  -7.16413856e-02,  -7.17825349e-03]],\n",
      "\n",
      "        [[  4.95154560e-02,  -2.34031267e-02,  -2.34309305e-03, ...,\n",
      "            1.01303644e-01,   8.26936308e-03,   2.86709331e-02],\n",
      "         [  2.28121690e-02,   1.14744678e-01,  -1.46260411e-02, ...,\n",
      "           -1.02163568e-01,   9.85640883e-02,  -6.12765327e-02],\n",
      "         [  5.88957854e-02,  -1.41096907e-02,   1.07921615e-01, ...,\n",
      "           -2.62470022e-02,  -5.16953692e-03,  -2.70526260e-02]],\n",
      "\n",
      "        [[  3.75874527e-02,   4.99994084e-02,   3.37712355e-02, ...,\n",
      "            6.50403947e-02,   4.39134315e-02,  -7.22887814e-02],\n",
      "         [ -7.68660754e-02,  -4.92863059e-02,  -8.80741477e-02, ...,\n",
      "           -9.63869318e-02,  -5.28316423e-02,   7.24385977e-02],\n",
      "         [  1.15608118e-01,   9.54979584e-02,   1.21967055e-01, ...,\n",
      "           -4.59182784e-02,   7.61093721e-02,   9.62333903e-02]]],\n",
      "\n",
      "\n",
      "       [[[  5.82728013e-02,   8.75593275e-02,   8.10639933e-03, ...,\n",
      "           -6.22091517e-02,   6.82124645e-02,   6.39162436e-02],\n",
      "         [ -8.22443794e-03,  -6.92800134e-02,  -2.25361250e-02, ...,\n",
      "           -8.35085437e-02,  -2.58776359e-02,   1.11458331e-01],\n",
      "         [ -8.66243392e-02,   6.16802685e-02,   9.76638682e-03, ...,\n",
      "            6.39855638e-02,   1.00377962e-01,   1.16305195e-01]],\n",
      "\n",
      "        [[  6.71322122e-02,  -1.14692986e-01,  -1.78950727e-02, ...,\n",
      "           -8.64456072e-02,   6.07559923e-03,  -7.16471393e-03],\n",
      "         [ -8.34105313e-02,   3.65711935e-03,  -7.41007328e-02, ...,\n",
      "           -8.05555508e-02,  -1.50799304e-02,  -3.05650532e-02],\n",
      "         [  1.11214176e-01,  -1.65175591e-02,   3.69292721e-02, ...,\n",
      "           -1.15142889e-01,   6.94258735e-02,   5.53708449e-02]],\n",
      "\n",
      "        [[ -7.29110539e-02,  -7.38025382e-02,  -6.11010306e-02, ...,\n",
      "           -3.79558653e-02,  -4.06549759e-02,  -7.97340423e-02],\n",
      "         [  9.96500440e-03,   5.08636348e-02,   7.32045397e-02, ...,\n",
      "            8.28556530e-03,  -3.54502611e-02,   9.74562764e-02],\n",
      "         [  1.07551888e-01,   1.17199980e-02,   1.18499890e-01, ...,\n",
      "            9.80330352e-03,  -6.36508595e-03,   3.81585769e-03]],\n",
      "\n",
      "        [[ -5.68338595e-02,   3.59817073e-02,   5.03786020e-02, ...,\n",
      "            1.34070348e-02,  -8.21161568e-02,  -1.07147554e-02],\n",
      "         [  4.35331613e-02,  -1.03395665e-02,   4.84904274e-02, ...,\n",
      "            4.38889079e-02,   7.72757903e-02,   1.04683876e-01],\n",
      "         [  1.17996082e-01,   3.65897343e-02,  -3.63575071e-02, ...,\n",
      "           -9.42302588e-03,  -6.42398670e-02,  -9.80756506e-02]],\n",
      "\n",
      "        [[ -3.89925204e-02,   1.21511063e-02,   6.96534216e-02, ...,\n",
      "            6.51760250e-02,   2.77065001e-02,   7.90851787e-02],\n",
      "         [ -3.49739157e-02,  -1.18168704e-01,   4.53177169e-02, ...,\n",
      "            3.87537107e-02,  -3.46393064e-02,   1.18345343e-01],\n",
      "         [  3.55992727e-02,  -4.49449047e-02,  -4.00469601e-02, ...,\n",
      "           -1.57580338e-02,  -9.28946212e-02,   2.33857688e-02]]],\n",
      "\n",
      "\n",
      "       [[[  7.87142590e-02,  -7.49542341e-02,   8.59181881e-02, ...,\n",
      "            5.92627674e-02,   7.50857219e-02,   1.12826414e-01],\n",
      "         [  8.18666145e-02,   4.56172377e-02,  -9.43103135e-02, ...,\n",
      "            6.56536892e-02,   8.98926258e-02,  -1.45079661e-02],\n",
      "         [ -9.07663926e-02,  -1.20226227e-01,  -9.27164331e-02, ...,\n",
      "           -9.94378403e-02,  -1.47800893e-06,   1.02100939e-01]],\n",
      "\n",
      "        [[ -7.99955800e-02,   2.78235953e-02,  -9.76201743e-02, ...,\n",
      "            6.49992004e-02,  -7.73728732e-03,   2.43381038e-02],\n",
      "         [  6.28766418e-02,   4.14928757e-02,  -1.25017166e-02, ...,\n",
      "           -5.02670333e-02,   3.60404663e-02,   7.67069459e-02],\n",
      "         [  1.67844333e-02,  -1.94059461e-02,   4.90270667e-02, ...,\n",
      "           -5.32458574e-02,  -3.71608809e-02,  -9.83828157e-02]],\n",
      "\n",
      "        [[  1.09396979e-01,  -9.15244222e-02,  -5.44343889e-02, ...,\n",
      "            5.37374765e-02,  -2.94019580e-02,  -8.15010220e-02],\n",
      "         [  8.18340406e-02,   1.51225990e-02,  -1.18728474e-01, ...,\n",
      "            5.46977762e-03,  -8.88876468e-02,   8.19046348e-02],\n",
      "         [  8.52434933e-02,   4.14111391e-02,   4.88489754e-02, ...,\n",
      "           -3.92867066e-02,   3.14796232e-02,   9.20945928e-02]],\n",
      "\n",
      "        [[ -7.70370290e-02,  -8.65865778e-03,   2.35295817e-02, ...,\n",
      "            8.95665661e-02,   7.22724199e-02,  -9.95708779e-02],\n",
      "         [ -6.45709485e-02,  -2.67246328e-02,   7.07176998e-02, ...,\n",
      "           -6.13581724e-02,  -7.78834522e-02,   1.14698589e-01],\n",
      "         [ -2.94430777e-02,  -7.41297845e-03,   7.16479346e-02, ...,\n",
      "           -1.12469368e-01,   2.83048749e-02,   4.53514159e-02]],\n",
      "\n",
      "        [[  1.16270207e-01,  -2.63458565e-02,   5.30289039e-02, ...,\n",
      "            2.69282516e-02,   1.79278553e-02,   4.47299071e-02],\n",
      "         [  1.05727211e-01,   1.08279623e-01,  -7.68516660e-02, ...,\n",
      "            1.65065750e-02,  -2.93835327e-02,   1.15708642e-01],\n",
      "         [ -9.31074247e-02,   8.02187622e-02,  -9.20056626e-02, ...,\n",
      "            1.92552321e-02,  -2.13401876e-02,  -6.41951784e-02]]]], dtype=float32), array([ 0.00999923, -0.00999965,  0.00999979, -0.00999986,  0.00999984,\n",
      "       -0.00991977,  0.00999693, -0.00999968,  0.00999974, -0.00999932,\n",
      "       -0.00997979,  0.0099998 , -0.00999977, -0.0099994 ,  0.00999302,\n",
      "        0.00999964], dtype=float32), array([[[[  4.75437194e-03,  -1.87337212e-03,  -1.67268403e-02, ...,\n",
      "           -2.77940091e-03,   1.72735192e-04,   1.10811861e-02],\n",
      "         [ -1.17732044e-02,  -1.43767856e-02,  -2.15689875e-02, ...,\n",
      "            1.59624554e-02,   6.91991299e-04,   4.36342414e-03],\n",
      "         [  7.46113388e-03,   7.29004247e-03,  -1.34521732e-02, ...,\n",
      "            3.05537321e-03,  -1.10343564e-03,  -4.85116988e-03],\n",
      "         ..., \n",
      "         [ -3.31447972e-03,   4.36149910e-03,   1.64348669e-02, ...,\n",
      "            2.12387443e-02,   1.66525766e-02,   1.20230475e-02],\n",
      "         [  5.93438651e-03,   9.09798872e-03,  -2.05125809e-02, ...,\n",
      "           -5.94924670e-04,   3.19653563e-03,  -1.97444391e-03],\n",
      "         [  8.91009811e-04,  -1.12491194e-03,  -1.95413195e-02, ...,\n",
      "            1.80811025e-02,   1.19847870e-02,  -3.50738689e-03]],\n",
      "\n",
      "        [[  4.01028711e-03,   1.86794102e-02,  -1.52752437e-02, ...,\n",
      "            2.31661610e-02,   6.35480508e-04,   1.33367786e-02],\n",
      "         [ -1.84988976e-02,   1.00020552e-02,   2.88568623e-03, ...,\n",
      "            2.30978560e-02,   4.81576193e-03,  -8.31079762e-03],\n",
      "         [  1.77322291e-02,   4.59720800e-03,  -1.80985294e-02, ...,\n",
      "           -1.96249299e-02,   2.04125978e-02,  -1.20561570e-02],\n",
      "         ..., \n",
      "         [  1.19698457e-02,   1.17130270e-02,  -8.09676386e-03, ...,\n",
      "            7.51403999e-03,   8.77340045e-03,  -6.76690415e-03],\n",
      "         [  9.38977394e-03,  -8.31387937e-04,   8.96609947e-03, ...,\n",
      "            2.12969929e-02,   6.56833500e-03,  -1.35081522e-02],\n",
      "         [  9.47763212e-04,   1.38867144e-02,  -2.37117149e-02, ...,\n",
      "            9.22399946e-03,   4.99732513e-03,  -8.79813358e-03]],\n",
      "\n",
      "        [[  1.58882830e-02,   6.54825289e-03,  -1.89782940e-02, ...,\n",
      "            3.95234115e-03,   2.44035479e-02,  -1.03992112e-02],\n",
      "         [ -1.25812069e-02,   1.39401434e-02,  -1.36107579e-02, ...,\n",
      "           -3.15892138e-03,   2.37433948e-02,  -1.03250053e-02],\n",
      "         [  9.42286383e-03,   9.02914256e-03,   1.59623139e-02, ...,\n",
      "           -1.58393141e-02,   1.38410870e-02,   3.62417661e-04],\n",
      "         ..., \n",
      "         [  8.25967267e-03,   9.33929719e-03,   3.61461937e-03, ...,\n",
      "           -1.49913318e-02,   2.36899480e-02,   1.47483787e-02],\n",
      "         [ -1.10272290e-02,  -2.04701163e-03,  -1.77555792e-02, ...,\n",
      "            4.13481332e-03,  -2.21868400e-02,  -1.28109213e-02],\n",
      "         [  2.22778656e-02,   6.89175539e-03,  -2.37259995e-02, ...,\n",
      "           -2.38636658e-02,   1.23429913e-02,  -6.89143222e-03]],\n",
      "\n",
      "        ..., \n",
      "        [[  1.53425159e-02,  -2.07482856e-02,   3.58272158e-03, ...,\n",
      "           -8.68696719e-03,   1.83229744e-02,   1.55312847e-03],\n",
      "         [ -3.16657778e-03,   1.59799233e-02,   2.22826917e-02, ...,\n",
      "           -1.41937518e-02,   3.67377512e-03,  -1.11086061e-02],\n",
      "         [  1.93081312e-02,  -9.38273035e-03,  -1.48958359e-02, ...,\n",
      "           -1.50996819e-03,   1.27230557e-02,  -2.42416747e-04],\n",
      "         ..., \n",
      "         [  9.61024780e-03,  -7.46251736e-03,   1.72868259e-02, ...,\n",
      "           -2.28568763e-02,   1.37433708e-02,  -3.09920032e-03],\n",
      "         [  5.48482593e-03,   7.89796654e-03,  -1.46755986e-02, ...,\n",
      "            4.89573739e-03,  -1.13975490e-02,   3.87584325e-03],\n",
      "         [  1.43746054e-02,   3.51314805e-03,   9.86973569e-03, ...,\n",
      "           -2.21306216e-02,  -2.03691423e-03,   3.53129860e-03]],\n",
      "\n",
      "        [[  2.79763434e-03,  -2.47294791e-02,   3.48967686e-03, ...,\n",
      "           -2.01218650e-02,   2.36209054e-02,  -2.25548446e-02],\n",
      "         [  1.31466612e-02,   1.05932616e-02,   1.56025775e-03, ...,\n",
      "           -8.12059268e-04,   1.18152974e-02,  -1.55494092e-02],\n",
      "         [ -2.38156971e-03,  -6.24330901e-03,  -1.06203146e-02, ...,\n",
      "           -3.32153402e-04,   6.66723121e-03,  -8.38896260e-03],\n",
      "         ..., \n",
      "         [ -4.17004526e-03,  -2.96235085e-05,   4.87631001e-03, ...,\n",
      "           -2.11674944e-02,   1.69464108e-03,  -7.98792392e-03],\n",
      "         [ -4.76223230e-03,  -7.02878600e-03,  -1.75989643e-02, ...,\n",
      "           -9.46695730e-03,  -1.52620487e-04,   9.76081099e-03],\n",
      "         [  1.85686927e-02,  -7.67584983e-03,  -2.56120041e-03, ...,\n",
      "            8.51235352e-04,  -4.35935240e-03,   3.77238449e-03]],\n",
      "\n",
      "        [[  1.72115676e-02,  -9.85377189e-03,  -2.18264647e-02, ...,\n",
      "           -1.67378373e-02,   5.87365590e-04,  -1.56770516e-02],\n",
      "         [ -7.77281821e-04,  -4.95774765e-03,  -1.77538879e-02, ...,\n",
      "           -2.28219535e-02,  -3.41178291e-03,   2.83696596e-03],\n",
      "         [  9.10973176e-04,  -2.33586468e-02,  -2.03045346e-02, ...,\n",
      "           -2.11865976e-02,  -3.25032882e-03,  -1.76441055e-02],\n",
      "         ..., \n",
      "         [  1.97893512e-02,  -1.23958010e-02,   1.90562047e-02, ...,\n",
      "           -2.20796745e-03,   2.34603472e-02,  -1.96498670e-02],\n",
      "         [  1.84419565e-03,  -1.72347315e-02,  -7.35567044e-03, ...,\n",
      "            2.06968486e-02,   5.48424385e-03,  -1.17374584e-03],\n",
      "         [ -5.10260183e-03,  -1.39766578e-02,  -8.82388745e-03, ...,\n",
      "           -6.82547502e-03,   1.04229217e-02,   1.19135715e-03]]],\n",
      "\n",
      "\n",
      "       [[[ -4.70221974e-03,  -2.13449094e-02,   1.32088456e-02, ...,\n",
      "            2.27617584e-02,   1.25681655e-02,   2.63021421e-03],\n",
      "         [  5.05437888e-03,   1.45289181e-02,   7.82435387e-03, ...,\n",
      "           -3.79952602e-03,   8.03755317e-03,   1.08975330e-02],\n",
      "         [  1.00182686e-02,   7.16198981e-03,   2.07937546e-02, ...,\n",
      "           -1.31827965e-03,   2.41064858e-02,  -9.26053524e-03],\n",
      "         ..., \n",
      "         [  5.92051074e-05,   1.59640703e-02,  -3.53662949e-03, ...,\n",
      "            1.89241506e-02,   8.32089782e-03,   1.11955227e-02],\n",
      "         [ -2.81680375e-03,   1.05087934e-02,   2.76337378e-03, ...,\n",
      "           -2.68016011e-03,   1.91729590e-02,  -1.71062350e-03],\n",
      "         [  7.66803045e-03,  -2.05417573e-02,   1.26225781e-03, ...,\n",
      "            1.90524384e-02,   2.21639872e-02,   2.49718316e-04]],\n",
      "\n",
      "        [[  6.52046222e-03,  -5.37570659e-03,   1.15053151e-02, ...,\n",
      "            3.16888466e-03,   2.22811336e-03,   1.02224322e-02],\n",
      "         [  1.63039733e-02,   9.71150119e-03,  -3.14296689e-03, ...,\n",
      "            8.66982527e-03,  -4.21915948e-03,  -8.83954205e-03],\n",
      "         [  4.36631963e-05,   1.95813626e-02,  -1.25513738e-02, ...,\n",
      "           -1.63547453e-02,   1.47688203e-03,   7.73483608e-03],\n",
      "         ..., \n",
      "         [ -4.49996442e-04,   1.99241545e-02,   2.52266899e-02, ...,\n",
      "            5.72698005e-03,   2.23115701e-02,  -6.38456084e-04],\n",
      "         [ -1.32845072e-02,   6.14988152e-03,   2.19786819e-03, ...,\n",
      "            1.83917526e-02,   1.84323937e-02,   1.48101272e-02],\n",
      "         [  2.33493485e-02,   1.40038840e-02,   1.76643617e-02, ...,\n",
      "            1.45605765e-04,   1.15251448e-03,  -1.18440501e-02]],\n",
      "\n",
      "        [[  2.08125487e-02,   2.18831562e-02,   1.92850493e-02, ...,\n",
      "           -1.05872452e-02,   1.43017396e-02,  -1.44951642e-02],\n",
      "         [  1.51482234e-02,   1.56592950e-02,   1.28896907e-02, ...,\n",
      "           -1.19784903e-02,   6.00317959e-03,  -2.29399279e-03],\n",
      "         [ -5.41928317e-03,   2.37077810e-02,   2.46308856e-02, ...,\n",
      "           -1.98002718e-03,   1.07465507e-02,   1.22839501e-02],\n",
      "         ..., \n",
      "         [  7.31623452e-03,   1.37340045e-02,   1.11142006e-02, ...,\n",
      "           -1.97183602e-02,  -2.67554447e-03,  -1.21696386e-03],\n",
      "         [  9.17560887e-04,   1.43422866e-02,   4.00491431e-03, ...,\n",
      "            2.36548930e-02,   2.42940430e-03,  -6.92818314e-03],\n",
      "         [  2.24970430e-02,  -4.82000131e-03,   1.23873977e-02, ...,\n",
      "           -2.20180359e-02,   2.22108476e-02,  -2.04088353e-03]],\n",
      "\n",
      "        ..., \n",
      "        [[  9.28751100e-03,   3.35346907e-04,   5.00252470e-03, ...,\n",
      "           -2.27097683e-02,  -2.85084173e-03,  -2.16958653e-02],\n",
      "         [  1.53776975e-02,   1.55176148e-02,   1.24165211e-02, ...,\n",
      "           -4.79076244e-03,   1.43259699e-02,  -1.26719289e-02],\n",
      "         [ -1.26770977e-03,  -1.22164050e-02,   2.33877208e-02, ...,\n",
      "           -3.33990995e-03,   1.56350210e-02,  -6.71807583e-03],\n",
      "         ..., \n",
      "         [ -1.35295279e-03,  -1.76959019e-02,  -1.19094085e-03, ...,\n",
      "            2.05314830e-02,  -3.11627053e-04,  -2.31417529e-02],\n",
      "         [ -1.00412453e-02,  -1.39574520e-04,   2.01728777e-03, ...,\n",
      "           -4.22954652e-03,  -7.92555511e-03,  -1.47812217e-02],\n",
      "         [  1.58968866e-02,  -2.16461662e-02,   6.60642516e-03, ...,\n",
      "           -1.41619705e-02,   2.20328160e-02,   4.60932124e-03]],\n",
      "\n",
      "        [[  4.50258423e-03,  -2.41808202e-02,   1.98941585e-02, ...,\n",
      "           -3.53202038e-03,   1.63427740e-02,  -2.14288030e-02],\n",
      "         [ -3.38620972e-03,   7.99086317e-03,   5.08471485e-03, ...,\n",
      "           -1.77738443e-02,   2.32072212e-02,  -1.69083327e-02],\n",
      "         [  2.06360202e-02,  -1.95846222e-02,   1.95848271e-02, ...,\n",
      "            5.50039485e-03,   1.82772614e-02,  -2.74131913e-03],\n",
      "         ..., \n",
      "         [ -1.53717492e-03,   3.59675661e-03,   1.77407749e-02, ...,\n",
      "           -1.09823598e-02,   1.13012046e-02,  -6.88356347e-03],\n",
      "         [  2.12932471e-03,   3.46130785e-03,   1.84738487e-02, ...,\n",
      "            3.47901881e-03,  -1.29867196e-02,   3.45714856e-03],\n",
      "         [  2.96294782e-03,  -3.96291539e-03,   1.54910162e-02, ...,\n",
      "           -1.77737642e-02,   7.89749529e-03,  -1.58089250e-02]],\n",
      "\n",
      "        [[  1.04692159e-02,  -1.81470066e-02,   3.35828494e-03, ...,\n",
      "            2.09795218e-03,   6.76540192e-03,  -4.27187327e-03],\n",
      "         [ -2.48164218e-03,   2.14162748e-02,   7.49805756e-03, ...,\n",
      "            3.97168100e-04,   2.25421302e-02,  -1.17476676e-02],\n",
      "         [  2.49890685e-02,  -1.91024393e-02,  -2.41325032e-02, ...,\n",
      "            8.36475752e-03,  -5.38903289e-04,  -2.48338208e-02],\n",
      "         ..., \n",
      "         [ -1.22943614e-03,  -1.40542397e-02,   1.76816825e-02, ...,\n",
      "           -6.91815373e-03,   2.15398036e-02,  -6.92873262e-04],\n",
      "         [ -1.31167900e-02,   3.52213252e-03,   1.95151605e-02, ...,\n",
      "            8.64907168e-03,   1.54771749e-02,  -1.06626395e-02],\n",
      "         [  9.14874766e-03,  -2.42802612e-02,   1.96394585e-02, ...,\n",
      "            6.63839281e-04,  -3.96332704e-03,   7.52872787e-04]]],\n",
      "\n",
      "\n",
      "       [[[  1.39827887e-02,  -1.37038892e-02,  -1.26495613e-02, ...,\n",
      "           -1.76551193e-03,   9.94013809e-03,  -1.36426948e-02],\n",
      "         [ -1.67123452e-02,   7.08850101e-04,  -1.18604638e-02, ...,\n",
      "            1.20925745e-02,   3.49997729e-03,  -4.30007465e-03],\n",
      "         [  1.10348118e-02,  -7.01351278e-03,  -1.34995934e-02, ...,\n",
      "            5.35278395e-03,  -5.03986329e-03,  -2.04605982e-04],\n",
      "         ..., \n",
      "         [ -7.76909292e-06,  -1.16133923e-02,  -2.13833135e-02, ...,\n",
      "           -2.61046458e-03,   1.83901303e-02,   1.31593784e-02],\n",
      "         [ -2.38597859e-03,  -2.94645526e-03,   1.15644429e-02, ...,\n",
      "           -1.92902125e-02,   2.21499149e-02,  -6.55704830e-03],\n",
      "         [  1.68315694e-03,  -1.03061916e-02,   9.78743844e-03, ...,\n",
      "            1.83279812e-02,  -3.55520938e-03,   6.80174213e-03]],\n",
      "\n",
      "        [[ -5.95426280e-03,  -2.32262798e-02,  -2.93109752e-03, ...,\n",
      "           -1.66275166e-02,   1.85093284e-03,   7.88228493e-03],\n",
      "         [  3.80001124e-03,  -6.35903981e-03,   5.20884991e-03, ...,\n",
      "            2.39229128e-02,   1.97513420e-02,  -7.20229652e-03],\n",
      "         [ -2.00521871e-02,   1.00211985e-03,  -3.10921110e-03, ...,\n",
      "           -4.26708255e-03,   1.26737915e-02,   1.94276776e-03],\n",
      "         ..., \n",
      "         [ -5.20902220e-03,  -2.35060360e-02,  -2.88838893e-03, ...,\n",
      "            2.17551924e-02,   2.75140256e-03,   9.73758381e-03],\n",
      "         [ -9.38231125e-04,  -6.21429412e-03,  -2.16880441e-02, ...,\n",
      "            4.47052112e-03,   2.11843215e-02,  -7.23400060e-03],\n",
      "         [ -1.75380632e-02,  -2.25274228e-02,  -5.66397607e-03, ...,\n",
      "            3.13838478e-03,   2.23169029e-02,  -3.27747315e-04]],\n",
      "\n",
      "        [[ -1.15069011e-02,  -2.16344930e-02,  -1.81212090e-02, ...,\n",
      "           -9.35510732e-03,   2.11809389e-02,  -1.12254620e-02],\n",
      "         [ -7.30182230e-03,  -7.58083444e-03,  -1.16133187e-02, ...,\n",
      "           -1.48150586e-02,   5.21055423e-04,   5.37659880e-03],\n",
      "         [ -1.15657682e-02,  -1.96132492e-02,  -7.58346636e-03, ...,\n",
      "            3.47559061e-03,   1.05116908e-02,  -9.79991443e-03],\n",
      "         ..., \n",
      "         [ -1.58563536e-02,  -1.98835246e-02,  -1.48393065e-02, ...,\n",
      "           -1.54063199e-02,  -4.85998578e-03,   1.39145618e-02],\n",
      "         [  3.93898506e-03,  -1.24594001e-02,   4.40199859e-04, ...,\n",
      "            1.15484521e-02,  -7.65121169e-03,   5.06879669e-03],\n",
      "         [  4.70574014e-03,   6.54796138e-04,   4.79159411e-03, ...,\n",
      "           -2.07288656e-02,  -3.87164392e-03,  -1.37892812e-02]],\n",
      "\n",
      "        ..., \n",
      "        [[  1.28435204e-02,  -2.41824631e-02,   2.40708962e-02, ...,\n",
      "           -7.20872451e-03,   6.97644707e-03,  -2.47048289e-02],\n",
      "         [  2.37879995e-03,  -6.64007105e-03,  -6.61613792e-03, ...,\n",
      "           -9.94123705e-03,   7.79904332e-03,  -2.78336369e-03],\n",
      "         [  2.36167992e-03,  -2.94379145e-03,  -2.70523783e-03, ...,\n",
      "           -9.47474502e-04,   7.39229098e-03,  -7.38794450e-03],\n",
      "         ..., \n",
      "         [  1.58344209e-02,   3.97911668e-03,   5.02340496e-03, ...,\n",
      "           -2.27363035e-02,   4.79672570e-03,  -1.64273623e-02],\n",
      "         [  1.43338647e-03,   9.16836131e-03,   1.60918571e-02, ...,\n",
      "            2.97176745e-03,   2.96796206e-03,   1.31967990e-02],\n",
      "         [  3.32481228e-04,   3.94116715e-03,   2.31770352e-02, ...,\n",
      "           -2.65423860e-03,   8.95475410e-03,   4.31650132e-03]],\n",
      "\n",
      "        [[  5.66506013e-03,  -3.32969148e-03,  -1.78095847e-02, ...,\n",
      "            5.11054136e-03,  -2.10451055e-03,  -2.96910014e-03],\n",
      "         [  1.33006023e-02,  -1.40896244e-02,  -2.20877584e-03, ...,\n",
      "           -1.18960310e-02,   2.48869788e-03,  -9.22958739e-03],\n",
      "         [  1.82073265e-02,  -1.79579202e-02,  -1.58221442e-02, ...,\n",
      "           -7.31114484e-03,   1.71606466e-02,  -2.11671293e-02],\n",
      "         ..., \n",
      "         [  1.90715510e-02,  -3.71332560e-03,  -2.36760005e-02, ...,\n",
      "           -4.46851086e-03,   8.59125238e-03,  -1.68595240e-02],\n",
      "         [ -2.26266962e-03,   1.96220353e-04,  -1.22469794e-02, ...,\n",
      "           -2.22483240e-02,  -1.10854134e-02,  -8.96065030e-03],\n",
      "         [ -2.09750421e-03,  -1.38992071e-02,  -4.73726913e-03, ...,\n",
      "           -2.09180769e-02,   5.38240653e-03,  -1.83162428e-02]],\n",
      "\n",
      "        [[  1.83159988e-02,  -2.39794627e-02,   1.86546799e-03, ...,\n",
      "            2.80917622e-03,   8.62010103e-03,   3.21465544e-04],\n",
      "         [  9.82950814e-03,  -5.26532158e-03,  -1.08174123e-02, ...,\n",
      "           -1.73136648e-02,   1.12473648e-02,  -6.47318456e-03],\n",
      "         [  4.46164235e-03,  -2.16192901e-02,  -1.85532384e-02, ...,\n",
      "           -1.38500584e-02,   1.75072197e-02,  -1.45677524e-02],\n",
      "         ..., \n",
      "         [ -1.47484615e-03,  -8.78728647e-03,   9.84565355e-04, ...,\n",
      "           -3.54259554e-03,   1.06777214e-02,  -4.68156300e-03],\n",
      "         [ -3.36917304e-03,  -2.03496702e-02,   3.83242313e-03, ...,\n",
      "            2.19731200e-02,   1.53386118e-02,  -1.01690199e-02],\n",
      "         [  1.80567633e-02,  -2.24923566e-02,  -9.56653804e-03, ...,\n",
      "           -2.11772732e-02,  -2.60861591e-04,  -7.56241567e-03]]],\n",
      "\n",
      "\n",
      "       ..., \n",
      "       [[[  1.51844770e-02,  -2.06907801e-02,  -3.90546024e-03, ...,\n",
      "            1.76330376e-02,  -4.79859766e-03,  -2.60177813e-03],\n",
      "         [  1.38386898e-03,  -1.04388734e-02,   2.14490537e-02, ...,\n",
      "            2.03528386e-02,   2.22248957e-03,   4.83704638e-03],\n",
      "         [  2.41013672e-02,  -1.52907949e-02,  -1.69251710e-02, ...,\n",
      "           -3.25319543e-03,   2.41782721e-02,   9.52665042e-03],\n",
      "         ..., \n",
      "         [ -2.46898551e-03,   1.43262371e-03,  -1.80497207e-03, ...,\n",
      "            3.08233965e-03,  -2.04258412e-03,   9.07140132e-03],\n",
      "         [ -3.80900688e-03,  -1.45709524e-02,   2.23653354e-02, ...,\n",
      "           -1.79516580e-02,   1.04409382e-02,  -5.06579690e-03],\n",
      "         [  1.58193223e-02,   1.16838329e-03,   6.17393572e-03, ...,\n",
      "           -2.90206168e-03,   7.42435362e-03,   6.13414682e-04]],\n",
      "\n",
      "        [[  9.83781926e-03,  -1.17237773e-02,  -1.26359323e-02, ...,\n",
      "           -2.99216248e-04,   1.46217113e-02,  -4.18457203e-03],\n",
      "         [ -2.81106494e-03,  -3.92010622e-03,  -2.09008940e-02, ...,\n",
      "            1.92112420e-02,   2.27382332e-02,   3.54761910e-03],\n",
      "         [ -2.28601508e-03,  -2.12523397e-02,  -2.02541836e-02, ...,\n",
      "           -4.05222643e-03,   4.82764095e-03,   1.52160926e-02],\n",
      "         ..., \n",
      "         [  2.13201884e-02,  -1.97636746e-02,   6.18613698e-03, ...,\n",
      "            2.45318143e-03,   6.23681024e-03,   1.31483087e-02],\n",
      "         [  1.68490689e-02,   1.19837122e-02,   1.94311291e-02, ...,\n",
      "           -4.19502985e-03,  -1.24497730e-02,  -6.38297014e-03],\n",
      "         [  6.40657544e-03,  -6.63193595e-03,   3.91048007e-03, ...,\n",
      "            2.43765451e-02,  -2.88409833e-03,  -5.66104986e-03]],\n",
      "\n",
      "        [[  4.06356528e-03,  -1.78133436e-02,  -7.57249165e-03, ...,\n",
      "            6.75819349e-03,   2.01853812e-02,  -1.28967753e-02],\n",
      "         [  5.31914178e-03,  -8.24051723e-03,  -9.42663196e-03, ...,\n",
      "            2.59295199e-03,  -2.76478939e-03,   1.02389408e-02],\n",
      "         [  3.46373022e-03,  -9.84376296e-04,  -5.72478399e-03, ...,\n",
      "           -4.66029532e-03,   1.16912760e-02,  -1.02544203e-04],\n",
      "         ..., \n",
      "         [ -3.57490033e-04,  -2.29252502e-02,   5.28212357e-03, ...,\n",
      "           -4.87336889e-03,   1.14725102e-02,   1.30007872e-02],\n",
      "         [  5.77941397e-03,   2.31680106e-02,   2.41611768e-02, ...,\n",
      "           -1.83099378e-02,  -1.36895729e-02,   2.11569481e-04],\n",
      "         [  2.31367089e-02,  -1.76367611e-02,  -1.70099754e-02, ...,\n",
      "            1.43043855e-02,   1.10760238e-02,   7.70352315e-03]],\n",
      "\n",
      "        ..., \n",
      "        [[  1.96784176e-02,  -4.96962853e-03,   1.59079507e-02, ...,\n",
      "            7.97335897e-03,   1.07429344e-02,  -1.19012566e-02],\n",
      "         [  1.30185392e-03,  -1.70590580e-02,   1.38687249e-02, ...,\n",
      "            1.65429488e-02,   1.49400719e-03,  -6.48685079e-03],\n",
      "         [  2.48877518e-02,  -1.43112661e-02,   1.76434368e-02, ...,\n",
      "           -2.49400921e-03,  -8.53107311e-04,  -4.03710082e-03],\n",
      "         ..., \n",
      "         [  6.47105742e-03,  -2.13664286e-02,  -9.51515511e-04, ...,\n",
      "           -3.60086467e-03,   3.81445605e-03,  -3.74861713e-03],\n",
      "         [ -4.30609193e-03,   1.33091025e-03,   3.67719773e-03, ...,\n",
      "           -2.10201982e-02,  -2.42191441e-02,  -8.37187935e-03],\n",
      "         [  4.42656130e-03,  -1.42093385e-02,   1.71449296e-02, ...,\n",
      "            2.20640711e-02,   2.41690949e-02,  -1.59387290e-03]],\n",
      "\n",
      "        [[  5.52450214e-03,  -1.87399872e-02,   3.54945939e-03, ...,\n",
      "            4.84430045e-03,   3.24227940e-03,  -2.26758439e-02],\n",
      "         [  1.91576555e-02,  -1.16310725e-02,   1.13442829e-02, ...,\n",
      "            8.79763719e-03,   1.98016893e-02,  -2.48757694e-02],\n",
      "         [ -2.45570112e-03,   3.27414647e-03,  -2.05894839e-02, ...,\n",
      "            4.56260983e-03,  -3.22032813e-03,   1.54872891e-03],\n",
      "         ..., \n",
      "         [  1.20833907e-02,  -1.53586715e-02,   1.40266772e-03, ...,\n",
      "           -4.66297939e-03,  -7.91136175e-04,  -2.86121760e-03],\n",
      "         [  1.21179866e-02,   6.17698487e-03,   2.12933831e-02, ...,\n",
      "           -3.28967813e-03,   1.26817301e-02,  -4.20777500e-03],\n",
      "         [  1.69804655e-02,  -2.23613195e-02,   8.96953046e-03, ...,\n",
      "            1.66811403e-02,  -1.92569476e-03,  -2.88898218e-03]],\n",
      "\n",
      "        [[  1.72568411e-02,  -1.09788505e-02,  -1.93617716e-02, ...,\n",
      "           -1.21072503e-02,   3.65953054e-03,  -8.26931559e-03],\n",
      "         [  2.11391412e-02,  -1.33930305e-02,  -1.43769383e-03, ...,\n",
      "           -1.76634490e-02,   1.21096028e-02,  -4.28309292e-03],\n",
      "         [  1.31262299e-02,  -2.99150124e-05,  -2.28804909e-02, ...,\n",
      "           -1.31418156e-02,   1.73966400e-02,  -2.12621111e-02],\n",
      "         ..., \n",
      "         [ -7.20136799e-04,  -2.92500760e-03,  -6.32272940e-03, ...,\n",
      "            2.50348803e-02,  -4.83545847e-03,  -1.23514701e-03],\n",
      "         [  8.76657013e-03,  -9.87825543e-03,   1.92475263e-02, ...,\n",
      "            1.62310898e-04,   6.80000521e-04,  -7.02057127e-03],\n",
      "         [  2.11168043e-02,  -1.12759955e-02,  -2.21600942e-03, ...,\n",
      "            2.38044281e-03,   3.25852539e-03,  -2.26085000e-02]]],\n",
      "\n",
      "\n",
      "       [[[  1.11257378e-02,  -5.23926690e-04,  -1.59181617e-02, ...,\n",
      "            1.24937138e-02,   1.80321205e-02,   1.21917231e-02],\n",
      "         [  2.04009563e-02,  -1.27401361e-02,   8.00324138e-03, ...,\n",
      "            1.33287162e-02,   1.54257212e-02,  -2.27955170e-04],\n",
      "         [  1.54739246e-05,  -1.44784730e-02,  -8.03432520e-03, ...,\n",
      "            8.33916105e-03,  -3.14119365e-03,   1.77853648e-03],\n",
      "         ..., \n",
      "         [  1.76316872e-02,  -1.77818760e-02,  -1.85179487e-02, ...,\n",
      "            2.10971460e-02,   9.41735879e-03,  -5.61273843e-03],\n",
      "         [  1.26757761e-02,  -7.89524987e-04,  -8.15025717e-03, ...,\n",
      "           -1.97998807e-02,   1.15030957e-02,   1.00165410e-02],\n",
      "         [  7.16209784e-03,   3.39490827e-03,   1.06931962e-02, ...,\n",
      "            1.77041255e-02,   2.27386039e-03,   3.36199533e-03]],\n",
      "\n",
      "        [[  1.22917173e-02,   3.83935496e-03,  -2.04232279e-02, ...,\n",
      "           -2.54133716e-03,   1.25881005e-02,   1.17340228e-02],\n",
      "         [  1.62714273e-02,   1.79867540e-03,  -7.82832410e-03, ...,\n",
      "            1.60235930e-02,   1.86998919e-02,   2.89961230e-03],\n",
      "         [ -2.15037130e-02,  -8.25764425e-03,  -1.64178088e-02, ...,\n",
      "            1.21667963e-02,   1.67056322e-02,  -1.00737698e-02],\n",
      "         ..., \n",
      "         [  6.45599328e-03,  -1.68652982e-02,  -2.72345450e-03, ...,\n",
      "            1.99811962e-02,   3.04169394e-03,  -9.57319327e-03],\n",
      "         [  1.45072257e-02,   1.00447377e-02,  -4.87184618e-03, ...,\n",
      "            3.32206953e-03,  -8.37759487e-03,  -1.51032042e-02],\n",
      "         [  1.65324043e-02,  -1.61892846e-02,   4.91890684e-03, ...,\n",
      "           -1.45629048e-04,  -5.07907011e-04,   6.20029029e-03]],\n",
      "\n",
      "        [[  9.84283444e-03,  -3.30084935e-03,   1.12411100e-03, ...,\n",
      "           -1.70100201e-03,   4.57423180e-03,   1.59524102e-03],\n",
      "         [  2.35465188e-02,  -2.23538131e-02,  -9.04742163e-03, ...,\n",
      "            1.43111777e-03,   7.77104869e-05,  -1.37639418e-02],\n",
      "         [  6.51680864e-04,  -2.31448188e-02,  -1.72217824e-02, ...,\n",
      "            2.19237059e-04,   2.51146518e-02,  -1.27017554e-02],\n",
      "         ..., \n",
      "         [  1.84431095e-02,  -2.94684060e-03,  -2.08261348e-02, ...,\n",
      "            1.66360624e-02,   1.19049139e-02,  -6.43538032e-03],\n",
      "         [  9.69023909e-03,  -8.07094947e-03,  -2.28009708e-02, ...,\n",
      "           -2.46333331e-03,  -1.86423641e-02,  -8.16021487e-03],\n",
      "         [  1.05184121e-02,  -6.30147196e-03,   5.00411633e-03, ...,\n",
      "            9.48196556e-03,   1.83956157e-02,  -1.47433607e-02]],\n",
      "\n",
      "        ..., \n",
      "        [[  2.27231514e-02,  -1.33423582e-02,   8.91460758e-03, ...,\n",
      "           -5.06135635e-04,  -7.94412568e-04,  -1.89858302e-02],\n",
      "         [  1.01896925e-02,  -8.99384171e-03,  -2.21786648e-03, ...,\n",
      "            2.01057903e-02,  -2.46766582e-03,  -2.59166304e-03],\n",
      "         [  1.15290312e-02,  -1.56798400e-02,   2.00889446e-02, ...,\n",
      "            4.71362565e-03,   1.88824125e-02,  -1.79460198e-02],\n",
      "         ..., \n",
      "         [  1.32881012e-02,  -2.02090964e-02,   4.95634042e-03, ...,\n",
      "            2.20123418e-02,   6.11979514e-03,  -6.34970237e-03],\n",
      "         [ -4.88104019e-03,   6.04909752e-03,  -2.10030377e-02, ...,\n",
      "            1.60307735e-02,  -1.78893469e-02,   3.21600493e-03],\n",
      "         [  1.37567809e-02,   7.06436113e-04,   1.29525550e-04, ...,\n",
      "            9.49834846e-03,   1.52219953e-02,  -2.81483680e-03]],\n",
      "\n",
      "        [[ -1.63771026e-03,   1.20484270e-03,  -3.93061154e-03, ...,\n",
      "            2.30607018e-02,   6.50323182e-03,  -8.46916158e-03],\n",
      "         [  1.66071411e-02,  -2.30039302e-02,  -3.23902816e-03, ...,\n",
      "           -1.63647048e-02,   1.08474242e-02,  -2.66072899e-03],\n",
      "         [  1.22825103e-02,  -2.43353359e-02,   1.15084462e-02, ...,\n",
      "            3.80744971e-03,   1.95037238e-02,  -2.72196811e-03],\n",
      "         ..., \n",
      "         [  1.84339024e-02,  -4.75358032e-03,  -2.34123133e-02, ...,\n",
      "           -3.29833478e-03,  -4.33422811e-03,   5.56431059e-03],\n",
      "         [ -8.55080783e-03,  -1.66354328e-03,  -6.06563874e-03, ...,\n",
      "            1.88480094e-02,   2.37931050e-02,  -1.89191047e-02],\n",
      "         [  9.55023337e-03,  -1.73578896e-02,   1.97100267e-03, ...,\n",
      "           -2.50547659e-03,   6.57303445e-03,  -2.43099928e-02]],\n",
      "\n",
      "        [[  1.61177628e-02,  -1.22910924e-02,   1.25541631e-03, ...,\n",
      "            1.34584541e-02,   5.02087828e-03,  -1.14692878e-02],\n",
      "         [  8.99915490e-03,  -1.30619947e-03,  -9.88995377e-03, ...,\n",
      "           -1.67699512e-02,   3.38384416e-03,  -2.83292588e-03],\n",
      "         [  1.20687662e-02,  -7.60896876e-03,  -2.24171206e-02, ...,\n",
      "           -4.13462706e-03,  -1.80879422e-03,  -1.72237959e-03],\n",
      "         ..., \n",
      "         [  9.07176733e-03,   2.88282521e-03,   1.86584294e-02, ...,\n",
      "            1.76501516e-02,  -8.65577720e-04,   1.61728561e-02],\n",
      "         [ -6.87046908e-04,   1.00705819e-02,  -1.37750078e-02, ...,\n",
      "            2.80770939e-03,   6.22980110e-03,  -2.21064202e-02],\n",
      "         [  1.61709450e-03,   1.20393932e-03,  -3.72190028e-04, ...,\n",
      "            9.99880023e-03,   2.42629722e-02,  -1.02728419e-02]]],\n",
      "\n",
      "\n",
      "       [[[  3.52590065e-03,   3.33177019e-03,   2.27084644e-02, ...,\n",
      "            1.27005391e-02,   2.19972171e-02,   1.08078504e-02],\n",
      "         [  4.60855663e-05,  -7.17482995e-03,   3.15495580e-03, ...,\n",
      "            1.70562714e-02,   1.87059641e-02,  -1.48819257e-02],\n",
      "         [ -4.16623894e-03,  -1.50120622e-02,  -1.35708414e-02, ...,\n",
      "            1.50293605e-02,  -9.10407491e-03,  -7.23006111e-03],\n",
      "         ..., \n",
      "         [  1.79247279e-02,  -1.48667786e-02,   6.31636009e-04, ...,\n",
      "           -2.45061889e-03,   8.30046553e-03,  -1.35804638e-02],\n",
      "         [ -2.91363336e-03,   5.84348198e-03,   9.85922851e-03, ...,\n",
      "           -7.26750121e-04,   1.73601210e-02,  -8.38074461e-03],\n",
      "         [ -1.56125054e-03,   1.42186880e-04,   1.22438744e-02, ...,\n",
      "           -4.37197182e-03,   2.02342663e-02,  -8.29459354e-03]],\n",
      "\n",
      "        [[ -4.41588834e-03,  -8.59723520e-03,  -1.57890953e-02, ...,\n",
      "            9.49547812e-03,   1.82743296e-02,  -1.50234001e-02],\n",
      "         [  4.00635414e-03,  -7.57787470e-03,   1.98015384e-02, ...,\n",
      "            7.89473392e-03,   1.11327963e-02,  -9.39057581e-03],\n",
      "         [  1.06921233e-03,  -1.06364982e-02,  -2.15873122e-02, ...,\n",
      "           -1.13278739e-02,   9.52983461e-03,   1.09214801e-03],\n",
      "         ..., \n",
      "         [  1.38603421e-02,  -9.85905528e-03,   2.70782970e-04, ...,\n",
      "            1.91348754e-02,   1.41369635e-02,  -5.83541486e-03],\n",
      "         [ -1.45961251e-02,   7.02864770e-03,  -9.28615779e-03, ...,\n",
      "            3.00572440e-03,   4.99202777e-03,   3.29186674e-03],\n",
      "         [  7.79514574e-03,  -9.22078453e-03,   2.14277618e-02, ...,\n",
      "           -4.48888354e-03,   1.25704939e-02,   7.23114330e-03]],\n",
      "\n",
      "        [[  1.75313745e-03,  -1.68390311e-02,   2.25351378e-03, ...,\n",
      "           -3.83674726e-03,   1.54200867e-02,  -1.05827553e-02],\n",
      "         [  2.33300738e-02,  -1.58597995e-03,  -1.44596929e-02, ...,\n",
      "            7.85822980e-04,   1.39082652e-02,   1.61321927e-03],\n",
      "         [  7.49115739e-03,  -1.20960986e-02,  -3.57761048e-03, ...,\n",
      "            1.06768403e-03,   4.08795662e-03,  -7.63662625e-03],\n",
      "         ..., \n",
      "         [  4.00354248e-03,  -5.70651144e-03,  -2.49998197e-02, ...,\n",
      "            1.06534921e-03,   2.04818286e-02,  -1.30777592e-02],\n",
      "         [  1.58307403e-02,  -5.14238048e-03,  -1.33460285e-02, ...,\n",
      "           -1.14117758e-02,   2.16843970e-02,   1.45176528e-02],\n",
      "         [  3.66842560e-03,   3.19261011e-03,  -1.00628901e-02, ...,\n",
      "            2.94375420e-03,  -1.09431613e-03,  -6.34795986e-04]],\n",
      "\n",
      "        ..., \n",
      "        [[  2.44630575e-02,  -1.73811950e-02,   1.12713156e-02, ...,\n",
      "            9.82553139e-03,   8.51375517e-03,  -8.78584571e-04],\n",
      "         [  8.05623829e-03,  -2.18615122e-02,  -2.78978236e-03, ...,\n",
      "            2.12317109e-02,   9.74352099e-03,  -1.78642850e-02],\n",
      "         [  2.47160383e-02,  -1.05022639e-03,   4.03439440e-03, ...,\n",
      "            1.25684906e-02,   1.76879503e-02,   4.23947349e-05],\n",
      "         ..., \n",
      "         [  3.05704772e-04,  -1.84526574e-02,  -2.02524010e-02, ...,\n",
      "            1.34734968e-02,   2.83575431e-03,   2.37210207e-02],\n",
      "         [  5.97718172e-04,  -4.90568439e-03,   2.91057024e-03, ...,\n",
      "            2.05016788e-03,  -2.42940802e-03,   5.28599974e-03],\n",
      "         [  2.86703184e-03,  -1.56935528e-02,   2.52093188e-02, ...,\n",
      "            7.89800752e-03,   2.18611397e-02,  -1.89603977e-02]],\n",
      "\n",
      "        [[  1.97026432e-02,  -2.02309061e-03,   2.25978401e-02, ...,\n",
      "           -3.91762052e-03,  -3.65424994e-03,  -8.64076149e-03],\n",
      "         [  2.13516913e-02,  -2.47503035e-02,   2.32002437e-02, ...,\n",
      "            9.26344655e-04,   1.86918918e-02,   2.38699932e-03],\n",
      "         [ -3.80381942e-03,  -2.46897731e-02,  -2.02157907e-02, ...,\n",
      "            1.71723962e-03,   1.82540212e-02,  -2.20568106e-02],\n",
      "         ..., \n",
      "         [  1.75661184e-02,  -2.67448556e-03,  -1.44142937e-02, ...,\n",
      "           -4.56167944e-03,   1.61684584e-02,  -1.91755127e-03],\n",
      "         [  1.78327952e-02,  -1.87249277e-02,   1.19786076e-02, ...,\n",
      "           -1.30002815e-02,   1.20844338e-02,  -1.45367542e-02],\n",
      "         [  2.72090733e-03,   3.97721399e-03,   2.01881118e-02, ...,\n",
      "            2.21416801e-02,   5.68346120e-03,   5.75427897e-03]],\n",
      "\n",
      "        [[ -1.93308480e-03,  -1.13923056e-02,   1.96448807e-03, ...,\n",
      "            8.96530598e-03,   1.22765191e-02,  -1.34497462e-02],\n",
      "         [  3.49334255e-03,  -1.60854943e-02,  -1.06789330e-02, ...,\n",
      "            2.99009308e-03,   7.43620284e-03,  -4.13339119e-03],\n",
      "         [ -1.15769263e-03,  -1.05686635e-02,  -9.24089178e-03, ...,\n",
      "            3.06002796e-04,   2.46258900e-02,  -1.84421539e-02],\n",
      "         ..., \n",
      "         [  1.92992575e-03,  -1.68926008e-02,   2.39700358e-02, ...,\n",
      "           -1.71344280e-02,   6.73765223e-03,   1.67772658e-02],\n",
      "         [ -1.09444074e-02,  -1.48347896e-02,  -4.66627534e-03, ...,\n",
      "           -1.33294333e-02,   8.74530803e-03,  -2.05719844e-05],\n",
      "         [  2.51367055e-02,  -3.65078356e-03,   1.84348579e-02, ...,\n",
      "            8.00029002e-03,   1.77069269e-02,  -1.92086101e-02]]]], dtype=float32), array([ 0.00999971, -0.00999967, -0.00999954,  0.00999991, -0.00999952,\n",
      "        0.00999354,  0.00999975, -0.00999987, -0.00999979,  0.00999864,\n",
      "       -0.00999946, -0.00999985, -0.00999899,  0.00999993, -0.00999993,\n",
      "        0.00999993, -0.00999995, -0.00999989,  0.00999993,  0.00999992,\n",
      "        0.00999976, -0.00999987,  0.00990854, -0.00999978, -0.0099973 ,\n",
      "        0.00999983, -0.00999763, -0.00999934, -0.00999979, -0.00999954,\n",
      "        0.00999944,  0.00999991,  0.00999781,  0.00999989,  0.00999969,\n",
      "        0.00999983,  0.00999996,  0.00999979,  0.0099997 , -0.00999982,\n",
      "       -0.0099999 , -0.00999956, -0.00999945, -0.00999976, -0.00999956,\n",
      "        0.00999902,  0.00999963,  0.00999657, -0.00999967,  0.00999986,\n",
      "        0.00999971,  0.00999989, -0.00999986, -0.00999977,  0.00999974,\n",
      "       -0.00999991,  0.0099992 , -0.00999888,  0.00999989, -0.00999906,\n",
      "        0.00999978, -0.00999983,  0.00999786, -0.00999991,  0.0099999 ,\n",
      "        0.00999982, -0.00999981,  0.00999957,  0.00999968, -0.00999958,\n",
      "        0.00999953,  0.00998071,  0.00999989,  0.00998924, -0.00999971,\n",
      "       -0.00999983,  0.00999945, -0.00999933,  0.00999991, -0.00999968,\n",
      "       -0.00999979,  0.00999973,  0.00999962, -0.00999996,  0.00999844,\n",
      "       -0.00999977,  0.00999954,  0.00999936,  0.00999971,  0.00999977,\n",
      "        0.00999838,  0.0099999 , -0.00999992,  0.00999963,  0.00999984,\n",
      "       -0.00999989, -0.0099995 ,  0.00999976,  0.0099999 , -0.0099998 ,\n",
      "       -0.00999964, -0.00999968,  0.009993  ,  0.00999991, -0.00999901,\n",
      "        0.00999919, -0.00999969, -0.0099999 ,  0.00999988,  0.00999986,\n",
      "        0.00999961,  0.00999897,  0.00999966, -0.00999971, -0.00999977,\n",
      "       -0.00999693, -0.00999991, -0.00999972, -0.00999948, -0.00999817,\n",
      "        0.00999972, -0.0099647 , -0.0099998 , -0.00999964, -0.00999984,\n",
      "       -0.00996848, -0.00999995, -0.00999993,  0.00999963, -0.00999982,\n",
      "       -0.00999947, -0.0099998 , -0.00999987, -0.00999955, -0.0099997 ,\n",
      "        0.00997338,  0.00999986, -0.00999977,  0.00999992,  0.00999455,\n",
      "       -0.00999992,  0.00999787, -0.00999964,  0.00999955, -0.00999704,\n",
      "        0.00999988,  0.00999916,  0.00999991, -0.00999988, -0.00999955,\n",
      "        0.00999954,  0.00999973, -0.00999941,  0.00999991,  0.00999976,\n",
      "        0.00999812,  0.00999954,  0.00999972,  0.00999948, -0.00999993,\n",
      "        0.00999775,  0.00999994, -0.00999956,  0.00999901,  0.0099998 ,\n",
      "        0.00999988, -0.0099988 ,  0.00999978, -0.0099999 , -0.00999988,\n",
      "       -0.00999975, -0.0099999 , -0.00999979, -0.00999972, -0.00999995,\n",
      "       -0.00999911, -0.0099995 , -0.00999905,  0.00999984, -0.00999993,\n",
      "        0.00999851,  0.00999971,  0.00999984, -0.00999991,  0.0099997 ,\n",
      "        0.00999989,  0.00999898,  0.00992233, -0.0099996 , -0.00999979,\n",
      "        0.00999893,  0.00999988, -0.00999975, -0.00999961, -0.00999242,\n",
      "        0.00996949, -0.00999991, -0.0099999 ,  0.0099997 ,  0.00999885,\n",
      "       -0.00999984, -0.00999967, -0.00999989,  0.00999872, -0.00999934,\n",
      "        0.00999974, -0.00999981,  0.00999967, -0.00999463,  0.00999985,\n",
      "        0.00999822,  0.00999978,  0.00999966, -0.00999947,  0.00999914,\n",
      "       -0.00428353,  0.00999975,  0.0099998 , -0.00999993,  0.00999446,\n",
      "       -0.0099993 ,  0.00999969, -0.00999966, -0.00999987,  0.00999988,\n",
      "        0.00999987,  0.00999973,  0.00999984,  0.00999993,  0.00999987,\n",
      "        0.00999994, -0.00999942,  0.00999932,  0.00999996,  0.00999981,\n",
      "       -0.00999943, -0.00999964, -0.00999967, -0.00999685, -0.009997  ,\n",
      "       -0.00999663,  0.00999964, -0.00999993,  0.00999988, -0.00999954,\n",
      "       -0.00999973, -0.00999973, -0.00999973,  0.00999985,  0.00999973,\n",
      "       -0.00999906, -0.00999946, -0.00999995, -0.00999973, -0.00997109,\n",
      "        0.00999923, -0.00998869,  0.00998506,  0.00999982,  0.00999845,\n",
      "        0.00999991, -0.00998948, -0.00999966, -0.00999992, -0.00999981,\n",
      "       -0.00999774,  0.00999981,  0.00999976, -0.009992  , -0.00999965,\n",
      "       -0.00999978, -0.00999986,  0.00999982, -0.00999977,  0.00999984,\n",
      "        0.00999981, -0.00999929, -0.00999829, -0.00999939,  0.00999842,\n",
      "       -0.00999986,  0.00999959,  0.00999985,  0.00999966,  0.00999975,\n",
      "       -0.00995233,  0.00999993, -0.00999977, -0.00999724,  0.0099999 ,\n",
      "        0.0099996 ,  0.00999983,  0.00999542, -0.00999991,  0.00999981,\n",
      "        0.00999938, -0.00999989,  0.00999994, -0.00999836, -0.00999961,\n",
      "       -0.00999951,  0.00999886,  0.00999977, -0.00999929, -0.00999985,\n",
      "        0.00999971,  0.00999982,  0.00999988, -0.00999155,  0.00999971,\n",
      "        0.00999742,  0.00999986,  0.00999981,  0.0099947 , -0.0099988 ,\n",
      "        0.00999939,  0.00999926,  0.00999938,  0.00999922,  0.00999673,\n",
      "        0.00999959, -0.00999974,  0.00999966,  0.00999919,  0.00999983,\n",
      "       -0.00999883, -0.00999926,  0.00999977,  0.00999993, -0.00999983,\n",
      "        0.00999975, -0.00999194, -0.00999991, -0.00891307,  0.00999949,\n",
      "       -0.00999917,  0.00999943, -0.00999876,  0.00999959,  0.00999996,\n",
      "       -0.00999994, -0.009999  ,  0.00999979, -0.00999972, -0.00999989,\n",
      "       -0.00996651, -0.00999981, -0.00997417, -0.00998428, -0.00999992,\n",
      "        0.0099994 ,  0.00999892,  0.00999994, -0.0099999 , -0.0099993 ,\n",
      "       -0.00999831, -0.00999992,  0.00999534, -0.00999992, -0.00999942,\n",
      "        0.0099998 , -0.00999978,  0.00999822, -0.00999947,  0.00999957,\n",
      "        0.00999923, -0.00984393,  0.00999994,  0.00999993,  0.00999983,\n",
      "        0.00999326, -0.00999981,  0.00999984, -0.00999967,  0.00999977,\n",
      "        0.00999989,  0.0099994 , -0.00999963,  0.00999879, -0.00999923,\n",
      "        0.00999981,  0.00999979, -0.00999958, -0.00999987,  0.00999959,\n",
      "       -0.00999563, -0.00999977,  0.00999976,  0.0099999 , -0.00999946,\n",
      "       -0.0099996 , -0.00999942, -0.00999979, -0.00999939, -0.00999947,\n",
      "       -0.00999981,  0.00999982, -0.00999985,  0.00999907, -0.00999989,\n",
      "       -0.00997708,  0.00999924,  0.00999601,  0.00999981,  0.00999839,\n",
      "       -0.00996018,  0.00999939,  0.00999988,  0.00999978,  0.00999953,\n",
      "       -0.00999902, -0.00999991, -0.00999956,  0.00999981, -0.00999969,\n",
      "       -0.0099994 , -0.00999987, -0.00999962, -0.0099992 ,  0.00999993,\n",
      "       -0.00999988,  0.00999993,  0.00999992, -0.00999952,  0.00999944,\n",
      "       -0.00999992, -0.00999893,  0.00999937, -0.00999988,  0.00999982,\n",
      "       -0.00999989, -0.00999983, -0.00999992,  0.00999993, -0.00999981,\n",
      "        0.00999974, -0.00999994,  0.00999965, -0.00999994,  0.0099974 ,\n",
      "        0.00999863, -0.00999978, -0.00999979, -0.00999939,  0.0099999 ,\n",
      "        0.00999958, -0.00999995,  0.00999986,  0.00999974,  0.00999983,\n",
      "        0.00999973,  0.0099997 , -0.00999857,  0.0099997 ,  0.00999992,\n",
      "       -0.0099999 ,  0.0099997 , -0.0099978 , -0.00999982,  0.00999442,\n",
      "        0.00999931,  0.00999987,  0.00999966,  0.00999971, -0.00999934,\n",
      "        0.00999993,  0.00999986, -0.00999981, -0.00999903, -0.00999992,\n",
      "       -0.00999875, -0.00999912,  0.00999994,  0.00999975,  0.00999891,\n",
      "       -0.00999974,  0.00999962,  0.00999646,  0.00999993, -0.00999874,\n",
      "        0.00999971,  0.00999964, -0.00999957, -0.00999937, -0.00999834,\n",
      "       -0.00999959, -0.0099998 ,  0.00999968, -0.00999993,  0.00999472,\n",
      "       -0.00999802, -0.00999988,  0.00999929, -0.00999927,  0.00998723,\n",
      "       -0.00999984, -0.00999878,  0.00999845,  0.00999961, -0.00999886,\n",
      "        0.        , -0.00999728, -0.00999969,  0.00999979, -0.00999964,\n",
      "        0.00998327,  0.00999959, -0.00999973,  0.00999978,  0.00999964,\n",
      "        0.00999988, -0.00999651], dtype=float32), array([[-0.02183763,  0.00557559,  0.01498034, ..., -0.01496404,\n",
      "         0.02130471,  0.00014387],\n",
      "       [ 0.00659312,  0.00141589,  0.0107805 , ...,  0.00268341,\n",
      "         0.01610176,  0.00040916],\n",
      "       [ 0.00055995,  0.0063026 ,  0.0233744 , ..., -0.01441088,\n",
      "         0.01885838, -0.0204145 ],\n",
      "       ..., \n",
      "       [ 0.00323264,  0.00171638,  0.01218238, ...,  0.00108999,\n",
      "        -0.00796651,  0.00479168],\n",
      "       [ 0.01001066,  0.0051175 ,  0.0016876 , ...,  0.00898026,\n",
      "         0.00242426,  0.0065578 ],\n",
      "       [-0.00968462,  0.01090718,  0.01201605, ...,  0.00516878,\n",
      "         0.00579467, -0.00745016]], dtype=float32), array([-0.00999541,  0.00999997,  0.00999993,  0.00999991, -0.00999998,\n",
      "        0.0099996 ,  0.00999991, -0.00999999,  0.00999999,  0.00999998,\n",
      "        0.00999996, -0.00999946,  0.        ,  0.        ,  0.00999998,\n",
      "       -0.00999998,  0.00999985, -0.00999993,  0.0099999 ,  0.00999921], dtype=float32), array([[-0.45255199, -0.37873504, -0.23716833, -0.01720259, -0.15694895,\n",
      "         0.21074425, -0.41348681,  0.25760609,  0.03401981,  0.24669926],\n",
      "       [ 0.05996209,  0.07620817,  0.33581996, -0.04610474,  0.24982293,\n",
      "        -0.34584314,  0.31273153, -0.15622047, -0.11955022, -0.0874029 ],\n",
      "       [ 0.32398468, -0.0980271 , -0.13277712,  0.40095639,  0.10619245,\n",
      "         0.40930399,  0.40382561, -0.20466401, -0.23688884, -0.3120586 ],\n",
      "       [ 0.27360076, -0.16150221, -0.05769658,  0.36569989, -0.05195109,\n",
      "         0.07493632,  0.22545631,  0.14175205, -0.42934319,  0.00325863],\n",
      "       [-0.2624985 , -0.13117486,  0.24832363,  0.08186847, -0.33518204,\n",
      "         0.37336448,  0.19701673, -0.00909116, -0.42593443, -0.13634351],\n",
      "       [ 0.42682195, -0.36432964,  0.36697164, -0.18157578,  0.41230214,\n",
      "        -0.2249697 , -0.20587897, -0.1891226 , -0.1017097 , -0.34381267],\n",
      "       [ 0.44990957,  0.04817355,  0.00973792, -0.15334286, -0.3619428 ,\n",
      "         0.24203278,  0.29337478,  0.18235834,  0.19851211, -0.01666508],\n",
      "       [-0.4074052 , -0.22217593, -0.13839005,  0.40598226,  0.23111553,\n",
      "        -0.30732757,  0.05323358,  0.08579873,  0.4026157 ,  0.15588491],\n",
      "       [ 0.33505988, -0.07842993, -0.03110134, -0.19288762,  0.42258501,\n",
      "        -0.1179184 , -0.1374068 ,  0.19663402,  0.27975267, -0.38927564],\n",
      "       [ 0.1555095 ,  0.20136927, -0.19944818,  0.3339211 ,  0.44432732,\n",
      "         0.06100664, -0.29186493, -0.21703328, -0.38935852, -0.02198963],\n",
      "       [ 0.160575  ,  0.08653784, -0.30957183,  0.13164569,  0.31954309,\n",
      "        -0.0197121 ,  0.09597955,  0.00383967,  0.25466973, -0.43736702],\n",
      "       [-0.07437652, -0.16368225, -0.33502313, -0.02442669,  0.13844894,\n",
      "        -0.28128663,  0.0680735 , -0.08904061,  0.34310362,  0.00590227],\n",
      "       [-0.12852758, -0.34528965,  0.02955681,  0.31144238, -0.37036037,\n",
      "         0.3597756 ,  0.10977894, -0.35049421, -0.02366084, -0.14144489],\n",
      "       [-0.04676285, -0.31578934,  0.19721329,  0.00331399,  0.43677443,\n",
      "        -0.40523314, -0.19064739, -0.24969557, -0.00698474, -0.31814307],\n",
      "       [ 0.14177929,  0.25433895,  0.17510635, -0.22178999, -0.22302563,\n",
      "         0.07214645, -0.22277074, -0.40025952, -0.43679455, -0.01259822],\n",
      "       [-0.11485213, -0.05478507, -0.3377488 ,  0.24572238, -0.12398665,\n",
      "         0.32507578,  0.17479216,  0.19813699,  0.02049187,  0.08194693],\n",
      "       [ 0.08244231,  0.3200902 , -0.12646614,  0.10157713, -0.38235435,\n",
      "        -0.36887866,  0.25645489, -0.43852422, -0.27488336,  0.32992387],\n",
      "       [-0.09189103, -0.4106383 ,  0.1633901 ,  0.36071202,  0.36998314,\n",
      "        -0.14635783,  0.20618483,  0.09611401, -0.14892006, -0.07382483],\n",
      "       [ 0.35101599,  0.01728464, -0.2005512 , -0.02959814,  0.07208853,\n",
      "         0.11224103,  0.16537732,  0.10100573,  0.39714718, -0.01708462],\n",
      "       [ 0.34603751, -0.117419  ,  0.14646587, -0.28467807,  0.25123316,\n",
      "        -0.07189298,  0.25751093, -0.04781597, -0.02309045, -0.34344241]], dtype=float32), array([ 0.00999999,  0.00999996,  0.00999995, -0.00999995,  0.00999997,\n",
      "       -0.00999997, -0.00999997, -0.00999995, -0.00999997, -0.00999998], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, 5, 5, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(512, 7, 7, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "model.fit(X_train, Y_train, batch_size=400, nb_epoch=8, verbose=1, \\\n",
    "            validation_data=(X_test, Y_test))\n",
    "weights = model.get_weights()\n",
    "for layer in model.layers:\n",
    "    g=layer.get_config()\n",
    "    h=layer.get_weights()\n",
    "    print (g)\n",
    "    print (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), input_shape=(3, 32, 32..., activation=\"relu\", padding=\"same\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (7, 7), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 512, 16, 16)       401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 512, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                655380    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,058,726\n",
      "Trainable params: 1,058,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/8\n",
      "73257/73257 [==============================] - 152s - loss: 0.3086 - acc: 0.9028 - val_loss: 0.2439 - val_acc: 0.9167\n",
      "Epoch 2/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.1879 - acc: 0.9336 - val_loss: 0.1535 - val_acc: 0.9450\n",
      "Epoch 3/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.1188 - acc: 0.9589 - val_loss: 0.1283 - val_acc: 0.9560\n",
      "Epoch 4/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.0959 - acc: 0.9677 - val_loss: 0.1203 - val_acc: 0.9599\n",
      "Epoch 5/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.0843 - acc: 0.9720 - val_loss: 0.0997 - val_acc: 0.9669\n",
      "Epoch 6/8\n",
      "73257/73257 [==============================] - 149s - loss: 0.0773 - acc: 0.9745 - val_loss: 0.0961 - val_acc: 0.9686\n",
      "Epoch 7/8\n",
      "73257/73257 [==============================] - 148s - loss: 0.0725 - acc: 0.9762 - val_loss: 0.0851 - val_acc: 0.9725\n",
      "Epoch 8/8\n",
      "73257/73257 [==============================] - 147s - loss: 0.0684 - acc: 0.9777 - val_loss: 0.0927 - val_acc: 0.9695\n",
      "{'bias_constraint': None, 'name': 'conv2d_1', 'kernel_size': (5, 5), 'data_format': 'channels_first', 'bias_regularizer': None, 'batch_input_shape': (None, 3, 32, 32), 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}, 'use_bias': True, 'activation': 'relu', 'kernel_regularizer': None, 'activity_regularizer': None, 'kernel_initializer': {'config': {'distribution': 'uniform', 'scale': 1.0, 'mode': 'fan_avg', 'seed': None}, 'class_name': 'VarianceScaling'}, 'dtype': 'float32', 'trainable': True, 'kernel_constraint': None, 'dilation_rate': (1, 1), 'filters': 16, 'strides': (1, 1), 'padding': 'same'}\n",
      "[array([[[[ -6.77184910e-02,   1.87774748e-02,  -1.26069542e-02, ...,\n",
      "            8.66720602e-02,  -4.05601114e-02,  -7.09021762e-02],\n",
      "         [ -2.51114108e-02,   2.33710604e-03,  -4.05556150e-02, ...,\n",
      "           -3.63870338e-02,  -1.00058496e-01,  -7.63822272e-02],\n",
      "         [ -5.62616289e-02,   3.65398999e-04,   2.29502190e-02, ...,\n",
      "            2.62389407e-02,  -5.32152876e-02,   7.25693554e-02]],\n",
      "\n",
      "        [[  4.53134961e-02,  -1.11437984e-01,   1.00534949e-02, ...,\n",
      "            8.07778388e-02,  -3.02626174e-02,  -3.92902642e-02],\n",
      "         [  1.01658091e-01,  -1.89241804e-02,   1.10237626e-02, ...,\n",
      "            9.53122415e-03,  -5.70032001e-03,   4.53184210e-02],\n",
      "         [  2.12082695e-02,  -1.26648530e-01,  -6.55048192e-02, ...,\n",
      "            1.44311681e-01,   7.16012195e-02,  -4.75874543e-03]],\n",
      "\n",
      "        [[ -1.11764610e-01,   2.18585115e-02,  -1.06563985e-01, ...,\n",
      "            1.06912665e-01,  -3.61206084e-02,   7.70131052e-02],\n",
      "         [  3.53994519e-02,   4.23276145e-03,  -6.46302551e-02, ...,\n",
      "            1.19823232e-01,   6.39206320e-02,  -3.58156860e-02],\n",
      "         [ -1.05325937e-01,   2.28303783e-02,  -5.18318564e-02, ...,\n",
      "            1.13468893e-01,  -7.27912271e-03,   1.05251014e-01]],\n",
      "\n",
      "        [[  1.01851486e-02,  -1.11301541e-01,  -9.81639549e-02, ...,\n",
      "            1.04610704e-01,  -9.00468901e-02,  -4.71794643e-02],\n",
      "         [ -1.65805016e-02,   8.27239156e-02,  -3.98169793e-02, ...,\n",
      "            5.16426302e-02,   6.18232936e-02,   1.07778236e-01],\n",
      "         [  4.53457385e-02,  -3.30408774e-02,   4.10738513e-02, ...,\n",
      "            1.45904958e-01,  -4.30367291e-02,  -4.07203436e-02]],\n",
      "\n",
      "        [[  1.76750813e-02,  -1.88867897e-02,  -3.67922187e-02, ...,\n",
      "            3.40042002e-02,   8.09448510e-02,   8.93316567e-02],\n",
      "         [ -8.66643712e-02,   6.13595098e-02,   6.25960752e-02, ...,\n",
      "            3.37557606e-02,  -9.67318639e-02,  -5.42771518e-02],\n",
      "         [  2.27580890e-02,   7.75861889e-02,   8.91926363e-02, ...,\n",
      "            1.24537736e-01,  -3.60716134e-02,  -7.46003687e-02]]],\n",
      "\n",
      "\n",
      "       [[[  3.61676030e-02,   1.46654714e-02,   3.39539200e-02, ...,\n",
      "            2.23382227e-02,   8.03525373e-02,  -2.12782547e-02],\n",
      "         [ -7.83841312e-02,  -8.99459329e-03,   1.82844289e-02, ...,\n",
      "            1.15084998e-01,   1.14800110e-01,  -1.33716151e-01],\n",
      "         [  4.37257886e-02,   5.30883670e-02,   8.66388902e-02, ...,\n",
      "            1.21966731e-02,  -7.73866102e-02,   5.61516322e-02]],\n",
      "\n",
      "        [[  5.89033403e-02,   1.82619989e-02,   9.34965387e-02, ...,\n",
      "            1.07616819e-01,   3.26053277e-02,   7.66161829e-02],\n",
      "         [ -2.24735718e-02,  -8.23664591e-02,  -7.36304224e-02, ...,\n",
      "           -2.27055810e-02,   4.40164134e-02,   7.38190338e-02],\n",
      "         [  5.63901328e-02,   3.08812782e-02,  -2.90128402e-02, ...,\n",
      "            3.11999395e-02,  -1.11811437e-01,   4.95361127e-02]],\n",
      "\n",
      "        [[ -5.56982569e-02,  -5.22073619e-02,   1.50773954e-02, ...,\n",
      "            1.85027495e-02,  -4.26554568e-02,  -7.05615953e-02],\n",
      "         [  9.95619074e-02,   7.28743384e-03,   8.32423717e-02, ...,\n",
      "            2.95638219e-02,   1.42387049e-02,   2.35577743e-03],\n",
      "         [ -5.19004390e-02,   7.90817216e-02,   6.90169185e-02, ...,\n",
      "            1.37297511e-02,   8.28985274e-02,   7.64830709e-02]],\n",
      "\n",
      "        [[  8.69365782e-02,  -9.82864946e-02,   1.14575915e-01, ...,\n",
      "            7.07713887e-02,   9.51811448e-02,   5.06439060e-02],\n",
      "         [ -1.12474643e-01,   4.46858443e-02,   7.57763907e-03, ...,\n",
      "            7.71903396e-02,   2.89644785e-02,  -3.79027352e-02],\n",
      "         [ -5.24084829e-03,   3.66220064e-02,   8.90593082e-02, ...,\n",
      "            1.14819914e-01,  -7.94898793e-02,  -2.85525844e-02]],\n",
      "\n",
      "        [[  6.93552122e-02,   7.98881352e-02,   1.21330671e-01, ...,\n",
      "           -9.89524350e-02,  -1.25151593e-02,  -3.67142707e-02],\n",
      "         [  4.36284877e-02,   1.78269986e-02,   7.86096007e-02, ...,\n",
      "            1.18540064e-01,  -1.03847124e-01,   1.90723483e-02],\n",
      "         [  5.89597002e-02,   1.69968884e-02,  -5.02381921e-02, ...,\n",
      "            3.41103822e-02,  -8.98436680e-02,   6.40847012e-02]]],\n",
      "\n",
      "\n",
      "       [[[  7.48220757e-02,   8.52094516e-02,  -1.01612337e-01, ...,\n",
      "           -8.12374726e-02,   2.83718039e-03,  -1.50994211e-01],\n",
      "         [  5.62293343e-02,   1.30250994e-02,  -4.07712650e-04, ...,\n",
      "           -1.55005949e-02,  -8.66510440e-03,  -7.08158985e-02],\n",
      "         [ -3.36450972e-02,  -9.72921550e-02,   5.58547080e-02, ...,\n",
      "            8.24488550e-02,  -1.20372042e-01,  -2.25423630e-02]],\n",
      "\n",
      "        [[  6.83264360e-02,  -1.23533301e-01,   4.40529101e-02, ...,\n",
      "           -6.02731593e-02,   7.02245021e-03,   5.90729229e-02],\n",
      "         [ -7.18776882e-02,  -1.00504495e-01,   6.79176822e-02, ...,\n",
      "           -6.71473071e-02,  -1.00298010e-01,  -3.72124463e-02],\n",
      "         [  2.56968662e-02,  -1.28111348e-01,   5.86566366e-02, ...,\n",
      "           -9.26326290e-02,  -1.67793296e-02,  -5.14071956e-02]],\n",
      "\n",
      "        [[ -1.17146976e-01,  -5.64476177e-02,   4.80081066e-02, ...,\n",
      "            8.28123763e-02,   4.59967274e-03,  -4.52615917e-02],\n",
      "         [  6.84653036e-03,  -9.78499418e-04,  -1.04044065e-01, ...,\n",
      "            6.48528486e-02,  -4.37433785e-03,  -6.64389953e-02],\n",
      "         [ -3.75248864e-02,   5.69949821e-02,   1.15079939e-01, ...,\n",
      "           -5.47374599e-02,  -1.20115411e-02,   6.73352508e-03]],\n",
      "\n",
      "        [[ -1.14150077e-01,   3.43209952e-02,   1.41917793e-02, ...,\n",
      "            2.65807402e-03,   8.91979188e-02,   1.11195119e-02],\n",
      "         [ -9.29484218e-02,   1.02431484e-01,   4.68816282e-03, ...,\n",
      "            5.29528707e-02,   5.75895328e-03,   1.27929658e-01],\n",
      "         [ -1.75218489e-02,   4.16091159e-02,   3.90376523e-02, ...,\n",
      "           -3.47043760e-02,  -7.75811747e-02,   1.23278476e-01]],\n",
      "\n",
      "        [[ -8.80979076e-02,  -7.37316087e-02,  -8.40960890e-02, ...,\n",
      "            6.27201572e-02,   1.01868831e-01,   1.51823804e-01],\n",
      "         [  4.44254838e-02,   7.35634118e-02,   5.32088876e-02, ...,\n",
      "            8.31029266e-02,   1.47929741e-02,   1.35100037e-01],\n",
      "         [  2.21953429e-02,   6.26142099e-02,  -8.53767060e-03, ...,\n",
      "            2.49682292e-02,   4.00101654e-02,   5.77230565e-03]]],\n",
      "\n",
      "\n",
      "       [[[  9.79046747e-02,   7.57863224e-02,  -1.12924829e-01, ...,\n",
      "           -1.35862464e-02,   9.44347382e-02,  -1.58360586e-01],\n",
      "         [ -7.64008611e-02,   3.28159817e-02,  -4.79520708e-02, ...,\n",
      "           -5.91820255e-02,   1.02848828e-01,  -1.41424626e-01],\n",
      "         [ -8.13149214e-02,   6.82473108e-02,  -7.82065187e-03, ...,\n",
      "           -1.27789518e-02,  -7.60274902e-02,  -1.62743211e-01]],\n",
      "\n",
      "        [[  1.10048942e-01,  -1.09403588e-01,  -5.34662232e-03, ...,\n",
      "           -7.88600668e-02,  -4.93073389e-02,  -1.39919743e-01],\n",
      "         [  1.01452500e-01,  -2.04241034e-02,  -1.11670099e-01, ...,\n",
      "           -1.13109134e-01,   3.35744806e-02,  -1.35803580e-01],\n",
      "         [  7.43561005e-03,  -5.73542006e-02,  -9.14353281e-02, ...,\n",
      "           -1.37003839e-01,  -9.07942131e-02,  -5.98359182e-02]],\n",
      "\n",
      "        [[ -8.97991508e-02,   2.05510706e-02,   1.03385352e-01, ...,\n",
      "           -1.19799882e-01,  -3.98012474e-02,   5.03200591e-02],\n",
      "         [  5.14225066e-02,  -5.39567955e-02,   8.15168098e-02, ...,\n",
      "            2.63378187e-03,  -5.26501127e-02,   7.37241283e-02],\n",
      "         [  5.17306700e-02,   1.79511011e-02,   2.51367074e-02, ...,\n",
      "           -1.27910018e-01,   4.82893661e-02,   5.47664873e-02]],\n",
      "\n",
      "        [[ -1.09837197e-01,  -1.53844673e-02,   1.53556745e-02, ...,\n",
      "           -2.27216631e-02,  -4.43035066e-02,   7.93547332e-02],\n",
      "         [ -7.00900853e-02,  -5.38365543e-02,   1.66182388e-02, ...,\n",
      "           -1.54513016e-01,   2.58833775e-03,   1.58564851e-01],\n",
      "         [ -8.27131718e-02,  -9.70303267e-02,   3.79616097e-02, ...,\n",
      "           -3.02102678e-02,  -5.19913174e-02,   1.19790062e-01]],\n",
      "\n",
      "        [[ -8.61956999e-02,  -1.29166156e-01,  -8.80994797e-02, ...,\n",
      "           -1.58119291e-01,  -8.07082187e-03,   4.50471938e-02],\n",
      "         [  8.35596323e-02,   3.23109375e-03,  -9.41959396e-02, ...,\n",
      "           -3.25526972e-03,  -9.05089155e-02,   1.71660289e-01],\n",
      "         [  2.71315631e-02,   2.94815004e-02,   1.08851008e-01, ...,\n",
      "           -1.09157763e-01,   1.18619449e-01,  -2.12570578e-02]]],\n",
      "\n",
      "\n",
      "       [[[  5.99002950e-02,   1.34543888e-02,   5.20699918e-02, ...,\n",
      "            6.92422036e-03,   1.94756757e-03,  -3.32674757e-02],\n",
      "         [  8.63606930e-02,   8.46987497e-03,  -8.32618624e-02, ...,\n",
      "           -1.18356138e-01,   1.13197014e-01,  -1.63143829e-01],\n",
      "         [ -8.57757851e-02,  -9.09472108e-02,   1.36825955e-02, ...,\n",
      "            5.05633317e-02,  -6.62275106e-02,   5.28365411e-02]],\n",
      "\n",
      "        [[  2.41345726e-02,   8.80115479e-02,   7.42834015e-03, ...,\n",
      "           -3.78456973e-02,  -6.82838857e-02,  -6.71145646e-03],\n",
      "         [ -8.46382827e-02,  -8.42679590e-02,  -2.59442870e-02, ...,\n",
      "           -1.35060951e-01,   3.85860242e-02,  -3.64336814e-03],\n",
      "         [ -9.71347839e-02,  -1.06242605e-01,  -1.23615853e-01, ...,\n",
      "           -1.65562090e-02,   8.84221271e-02,  -1.08205661e-01]],\n",
      "\n",
      "        [[  8.29457119e-02,   3.59323248e-02,   5.07729761e-02, ...,\n",
      "            3.55255492e-02,  -1.24032266e-01,  -1.04314916e-01],\n",
      "         [ -7.54475296e-02,  -6.46205321e-02,   8.66128132e-02, ...,\n",
      "            2.89523192e-02,  -1.30170622e-04,   8.18437487e-02],\n",
      "         [ -4.37579937e-02,   8.06851387e-02,  -1.06190898e-01, ...,\n",
      "            5.08294590e-02,   7.70293921e-02,  -5.39197661e-02]],\n",
      "\n",
      "        [[ -4.47633304e-02,  -1.37823626e-01,   5.98576330e-02, ...,\n",
      "           -1.30814938e-02,  -1.38510121e-02,   1.13982566e-01],\n",
      "         [ -6.95998818e-02,  -9.01963636e-02,  -9.01282579e-02, ...,\n",
      "           -1.20426834e-01,  -8.46167356e-02,  -2.14824826e-02],\n",
      "         [  4.01327014e-02,  -1.34221767e-03,   1.12114185e-02, ...,\n",
      "           -4.29507084e-02,   5.35926707e-02,  -7.44528398e-02]],\n",
      "\n",
      "        [[  1.08095072e-01,   2.18951255e-02,  -5.68575598e-02, ...,\n",
      "           -9.99926776e-02,   5.50655834e-02,   1.40304700e-01],\n",
      "         [ -1.39125716e-02,  -6.56644702e-02,  -1.11458465e-01, ...,\n",
      "           -8.99567381e-02,  -1.00680694e-01,  -2.49691028e-02],\n",
      "         [ -2.20241901e-02,  -8.43848810e-02,  -2.22663432e-02, ...,\n",
      "           -4.29315493e-02,   3.67912911e-02,   1.52617285e-04]]]], dtype=float32), array([ 0.00659658, -0.00829671, -0.01559392,  0.0058516 , -0.04646672,\n",
      "       -0.0214671 , -0.01446693,  0.01779694, -0.02002503,  0.09392072,\n",
      "       -0.01986353,  0.01745052,  0.03512106,  0.03523888, -0.00721084,\n",
      "        0.01467893], dtype=float32)]\n",
      "{'name': 'max_pooling2d_1', 'pool_size': (2, 2), 'padding': 'valid', 'trainable': True, 'data_format': 'channels_first', 'strides': (2, 2)}\n",
      "[]\n",
      "{'bias_constraint': None, 'name': 'conv2d_2', 'kernel_size': (7, 7), 'data_format': 'channels_first', 'bias_regularizer': None, 'dilation_rate': (1, 1), 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}, 'use_bias': True, 'activation': 'relu', 'kernel_regularizer': None, 'activity_regularizer': None, 'kernel_initializer': {'config': {'distribution': 'uniform', 'scale': 1.0, 'mode': 'fan_avg', 'seed': None}, 'class_name': 'VarianceScaling'}, 'trainable': True, 'kernel_constraint': None, 'filters': 512, 'strides': (1, 1), 'padding': 'same'}\n",
      "[array([[[[ -1.95769351e-02,  -3.06907333e-02,  -2.46540606e-02, ...,\n",
      "           -7.93282408e-03,  -7.65150413e-03,  -1.01563903e-02],\n",
      "         [  1.31467208e-02,  -2.73913983e-02,   1.15374243e-02, ...,\n",
      "           -2.28039268e-02,  -1.53194414e-02,  -2.26591676e-02],\n",
      "         [  5.49396733e-04,  -2.36627907e-02,   6.19286718e-03, ...,\n",
      "           -1.96827166e-02,  -2.23723687e-02,  -9.79812350e-03],\n",
      "         ..., \n",
      "         [  1.68089813e-03,   1.73584074e-02,  -9.15818103e-03, ...,\n",
      "           -5.73949423e-03,  -2.14337632e-02,  -1.26042832e-02],\n",
      "         [ -2.28498597e-02,  -2.90708207e-02,   2.90394370e-02, ...,\n",
      "           -1.03198504e-02,  -7.24694412e-03,  -2.50197761e-02],\n",
      "         [ -7.38758885e-04,   3.94422337e-02,  -1.91410109e-02, ...,\n",
      "           -1.41090825e-02,   2.64247926e-03,  -2.44825538e-02]],\n",
      "\n",
      "        [[  1.36755984e-02,   4.53117937e-02,  -2.35461630e-03, ...,\n",
      "           -1.25702703e-02,  -1.00983074e-02,  -1.57747827e-02],\n",
      "         [  7.44752400e-03,  -2.08269190e-02,   1.56911500e-02, ...,\n",
      "           -9.78741888e-03,  -2.41215341e-02,  -2.35709269e-02],\n",
      "         [ -1.53167313e-02,  -2.60200351e-02,  -4.44098515e-03, ...,\n",
      "           -1.86407287e-03,  -2.24777013e-02,  -1.40908621e-02],\n",
      "         ..., \n",
      "         [  7.81638455e-03,  -2.17976002e-03,   6.60676695e-03, ...,\n",
      "           -1.61982384e-02,  -1.17878309e-02,   3.03107779e-03],\n",
      "         [ -1.71297118e-02,  -3.28115784e-02,   2.43451074e-02, ...,\n",
      "           -2.39587948e-03,  -1.32031301e-02,  -1.51213417e-02],\n",
      "         [  4.41551907e-04,   1.80728175e-02,   1.34593376e-03, ...,\n",
      "            1.09035638e-03,  -1.00815129e-02,   3.22918105e-03]],\n",
      "\n",
      "        [[  1.23515762e-02,   3.64823043e-02,  -2.23336443e-02, ...,\n",
      "           -2.63539143e-04,  -1.31707056e-03,  -4.70502302e-03],\n",
      "         [  4.34727781e-03,  -1.48509657e-02,   1.85666624e-02, ...,\n",
      "           -1.87550709e-02,  -7.00885151e-03,  -4.03926661e-03],\n",
      "         [  6.32719835e-03,   1.35325978e-03,   9.65120643e-03, ...,\n",
      "           -6.24936819e-03,  -1.53115438e-02,  -8.57932307e-03],\n",
      "         ..., \n",
      "         [ -6.33939123e-03,   2.62243208e-02,  -1.19955745e-02, ...,\n",
      "           -1.62892435e-02,  -1.57098006e-02,  -1.12158069e-02],\n",
      "         [ -1.20735466e-02,  -1.92698482e-02,   2.15238463e-02, ...,\n",
      "           -2.00217869e-03,  -9.98659991e-04,  -4.98144887e-03],\n",
      "         [ -6.44830521e-03,  -5.46706244e-02,  -8.39680154e-03, ...,\n",
      "            2.16580275e-02,  -1.12510053e-03,   2.34717522e-02]],\n",
      "\n",
      "        ..., \n",
      "        [[ -8.71657580e-03,  -5.88962249e-02,  -1.53044397e-02, ...,\n",
      "           -1.90916341e-02,  -2.06467398e-02,  -4.53290716e-03],\n",
      "         [  2.48499326e-02,  -5.27506880e-03,   1.10650882e-02, ...,\n",
      "           -9.43612959e-03,  -1.53679065e-02,  -5.35638351e-03],\n",
      "         [ -6.60838839e-03,  -1.57563183e-02,   9.43827163e-03, ...,\n",
      "           -9.83680226e-03,  -6.51969342e-03,  -2.05972940e-02],\n",
      "         ..., \n",
      "         [ -2.30141147e-03,   8.43889564e-02,  -1.40430378e-02, ...,\n",
      "           -1.17514515e-02,  -9.25899483e-03,  -2.24736203e-02],\n",
      "         [ -3.95823456e-03,  -3.71248275e-03,  -2.01578457e-02, ...,\n",
      "            2.60045286e-03,  -2.46149562e-02,   1.13032181e-02],\n",
      "         [  1.03065297e-02,   2.76846830e-02,   1.18480250e-02, ...,\n",
      "           -3.58059444e-03,  -1.40085947e-02,  -2.61851959e-02]],\n",
      "\n",
      "        [[ -1.19540291e-02,   3.96669358e-02,   7.38438405e-03, ...,\n",
      "           -9.79589857e-03,  -8.47604685e-03,  -4.66659851e-03],\n",
      "         [  2.18135137e-02,   3.44096450e-03,  -4.23123315e-03, ...,\n",
      "            1.57019775e-03,  -2.00833362e-02,  -4.50479798e-04],\n",
      "         [  1.22075831e-03,  -3.21678743e-02,  -1.40746040e-02, ...,\n",
      "           -1.53214708e-02,  -1.16675068e-03,  -1.60396751e-02],\n",
      "         ..., \n",
      "         [ -1.68709811e-02,   7.64236823e-02,  -1.09181656e-02, ...,\n",
      "           -9.06622875e-03,  -1.24049047e-02,   3.68091580e-03],\n",
      "         [  1.28278546e-02,   6.97124843e-03,   2.92626787e-02, ...,\n",
      "           -5.35784476e-03,  -1.71993896e-02,  -2.67343130e-03],\n",
      "         [  9.67451371e-03,   2.96315793e-02,  -1.20747294e-02, ...,\n",
      "           -2.28805058e-02,   4.03648708e-03,   1.58299494e-03]],\n",
      "\n",
      "        [[ -2.09864099e-02,   2.68549230e-02,   2.54078256e-03, ...,\n",
      "           -1.97483934e-02,   2.40198933e-02,  -6.65792823e-03],\n",
      "         [ -4.47900593e-03,  -1.88359688e-03,   2.58582272e-03, ...,\n",
      "           -1.86277702e-02,  -2.45229080e-02,  -6.75923564e-03],\n",
      "         [  4.70815785e-03,  -6.02013096e-02,   5.39577659e-03, ...,\n",
      "           -2.30319332e-02,  -1.93449818e-02,  -1.70034282e-02],\n",
      "         ..., \n",
      "         [ -5.56099229e-03,   8.83048400e-02,  -5.53178694e-03, ...,\n",
      "           -2.34844629e-02,   2.44279392e-04,  -5.54018887e-04],\n",
      "         [ -2.23628469e-02,   4.80361376e-03,   2.01610215e-02, ...,\n",
      "            3.66433803e-03,  -1.96580030e-02,  -1.45061603e-02],\n",
      "         [  9.44933156e-04,   2.92662550e-02,  -1.18593164e-02, ...,\n",
      "           -5.99708222e-03,  -1.38251819e-02,  -1.00533618e-02]]],\n",
      "\n",
      "\n",
      "       [[[  5.38424030e-03,  -5.91157675e-02,  -1.71995945e-02, ...,\n",
      "           -1.12810684e-02,  -1.08309453e-02,  -1.19797196e-02],\n",
      "         [  1.49347410e-02,  -1.68942008e-02,   1.92757305e-02, ...,\n",
      "           -1.80104915e-02,   4.80077881e-03,  -2.40189303e-03],\n",
      "         [  6.90410845e-03,  -4.28850800e-02,  -1.64225250e-02, ...,\n",
      "            1.67906284e-04,  -1.34605709e-02,  -2.04733084e-03],\n",
      "         ..., \n",
      "         [  6.45962683e-03,  -3.27576734e-02,   8.86984728e-03, ...,\n",
      "           -1.31219402e-02,  -2.46740486e-02,  -4.60431125e-04],\n",
      "         [  4.65092622e-03,  -5.33047272e-03,  -5.42470347e-03, ...,\n",
      "           -2.12882496e-02,   2.28971709e-03,  -1.70366839e-02],\n",
      "         [ -9.97792091e-03,   3.98354754e-02,  -3.04078907e-02, ...,\n",
      "           -2.43734270e-02,  -3.22639104e-03,  -3.11182179e-02]],\n",
      "\n",
      "        [[  1.40194325e-02,   7.43931606e-02,  -1.39077771e-02, ...,\n",
      "           -1.29411584e-02,  -1.18384352e-02,  -1.24212541e-02],\n",
      "         [  1.65326931e-02,  -6.42488943e-03,   5.03596850e-03, ...,\n",
      "           -2.39206031e-02,  -7.81146809e-05,  -2.08672751e-02],\n",
      "         [ -2.68198550e-03,  -4.01437171e-02,  -1.03440769e-02, ...,\n",
      "           -4.51151561e-03,  -2.77356035e-03,  -2.79837102e-03],\n",
      "         ..., \n",
      "         [  8.61651357e-03,  -3.97490598e-02,  -1.31840874e-02, ...,\n",
      "           -2.03077798e-03,  -8.60371627e-03,  -1.03006810e-02],\n",
      "         [ -3.68519826e-03,  -1.04901018e-02,   5.00897784e-03, ...,\n",
      "           -1.31925065e-02,  -1.84562076e-02,   2.42265593e-02],\n",
      "         [ -1.46696623e-02,  -3.89027479e-03,  -2.18898375e-02, ...,\n",
      "            2.01478042e-03,  -2.38342658e-02,   5.60788205e-03]],\n",
      "\n",
      "        [[  5.88516751e-03,   8.59481394e-02,  -2.37468146e-02, ...,\n",
      "           -1.87604874e-02,  -1.83749534e-02,   9.65600275e-03],\n",
      "         [  9.46368650e-03,  -2.54128482e-02,   1.01105887e-02, ...,\n",
      "           -7.38005154e-03,  -9.26507637e-04,  -2.21479349e-02],\n",
      "         [ -2.79470929e-03,   4.61882167e-03,  -8.97133909e-03, ...,\n",
      "            4.51551285e-03,  -1.58597231e-02,  -8.53727385e-03],\n",
      "         ..., \n",
      "         [ -9.12837044e-04,  -4.83218171e-02,   4.74729063e-03, ...,\n",
      "           -1.12035908e-02,  -1.66571084e-02,  -2.86148232e-03],\n",
      "         [  1.75056269e-03,  -1.35250706e-02,  -1.69442967e-02, ...,\n",
      "            4.13563102e-05,  -1.54725211e-02,   7.73552340e-03],\n",
      "         [ -1.24462498e-02,  -1.36762291e-01,  -8.59317370e-03, ...,\n",
      "            1.40526230e-02,  -2.07852051e-02,   1.63185112e-02]],\n",
      "\n",
      "        ..., \n",
      "        [[ -5.72413264e-04,  -2.01928206e-02,  -2.68229772e-03, ...,\n",
      "           -6.22849260e-03,  -2.59716008e-02,   1.42738596e-02],\n",
      "         [ -2.60449573e-03,   1.92161044e-03,   1.09545630e-03, ...,\n",
      "           -2.83188559e-03,   1.53315812e-03,  -2.40548253e-02],\n",
      "         [ -1.37161715e-02,  -3.15731508e-03,  -3.85882147e-02, ...,\n",
      "            3.80393444e-03,  -7.60661624e-03,  -1.34811345e-02],\n",
      "         ..., \n",
      "         [  1.27172912e-03,   7.73295388e-02,   2.48021004e-03, ...,\n",
      "           -1.55596053e-02,   9.71593254e-04,  -1.83990202e-03],\n",
      "         [ -3.29256728e-02,  -3.24181933e-03,  -7.41170859e-03, ...,\n",
      "            1.19059999e-03,  -1.91426128e-02,  -6.54954463e-04],\n",
      "         [ -6.30160188e-03,  -1.68607086e-02,  -6.97140396e-03, ...,\n",
      "            6.54949807e-03,  -2.04356015e-02,   1.66421011e-02]],\n",
      "\n",
      "        [[ -1.21360030e-02,  -1.76228117e-03,   2.10690731e-03, ...,\n",
      "           -1.20741744e-02,   2.41306610e-03,  -4.03381977e-03],\n",
      "         [  2.00236291e-02,  -1.50084542e-02,   2.27338951e-02, ...,\n",
      "            1.60131697e-03,  -1.54258497e-02,  -7.08219595e-04],\n",
      "         [  1.21368729e-02,  -2.31045927e-03,  -2.99652778e-02, ...,\n",
      "           -7.40787573e-03,  -2.42418088e-02,  -3.18127102e-04],\n",
      "         ..., \n",
      "         [ -3.77376727e-03,   7.71193728e-02,  -1.31513933e-02, ...,\n",
      "           -2.03281958e-02,   5.08906029e-04,  -4.79476433e-03],\n",
      "         [ -5.31734526e-03,   1.31545700e-02,  -4.63289721e-03, ...,\n",
      "            1.34447590e-04,  -8.16244539e-03,  -1.91367883e-02],\n",
      "         [ -1.57435925e-03,  -6.75369380e-03,  -9.71462950e-03, ...,\n",
      "            1.05220079e-03,  -7.96761177e-03,  -1.06950821e-02]],\n",
      "\n",
      "        [[ -1.27840471e-02,   3.03058475e-02,  -1.04812568e-03, ...,\n",
      "           -5.25590545e-03,   2.73013990e-02,  -1.47195337e-02],\n",
      "         [  3.63434752e-04,  -5.20030875e-03,   2.00859737e-02, ...,\n",
      "           -2.42088810e-02,  -2.06154417e-02,   2.52943765e-03],\n",
      "         [  6.43979991e-03,   2.20128484e-02,  -3.29616144e-02, ...,\n",
      "           -8.05807952e-03,  -4.49222000e-03,  -2.61393003e-02],\n",
      "         ..., \n",
      "         [  1.91377674e-03,   1.04690567e-01,   5.64457802e-03, ...,\n",
      "           -4.90232091e-03,  -7.81821087e-03,  -2.20037568e-02],\n",
      "         [  2.73452210e-03,  -4.44370788e-03,   2.29248647e-02, ...,\n",
      "           -2.28592865e-02,   3.39438766e-03,  -1.85098089e-02],\n",
      "         [ -9.41998605e-03,  -1.14503466e-02,   6.86106621e-04, ...,\n",
      "           -1.55206090e-02,  -3.10738152e-03,  -2.06724890e-02]]],\n",
      "\n",
      "\n",
      "       [[[  7.95960054e-03,  -4.17628437e-02,  -3.28121372e-02, ...,\n",
      "           -6.68947073e-03,  -1.64775718e-02,  -1.15841608e-02],\n",
      "         [  1.29618477e-02,  -4.04544882e-02,   1.12786861e-02, ...,\n",
      "           -2.50856187e-02,  -2.36893725e-03,  -1.54393883e-02],\n",
      "         [ -1.59240104e-02,  -7.48503208e-02,   8.13128147e-03, ...,\n",
      "           -1.69777498e-02,  -1.17747402e-02,  -4.86193597e-03],\n",
      "         ..., \n",
      "         [  2.48784130e-03,  -1.21474259e-01,  -1.82729447e-03, ...,\n",
      "           -8.01571645e-03,  -1.99969276e-03,  -8.50067940e-03],\n",
      "         [  2.31966358e-02,   2.79836170e-02,   1.16261411e-02, ...,\n",
      "           -4.71484661e-03,  -6.13925699e-03,  -2.29606032e-02],\n",
      "         [ -6.92542177e-04,   4.75703664e-02,  -2.92605534e-02, ...,\n",
      "           -7.74642546e-03,  -1.47435181e-02,  -3.24554481e-02]],\n",
      "\n",
      "        [[ -4.64866078e-03,   3.44725400e-02,  -2.29259040e-02, ...,\n",
      "           -1.48667144e-02,  -1.03963725e-02,  -3.03155482e-02],\n",
      "         [  2.36658137e-02,  -1.02418046e-02,   7.39127677e-03, ...,\n",
      "           -6.87766634e-03,  -1.29063148e-03,  -4.76110307e-03],\n",
      "         [ -1.11848507e-02,  -1.04133904e-01,   6.18785759e-03, ...,\n",
      "           -8.27317126e-04,  -2.27001552e-02,  -1.97842671e-03],\n",
      "         ..., \n",
      "         [ -1.05073135e-02,  -1.53602540e-01,   2.69267010e-03, ...,\n",
      "           -5.51573467e-03,   1.26639870e-03,  -1.68607552e-02],\n",
      "         [  2.21894272e-02,   2.66375560e-02,   1.16404742e-02, ...,\n",
      "           -1.48629770e-03,  -2.00517438e-02,  -2.20956206e-02],\n",
      "         [  1.36259655e-02,   2.34855767e-02,   5.88759547e-03, ...,\n",
      "           -8.19655042e-03,  -1.80043112e-02,  -6.25089509e-03]],\n",
      "\n",
      "        [[ -8.78274720e-03,  -2.41642706e-02,  -1.55442692e-02, ...,\n",
      "           -2.41726134e-02,   3.04851728e-03,   1.36929378e-02],\n",
      "         [  2.15650350e-02,  -1.18547510e-02,   1.45120742e-02, ...,\n",
      "           -1.37412939e-02,  -9.31714289e-03,   2.88864202e-03],\n",
      "         [ -2.93415878e-03,   1.92427896e-02,  -3.07291024e-03, ...,\n",
      "            1.99783640e-03,  -2.26898063e-02,  -7.25519843e-03],\n",
      "         ..., \n",
      "         [  1.22337183e-02,  -1.41744390e-01,   2.17947294e-03, ...,\n",
      "           -1.96770150e-02,   2.92493496e-03,  -2.14655120e-02],\n",
      "         [  2.05141194e-02,   1.02078756e-02,   3.91465947e-02, ...,\n",
      "           -1.56814083e-02,  -2.35093422e-02,  -1.32317981e-02],\n",
      "         [ -2.20868420e-02,  -1.62474588e-01,   6.10667327e-03, ...,\n",
      "            1.05999801e-02,  -1.95716452e-02,   1.33816767e-02]],\n",
      "\n",
      "        ..., \n",
      "        [[ -8.50128755e-03,   7.93037377e-03,  -9.18640289e-03, ...,\n",
      "            9.21566319e-03,   6.64986297e-03,  -1.37268752e-02],\n",
      "         [  2.29800381e-02,  -1.86478198e-02,   2.47277748e-02, ...,\n",
      "           -1.30053423e-03,  -1.60460137e-02,  -5.18613309e-03],\n",
      "         [  2.61388050e-04,  -3.10625285e-02,   6.93634572e-03, ...,\n",
      "            4.69955243e-03,  -1.20580941e-02,  -4.70237900e-03],\n",
      "         ..., \n",
      "         [ -1.04550729e-02,  -2.48449948e-02,  -1.16251945e-03, ...,\n",
      "           -2.28578821e-02,  -8.89086223e-04,  -1.51725244e-02],\n",
      "         [  2.00652182e-02,   2.57217586e-02,   1.03769721e-02, ...,\n",
      "           -2.38211751e-02,  -2.24312786e-02,   2.36732140e-03],\n",
      "         [ -1.43942824e-02,  -1.17310174e-01,   6.93776133e-03, ...,\n",
      "            2.82813888e-03,  -2.75781890e-03,  -1.94295961e-03]],\n",
      "\n",
      "        [[  1.23510007e-02,  -1.18835806e-03,  -1.05289777e-03, ...,\n",
      "           -2.78126374e-02,   1.14638647e-02,  -1.99781060e-02],\n",
      "         [  2.28356067e-02,  -2.15418860e-02,   7.52701750e-03, ...,\n",
      "           -1.85398664e-02,  -2.46963501e-02,  -1.23519897e-02],\n",
      "         [ -2.39202660e-03,  -9.47584361e-02,  -3.40535538e-03, ...,\n",
      "           -1.61156128e-03,  -5.96287474e-03,  -1.31325629e-02],\n",
      "         ..., \n",
      "         [ -1.43207191e-02,  -1.27164107e-02,  -1.00922044e-02, ...,\n",
      "            3.63136549e-03,  -2.13328493e-03,  -1.05180331e-02],\n",
      "         [  7.91835412e-03,   3.74395959e-02,   3.32818404e-02, ...,\n",
      "           -1.55958049e-02,  -1.03710378e-02,  -1.01384055e-02],\n",
      "         [ -6.97604660e-03,  -4.27057184e-02,   1.13959434e-02, ...,\n",
      "           -1.69721562e-02,  -1.85612012e-02,  -9.03459731e-03]],\n",
      "\n",
      "        [[ -1.45442849e-02,   4.00749259e-02,  -1.84796844e-03, ...,\n",
      "           -1.08066555e-02,   1.84738114e-02,  -1.58286225e-02],\n",
      "         [  1.16845430e-03,  -1.91793703e-02,   2.37289201e-02, ...,\n",
      "           -1.54192457e-02,  -1.78049263e-02,  -6.39796164e-03],\n",
      "         [ -1.46766324e-04,  -3.03173568e-02,  -1.95510406e-03, ...,\n",
      "           -9.79882479e-03,  -9.23858210e-03,  -5.22812270e-03],\n",
      "         ..., \n",
      "         [  9.06413514e-03,   2.81507578e-02,   1.02202862e-03, ...,\n",
      "           -2.42884513e-02,  -7.00583123e-03,  -1.87599391e-03],\n",
      "         [  2.23455345e-03,  -7.33732432e-03,   1.19090742e-02, ...,\n",
      "            1.91353336e-02,  -1.11178691e-02,  -2.20934115e-02],\n",
      "         [  1.29829189e-02,  -8.93285871e-03,  -1.57737471e-02, ...,\n",
      "           -6.96513522e-03,  -1.69405267e-02,  -5.79380617e-03]]],\n",
      "\n",
      "\n",
      "       ..., \n",
      "       [[[  6.19040849e-03,  -2.35695858e-03,  -9.95944906e-03, ...,\n",
      "           -3.05275638e-02,   2.43249312e-02,  -1.95329357e-02],\n",
      "         [ -5.62612945e-03,  -2.37241257e-02,   5.89274941e-03, ...,\n",
      "            1.17681269e-03,  -2.25537159e-02,  -2.19293609e-02],\n",
      "         [ -6.81330275e-04,  -9.99630243e-02,  -1.71208463e-03, ...,\n",
      "           -1.55689307e-02,  -2.34520808e-03,  -9.61085130e-03],\n",
      "         ..., \n",
      "         [ -1.58008945e-03,   1.60022825e-02,   2.20565591e-03, ...,\n",
      "           -4.92383167e-03,   3.73318116e-03,  -1.00571215e-02],\n",
      "         [  1.68607216e-02,   8.45525321e-03,  -2.06158571e-02, ...,\n",
      "           -2.28235610e-02,  -3.33443098e-03,   4.91435640e-04],\n",
      "         [ -1.16065387e-02,   1.14511900e-01,  -1.30013488e-02, ...,\n",
      "           -2.26328969e-02,  -1.83500759e-02,  -3.09234001e-02]],\n",
      "\n",
      "        [[  9.09351464e-03,   4.83767688e-02,  -2.73869373e-02, ...,\n",
      "            6.66512363e-03,  -3.73440189e-03,  -3.00925858e-02],\n",
      "         [  5.84980939e-03,   1.58505440e-02,   7.38946721e-03, ...,\n",
      "           -2.38206144e-03,  -8.41608271e-04,  -2.08149175e-03],\n",
      "         [ -1.31592844e-02,  -6.97613731e-02,   5.14102867e-03, ...,\n",
      "           -7.58379418e-03,   6.21913001e-04,  -2.01512761e-02],\n",
      "         ..., \n",
      "         [  1.13543635e-02,  -7.86655210e-03,  -6.66977838e-03, ...,\n",
      "           -2.79812055e-04,  -1.07542751e-02,  -2.97172293e-02],\n",
      "         [  4.99353670e-02,  -1.49471350e-02,   1.13917626e-02, ...,\n",
      "            1.13736903e-02,   1.76621918e-02,  -3.45033407e-03],\n",
      "         [ -1.47081120e-02,   6.37168139e-02,  -3.18259895e-02, ...,\n",
      "           -2.98790890e-03,  -1.33576589e-02,  -2.81166695e-02]],\n",
      "\n",
      "        [[ -1.91228092e-02,  -8.42563715e-03,  -3.13507952e-02, ...,\n",
      "            8.20022635e-03,  -1.40789822e-02,  -9.12845135e-03],\n",
      "         [  7.04952469e-03,   1.13248604e-03,   1.34140253e-02, ...,\n",
      "           -1.60828754e-02,   2.70285551e-03,  -6.41968427e-03],\n",
      "         [ -1.43072261e-02,   1.86453685e-02,   6.84604002e-03, ...,\n",
      "           -2.10240558e-02,  -1.34316403e-02,  -2.27598678e-02],\n",
      "         ..., \n",
      "         [  8.36395472e-03,  -7.26410970e-02,  -1.67444218e-02, ...,\n",
      "            2.42097955e-03,  -2.02800632e-02,  -1.97770558e-02],\n",
      "         [  4.22977135e-02,   2.59641651e-03,   4.36770497e-03, ...,\n",
      "            2.02507097e-02,  -3.65027320e-03,   1.39015652e-02],\n",
      "         [ -8.42918933e-04,  -7.26027265e-02,   3.86663189e-04, ...,\n",
      "           -4.31334786e-03,  -3.03117558e-03,  -1.82276741e-02]],\n",
      "\n",
      "        ..., \n",
      "        [[ -5.18343924e-03,  -4.04929183e-03,   9.50905960e-03, ...,\n",
      "            2.08650045e-02,   1.07131284e-02,  -1.69927925e-02],\n",
      "         [ -5.28116431e-03,   7.37446221e-03,   8.86513013e-03, ...,\n",
      "            4.33646608e-03,  -2.48705372e-02,  -8.59375205e-03],\n",
      "         [  8.37687310e-03,   4.18248475e-02,  -1.16933847e-03, ...,\n",
      "           -7.20289350e-03,  -2.11404152e-02,  -2.42519211e-02],\n",
      "         ..., \n",
      "         [  1.17963217e-02,  -1.30897477e-01,  -7.30732316e-03, ...,\n",
      "            2.73076585e-03,  -1.40298894e-02,  -1.96610577e-02],\n",
      "         [  3.25575210e-02,  -2.52155159e-02,   1.27534457e-02, ...,\n",
      "           -7.44113885e-03,  -2.93099228e-03,   3.24946176e-03],\n",
      "         [  7.14114867e-03,   9.19102109e-04,  -1.33331856e-02, ...,\n",
      "           -1.56731885e-02,   4.45208233e-03,  -8.76780041e-03]],\n",
      "\n",
      "        [[ -1.10405069e-02,   7.28640631e-02,  -4.85595362e-03, ...,\n",
      "           -1.67077929e-02,   6.73560426e-03,  -2.36348510e-02],\n",
      "         [  1.49861732e-02,   2.03542989e-02,   1.02384677e-02, ...,\n",
      "           -1.71123147e-02,  -2.10669432e-02,  -1.83974393e-02],\n",
      "         [  4.42299852e-03,  -1.17283994e-02,  -2.82880338e-03, ...,\n",
      "           -1.47706456e-03,  -5.76903811e-03,  -1.91033855e-02],\n",
      "         ..., \n",
      "         [  1.04233241e-02,  -1.33258671e-01,   4.90017561e-03, ...,\n",
      "            4.35208296e-03,  -2.66640075e-03,  -1.88265070e-02],\n",
      "         [  3.70770395e-02,   8.74762144e-03,   3.31233884e-03, ...,\n",
      "           -3.70966922e-03,   2.75355391e-03,   1.74763910e-02],\n",
      "         [  5.46091376e-03,   4.34454791e-02,  -1.72397398e-04, ...,\n",
      "            1.08468439e-03,  -2.01618336e-02,  -1.37850353e-02]],\n",
      "\n",
      "        [[ -8.96653160e-03,   6.94969073e-02,  -5.86221134e-03, ...,\n",
      "           -1.15305167e-02,   7.27889594e-03,  -1.36997793e-02],\n",
      "         [  8.01380258e-03,   1.63018834e-02,   7.59169599e-03, ...,\n",
      "            1.64523441e-03,  -2.09898315e-02,   5.71660115e-04],\n",
      "         [  8.78712721e-03,   1.02625485e-03,  -1.20376078e-02, ...,\n",
      "           -3.54528613e-03,   1.43205343e-05,  -1.82872936e-02],\n",
      "         ..., \n",
      "         [ -1.10552842e-02,  -6.97956532e-02,   6.59200689e-03, ...,\n",
      "           -8.87491181e-03,  -1.95333511e-02,  -2.66371220e-02],\n",
      "         [  2.70529985e-02,  -2.05985531e-02,   2.07313467e-02, ...,\n",
      "            2.26882715e-02,  -1.31351696e-02,  -2.24808045e-02],\n",
      "         [ -8.96909274e-03,   9.96952690e-03,   6.46770978e-03, ...,\n",
      "           -2.11787950e-02,  -2.13162638e-02,  -9.07839183e-03]]],\n",
      "\n",
      "\n",
      "       [[[ -6.30269991e-03,   3.03417095e-04,  -1.40143968e-02, ...,\n",
      "           -1.31972088e-02,  -2.72091236e-02,  -1.52919544e-02],\n",
      "         [  1.02404058e-02,  -4.57298234e-02,   9.40756500e-03, ...,\n",
      "           -2.28287987e-02,  -1.68194622e-02,  -1.79756377e-02],\n",
      "         [ -1.45165157e-02,  -1.07526869e-01,  -1.44437654e-02, ...,\n",
      "           -4.84925229e-03,  -1.44225033e-02,   8.54123849e-03],\n",
      "         ..., \n",
      "         [ -1.17168361e-02,   3.24201770e-02,  -6.72889501e-03, ...,\n",
      "           -1.73169766e-02,  -1.18463654e-02,   7.11614173e-03],\n",
      "         [  1.17123397e-02,  -3.78947593e-02,   4.95877070e-03, ...,\n",
      "            4.32102662e-03,   9.59293079e-03,   2.31683217e-02],\n",
      "         [  1.42160952e-02,   1.20447822e-01,   7.77844572e-04, ...,\n",
      "           -2.39390600e-02,  -6.30197953e-03,  -1.07740462e-02]],\n",
      "\n",
      "        [[ -2.92726047e-02,  -1.76021606e-02,  -5.95548423e-03, ...,\n",
      "            1.20712258e-03,  -1.99509952e-02,  -1.48286410e-02],\n",
      "         [ -2.85315122e-02,  -6.39300048e-02,   6.00229111e-03, ...,\n",
      "           -2.49490701e-02,  -1.60880648e-02,   1.23692658e-02],\n",
      "         [ -9.39675979e-03,  -1.06043845e-01,   8.48534633e-04, ...,\n",
      "           -1.53936669e-02,  -1.42488312e-02,   1.36221172e-02],\n",
      "         ..., \n",
      "         [ -5.14999451e-03,   2.67865546e-02,  -4.58579464e-03, ...,\n",
      "            3.33135831e-03,  -1.48569169e-02,   7.22429622e-03],\n",
      "         [ -5.80875669e-04,  -8.83435551e-03,   3.61424051e-02, ...,\n",
      "            3.76825314e-03,  -2.20737364e-02,   1.82691850e-02],\n",
      "         [  3.23241903e-03,   9.48591232e-02,   1.08979689e-02, ...,\n",
      "           -7.61655206e-03,   2.28500273e-02,  -5.46274148e-03]],\n",
      "\n",
      "        [[  1.28784310e-02,  -4.85655740e-02,  -1.23157799e-02, ...,\n",
      "           -2.79537477e-02,  -1.78573728e-02,   1.27503620e-02],\n",
      "         [ -2.66762916e-02,  -1.37787955e-02,   1.30743543e-02, ...,\n",
      "           -9.28950869e-03,  -1.43146794e-02,   7.75118032e-03],\n",
      "         [ -2.84637958e-02,  -4.45978343e-02,  -5.36951423e-03, ...,\n",
      "            1.00980885e-03,  -2.05637813e-02,  -1.39464457e-02],\n",
      "         ..., \n",
      "         [ -1.88827608e-02,  -1.54031254e-02,  -1.21744638e-02, ...,\n",
      "            2.63108872e-03,  -1.97468810e-02,   3.62114515e-03],\n",
      "         [ -1.16395224e-02,  -4.58781794e-03,   5.52945212e-02, ...,\n",
      "            8.19158088e-03,  -2.24080421e-02,   2.37254351e-02],\n",
      "         [ -7.41051231e-03,  -6.02559932e-02,   4.38727485e-03, ...,\n",
      "            7.26087671e-03,  -5.17928414e-03,   2.40431726e-03]],\n",
      "\n",
      "        ..., \n",
      "        [[ -5.10887802e-03,  -4.12683794e-03,   8.67332984e-03, ...,\n",
      "            1.16655789e-03,  -3.21408664e-03,  -1.46416891e-02],\n",
      "         [ -1.79967470e-02,  -3.40007655e-02,  -3.02422000e-03, ...,\n",
      "            4.71998099e-03,   2.59581860e-03,   1.32085551e-02],\n",
      "         [ -1.93743799e-02,  -1.91699457e-03,  -1.92044303e-02, ...,\n",
      "           -1.19293602e-02,  -1.73183400e-02,   8.06067698e-03],\n",
      "         ..., \n",
      "         [ -3.24002542e-02,  -5.94613957e-04,  -3.10396543e-03, ...,\n",
      "           -6.44331146e-03,  -1.24011328e-02,  -9.23204515e-03],\n",
      "         [ -1.62717625e-02,  -1.65140200e-02,  -1.26844877e-03, ...,\n",
      "           -2.35617682e-02,   3.39911878e-03,   1.64464712e-02],\n",
      "         [ -1.63986683e-02,   3.17981690e-02,  -9.31968167e-03, ...,\n",
      "           -8.57932086e-04,  -8.61930195e-03,   1.57295167e-03]],\n",
      "\n",
      "        [[ -3.03211138e-02,   7.56065771e-02,  -3.14996615e-02, ...,\n",
      "            2.50621643e-02,  -2.03674324e-02,  -2.86411494e-04],\n",
      "         [ -6.57899491e-03,   2.73287576e-03,   1.84378810e-02, ...,\n",
      "           -3.93296126e-03,  -4.12846636e-03,  -1.06080656e-03],\n",
      "         [ -1.61601063e-02,  -1.28250327e-02,  -4.42468678e-04, ...,\n",
      "           -1.65113807e-03,  -1.86141450e-02,  -3.33820842e-03],\n",
      "         ..., \n",
      "         [ -2.45297477e-02,  -2.69585364e-02,   7.33461371e-03, ...,\n",
      "           -7.78678060e-03,  -4.48123959e-04,  -5.48864622e-03],\n",
      "         [ -7.48980499e-04,   1.54858353e-02,   2.13079322e-02, ...,\n",
      "            1.24826133e-02,  -6.19446859e-03,   1.29588265e-02],\n",
      "         [ -7.57754594e-03,  -6.97257882e-03,  -4.01644409e-03, ...,\n",
      "           -1.96316615e-02,  -2.44946592e-02,  -1.43129108e-02]],\n",
      "\n",
      "        [[ -2.63281390e-02,   9.67012346e-02,   1.16970828e-02, ...,\n",
      "            3.24450247e-03,   2.21467335e-02,  -1.80920474e-02],\n",
      "         [ -2.98180804e-02,   7.92927146e-02,   2.29185466e-02, ...,\n",
      "           -1.27539225e-03,  -8.00165907e-03,   1.23561854e-02],\n",
      "         [ -9.04325955e-03,  -1.25584304e-02,  -6.66719454e-04, ...,\n",
      "           -1.72838122e-02,  -7.06153130e-03,   8.91135260e-03],\n",
      "         ..., \n",
      "         [ -2.59901788e-02,   6.50723116e-04,   3.51694552e-03, ...,\n",
      "           -1.44620519e-02,  -5.83021576e-03,   3.52530926e-03],\n",
      "         [  1.10896840e-03,  -5.32393344e-03,   2.55435333e-02, ...,\n",
      "            2.41572559e-02,  -4.39411588e-03,  -3.61267198e-03],\n",
      "         [ -3.35174091e-02,  -4.71788750e-04,  -1.89030031e-03, ...,\n",
      "           -1.80381369e-02,  -2.45300587e-02,   1.39881019e-02]]],\n",
      "\n",
      "\n",
      "       [[[  7.61042954e-03,   1.04386183e-02,  -1.00550121e-02, ...,\n",
      "           -3.01598776e-02,  -1.50406535e-03,  -2.91354097e-02],\n",
      "         [ -4.27046139e-03,  -1.82950888e-02,  -6.57651713e-03, ...,\n",
      "           -1.78294834e-02,   4.73052729e-03,   2.23943964e-02],\n",
      "         [ -2.31786966e-02,  -9.92427766e-02,   5.98972104e-03, ...,\n",
      "            9.66396183e-05,  -2.03686915e-02,   4.27836925e-03],\n",
      "         ..., \n",
      "         [ -1.28109772e-02,   5.41098379e-02,  -1.39379324e-02, ...,\n",
      "           -1.57056917e-02,  -6.50382228e-03,  -2.78936513e-03],\n",
      "         [  2.83884965e-02,   6.40330557e-03,  -6.29375782e-03, ...,\n",
      "            9.11503099e-04,  -1.56801939e-03,   2.77893711e-03],\n",
      "         [ -9.88574792e-03,   8.46741796e-02,  -3.64929147e-04, ...,\n",
      "           -8.81513674e-03,  -7.82969780e-03,   1.30494256e-02]],\n",
      "\n",
      "        [[ -1.28392950e-02,  -3.14322747e-02,   7.72788515e-03, ...,\n",
      "            7.86561053e-03,  -3.31449416e-03,  -1.42503837e-02],\n",
      "         [  2.85194907e-03,  -4.81893495e-02,  -2.89769354e-03, ...,\n",
      "            7.33011402e-04,  -7.30074011e-03,   6.48375787e-03],\n",
      "         [ -9.98661388e-03,  -1.00033276e-01,  -7.69856153e-03, ...,\n",
      "           -1.68132335e-02,  -9.82096978e-03,  -9.69390571e-03],\n",
      "         ..., \n",
      "         [  8.73443857e-03,   6.36896938e-02,  -9.19148326e-03, ...,\n",
      "           -1.77190397e-02,   5.99870691e-04,   1.60248298e-03],\n",
      "         [ -3.81001551e-03,  -2.42578015e-02,   4.89771087e-03, ...,\n",
      "            2.47325413e-02,  -1.67672914e-02,   1.68947596e-03],\n",
      "         [ -1.17847286e-02,   7.02912956e-02,  -1.09899314e-02, ...,\n",
      "           -1.18305832e-02,  -1.87017210e-02,  -1.62333250e-03]],\n",
      "\n",
      "        [[ -7.01010600e-03,  -4.11743224e-02,   1.23706842e-02, ...,\n",
      "           -3.37782316e-02,   6.34631957e-04,   1.09203262e-02],\n",
      "         [  1.21118352e-02,  -2.70961449e-02,   1.55865299e-02, ...,\n",
      "           -2.14734860e-03,  -1.92798600e-02,   1.99352931e-02],\n",
      "         [ -2.84481514e-02,  -4.23205942e-02,  -1.15808835e-02, ...,\n",
      "           -1.91950779e-02,  -2.40290929e-02,  -1.34390360e-02],\n",
      "         ..., \n",
      "         [ -1.34067796e-02,   2.91407481e-02,   5.58794662e-03, ...,\n",
      "           -1.52014531e-02,  -2.63300003e-03,  -1.49833942e-02],\n",
      "         [ -2.18414702e-03,  -1.51820974e-02,  -4.36946750e-03, ...,\n",
      "           -1.81546248e-02,  -1.13781272e-02,   1.73118263e-02],\n",
      "         [ -2.66349874e-02,  -2.92454939e-02,   1.14251496e-02, ...,\n",
      "           -1.10744163e-02,  -2.90449173e-03,  -3.76626849e-06]],\n",
      "\n",
      "        ..., \n",
      "        [[ -1.45608475e-02,  -3.70078422e-02,   1.16290404e-02, ...,\n",
      "           -7.77267665e-03,   3.62921460e-03,  -3.94260883e-03],\n",
      "         [  3.43892723e-04,  -1.57713499e-02,   1.49636064e-02, ...,\n",
      "           -1.39447078e-02,  -1.60834845e-02,   1.68342132e-03],\n",
      "         [ -1.28346551e-02,  -1.51492320e-02,  -1.23773934e-03, ...,\n",
      "           -8.84538330e-03,  -2.41454914e-02,   4.56601009e-03],\n",
      "         ..., \n",
      "         [ -1.39870727e-02,   3.76211405e-02,   1.88059566e-04, ...,\n",
      "           -1.15539217e-02,  -1.93781196e-03,  -6.58543035e-03],\n",
      "         [ -1.61421311e-04,  -1.97869875e-02,   6.54304959e-03, ...,\n",
      "            2.31438614e-02,   6.74524810e-03,   1.00764073e-02],\n",
      "         [ -2.43116263e-02,  -4.49054060e-04,  -1.21902265e-02, ...,\n",
      "           -2.04249267e-02,  -1.02906432e-02,   9.75067355e-03]],\n",
      "\n",
      "        [[ -4.74062609e-03,   3.87095809e-02,  -9.55402013e-03, ...,\n",
      "           -2.52272468e-03,   2.21123248e-02,  -1.40681099e-02],\n",
      "         [ -1.45461960e-02,  -2.66291108e-02,  -3.85252200e-03, ...,\n",
      "           -4.18434665e-03,  -2.22121459e-03,   1.94811542e-02],\n",
      "         [ -2.76310146e-02,  -3.97127122e-03,  -5.15148370e-03, ...,\n",
      "            3.42033524e-03,  -1.54214948e-02,  -1.09927449e-03],\n",
      "         ..., \n",
      "         [ -2.32745223e-02,   1.50762340e-02,  -1.14334533e-02, ...,\n",
      "            2.99519021e-03,  -3.71237728e-03,  -1.43178729e-02],\n",
      "         [  1.41389323e-02,  -4.66452679e-03,  -2.45675556e-02, ...,\n",
      "            1.71733666e-02,  -1.53170181e-02,  -4.30982560e-04],\n",
      "         [  1.19191257e-03,  -3.49716004e-03,  -1.13747315e-03, ...,\n",
      "           -1.11335181e-02,  -1.32437572e-02,  -1.73171945e-02]],\n",
      "\n",
      "        [[ -8.33342317e-03,   8.74288306e-02,  -2.94562317e-02, ...,\n",
      "           -1.02866041e-02,   1.37672722e-02,  -3.45251560e-02],\n",
      "         [ -1.98174594e-03,  -3.85910645e-02,  -5.63807786e-03, ...,\n",
      "           -1.76163856e-03,   1.90563872e-03,   1.37093700e-02],\n",
      "         [ -3.28146853e-02,  -1.40077127e-02,  -8.98696110e-03, ...,\n",
      "           -3.21037695e-03,   4.56575165e-03,  -2.82345247e-03],\n",
      "         ..., \n",
      "         [ -2.21479088e-02,  -1.59267010e-03,  -1.85460073e-03, ...,\n",
      "           -4.39473335e-03,  -1.94107182e-02,  -1.01282811e-02],\n",
      "         [  2.53438158e-03,   1.59356766e-03,  -4.36623814e-03, ...,\n",
      "            1.36452820e-02,  -1.66893043e-02,  -1.62528399e-02],\n",
      "         [ -8.20017513e-03,  -1.16801411e-02,   6.71892567e-03, ...,\n",
      "           -1.99063309e-02,  -7.28680287e-03,  -1.51315611e-02]]]], dtype=float32), array([-0.00778679,  0.02267137, -0.00918498, -0.01300845, -0.00437762,\n",
      "       -0.02020199,  0.00726145, -0.01074333, -0.00931242,  0.01627854,\n",
      "       -0.00427226,  0.01466919,  0.03350379,  0.00196679, -0.02159327,\n",
      "       -0.00985686, -0.01545246, -0.00998766, -0.00981818,  0.04593164,\n",
      "        0.01138478, -0.00958078, -0.01028887, -0.00959681, -0.00985023,\n",
      "       -0.02106862,  0.01559717, -0.00972634, -0.02002896, -0.00983374,\n",
      "        0.07719703,  0.02742655, -0.00974489, -0.0099999 ,  0.01832416,\n",
      "       -0.00183267, -0.01081037, -0.00993669, -0.02001518,  0.02111278,\n",
      "       -0.00987576, -0.00727232, -0.00999985, -0.01043119,  0.01409453,\n",
      "        0.02321058, -0.01683163, -0.01992939, -0.02014741,  0.05352654,\n",
      "       -0.02925028, -0.00639616, -0.00932962, -0.01005936, -0.02107185,\n",
      "       -0.00923268, -0.02919354, -0.01984665, -0.01572929, -0.02220442,\n",
      "       -0.02014098, -0.01004995,  0.00542906, -0.01008114, -0.01117081,\n",
      "       -0.01019695, -0.01999501, -0.00990673, -0.00701492, -0.02130529,\n",
      "        0.06810264, -0.00981678, -0.00333962, -0.01014479, -0.01102549,\n",
      "       -0.02381952, -0.00425246, -0.004611  , -0.00999992, -0.01147706,\n",
      "       -0.01021812, -0.00941046,  0.0424635 , -0.01017422,  0.01222386,\n",
      "       -0.01031293, -0.01003722, -0.00976475,  0.00748641, -0.00999993,\n",
      "       -0.00999956, -0.01001514, -0.009774  , -0.00816149,  0.02295991,\n",
      "       -0.00834548, -0.00812937, -0.00662582,  0.01753586, -0.01997112,\n",
      "       -0.02241383, -0.01214954,  0.02164463, -0.00868743, -0.00964785,\n",
      "       -0.01239639,  0.01814479, -0.01398257, -0.00984652, -0.01998871,\n",
      "       -0.01009323, -0.00980813, -0.01075658, -0.009947  , -0.02288976,\n",
      "       -0.01005905,  0.01377454, -0.02011328, -0.04040527,  0.04263104,\n",
      "       -0.01849446,  0.05505971, -0.00999587, -0.03640111, -0.01179389,\n",
      "       -0.01996178,  0.01934381, -0.00551382, -0.03269847,  0.05375337,\n",
      "       -0.00907886,  0.02195234, -0.01019922, -0.00889275, -0.00271933,\n",
      "       -0.00979278,  0.00671176, -0.00813121, -0.00972539, -0.00796276,\n",
      "       -0.01008587, -0.0101552 , -0.01243621, -0.00312025, -0.01224018,\n",
      "       -0.00986464,  0.00628633, -0.0098589 , -0.00316682,  0.06817928,\n",
      "       -0.00793293, -0.00989696, -0.01216399, -0.01011116, -0.01710295,\n",
      "       -0.00343387, -0.01785274, -0.02002136,  0.0542244 , -0.00984575,\n",
      "       -0.01058052, -0.01896316, -0.00958275, -0.01597839, -0.00988157,\n",
      "       -0.00854772, -0.01566437, -0.03899673, -0.02073069,  0.01542966,\n",
      "       -0.00938596, -0.00974201, -0.00985383, -0.01849949, -0.00242078,\n",
      "       -0.00987053,  0.02810944, -0.00821774, -0.01147095,  0.02815185,\n",
      "       -0.02082488, -0.00818609,  0.0447728 ,  0.02849289, -0.03672111,\n",
      "       -0.01170418,  0.03045965,  0.05190775, -0.00987835, -0.00479642,\n",
      "       -0.0082869 , -0.0088553 , -0.00996681,  0.00198404, -0.03491157,\n",
      "       -0.00992732, -0.01109702, -0.00929266, -0.00800469, -0.0105081 ,\n",
      "       -0.00994935, -0.01826704,  0.02292499, -0.01011152, -0.01752591,\n",
      "        0.06310596,  0.02716391, -0.00937101,  0.01341827, -0.03223446,\n",
      "       -0.00980049, -0.0372198 ,  0.02766415,  0.01651371, -0.00163735,\n",
      "       -0.02025409, -0.00994113, -0.0097903 , -0.00182713, -0.00979114,\n",
      "       -0.00747754, -0.02382977, -0.00584388,  0.01440249, -0.01431056,\n",
      "        0.0496663 , -0.01996987, -0.01006917,  0.02027536, -0.02003071,\n",
      "        0.01784468,  0.09374101, -0.02413306,  0.04799328, -0.02262584,\n",
      "       -0.01599613,  0.01615694, -0.01999019, -0.01112471,  0.01819837,\n",
      "       -0.01486149, -0.01009136,  0.01999474, -0.02210971, -0.00862195,\n",
      "        0.0131934 ,  0.00512416, -0.01024863, -0.00998745, -0.00983332,\n",
      "       -0.00984172,  0.02530896, -0.01712683, -0.02003137, -0.0006775 ,\n",
      "       -0.01004398, -0.00876663,  0.04248947, -0.02014116, -0.00782561,\n",
      "       -0.00568833, -0.01032404, -0.01705351, -0.02309516,  0.04165289,\n",
      "       -0.0084487 , -0.0191974 , -0.00668745,  0.06345753, -0.01737614,\n",
      "        0.06520291,  0.03603382, -0.00825422, -0.00336523,  0.04828835,\n",
      "        0.01408677, -0.0067625 , -0.00892198, -0.0164212 ,  0.03849237,\n",
      "        0.01106728, -0.00952279,  0.02912942, -0.01020688,  0.05323455,\n",
      "        0.01255181, -0.00999988, -0.01033413,  0.01569451, -0.00712594,\n",
      "       -0.01001312, -0.01017492, -0.02614396, -0.01210936,  0.00709033,\n",
      "       -0.00942135, -0.00966962, -0.00985989,  0.02564769,  0.05510299,\n",
      "       -0.03586039, -0.01615649, -0.02916674,  0.01867383,  0.00766483,\n",
      "       -0.01998415, -0.01041934, -0.02002948, -0.01733698, -0.0125597 ,\n",
      "       -0.01749945, -0.00463564, -0.03485976,  0.05428164, -0.02126544,\n",
      "       -0.04411921,  0.08395047,  0.05944393, -0.01308496, -0.01876564,\n",
      "       -0.01002248, -0.01992364, -0.00833966, -0.03440237,  0.02558205,\n",
      "       -0.00981452, -0.0098468 , -0.01805571,  0.00598163,  0.03897423,\n",
      "       -0.00986244,  0.03463558,  0.05826214, -0.01127014,  0.04436694,\n",
      "       -0.00909546, -0.00811089,  0.05153646, -0.01005623,  0.04428628,\n",
      "       -0.00976615,  0.014218  , -0.02260485, -0.01033761, -0.00990941,\n",
      "       -0.0099999 , -0.01061424, -0.00395816,  0.04283537, -0.00942919,\n",
      "       -0.01023098,  0.02620401,  0.01796586, -0.02358558, -0.02009003,\n",
      "        0.08569419, -0.01972624, -0.02150785,  0.00804986, -0.00476308,\n",
      "       -0.0099929 , -0.01999637, -0.0075636 , -0.01005073, -0.02723434,\n",
      "       -0.00985449, -0.00999169, -0.00983821, -0.01815484, -0.00943966,\n",
      "        0.04390211,  0.01445722, -0.01688789, -0.01024416,  0.01377695,\n",
      "       -0.0097151 ,  0.04504786, -0.0035917 ,  0.03339258,  0.05477189,\n",
      "       -0.02642654, -0.0192055 , -0.01004778,  0.05317564, -0.00986326,\n",
      "        0.02891203,  0.03949774,  0.04447027,  0.06685415, -0.01993954,\n",
      "       -0.02049633, -0.01197361, -0.02081995,  0.05361093, -0.0078181 ,\n",
      "       -0.01011441, -0.01997599, -0.00999993,  0.02221967, -0.00987221,\n",
      "        0.04281228,  0.01283102, -0.01892906, -0.00977503,  0.04301813,\n",
      "       -0.01010167, -0.01050785, -0.00857995,  0.03918313, -0.02249588,\n",
      "       -0.00996505,  0.0274163 ,  0.01202252, -0.00918048,  0.04844373,\n",
      "       -0.0206976 ,  0.02591148, -0.020086  ,  0.06124988, -0.00999984,\n",
      "       -0.0096715 , -0.00964168,  0.02079787, -0.01366666,  0.03866694,\n",
      "       -0.00994895,  0.03840386, -0.00989011, -0.01105116, -0.01991278,\n",
      "       -0.00963718,  0.06936645, -0.00991195,  0.03576905, -0.00644725,\n",
      "       -0.01934455, -0.00999715,  0.04061247, -0.01148339, -0.00999971,\n",
      "       -0.0073804 ,  0.02419535,  0.0055286 , -0.00981617, -0.00600762,\n",
      "       -0.05278109, -0.00875421, -0.01001962, -0.02123177, -0.00868574,\n",
      "       -0.01010192, -0.01991035, -0.00974673, -0.01856846,  0.04465215,\n",
      "        0.03115064, -0.00950437, -0.00989733, -0.00768205, -0.01003235,\n",
      "       -0.01031988, -0.00983488, -0.00700213, -0.01999837,  0.02363223,\n",
      "       -0.01002424, -0.00113578, -0.00994511, -0.01309357,  0.00506275,\n",
      "        0.03103097,  0.00198112,  0.01736633,  0.07311706, -0.02278907,\n",
      "       -0.00931502, -0.00243825,  0.0626714 , -0.01997519, -0.0194322 ,\n",
      "        0.00550456, -0.01078534,  0.04995304,  0.01133153, -0.00301591,\n",
      "        0.05708254, -0.0100763 , -0.01005898, -0.00877907, -0.00715347,\n",
      "       -0.02684247, -0.01010301, -0.02189244,  0.00978526, -0.00976107,\n",
      "       -0.00989857, -0.02829115,  0.02609661, -0.00987526, -0.01568145,\n",
      "       -0.00984747, -0.01016646, -0.01250259, -0.00880343, -0.01100733,\n",
      "       -0.0200043 ,  0.00299716,  0.04879382, -0.01001904, -0.01224391,\n",
      "       -0.01027934, -0.01989779], dtype=float32)]\n",
      "{'name': 'max_pooling2d_2', 'pool_size': (2, 2), 'padding': 'valid', 'trainable': True, 'data_format': 'channels_first', 'strides': (2, 2)}\n",
      "[]\n",
      "{'name': 'flatten_1', 'trainable': True}\n",
      "[]\n",
      "{'bias_constraint': None, 'name': 'dense_1', 'bias_regularizer': None, 'units': 20, 'kernel_constraint': None, 'activation': 'relu', 'kernel_regularizer': None, 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}, 'kernel_initializer': {'config': {'distribution': 'uniform', 'scale': 1.0, 'mode': 'fan_avg', 'seed': None}, 'class_name': 'VarianceScaling'}, 'trainable': True, 'use_bias': True, 'activity_regularizer': None}\n",
      "[array([[ 0.00078416, -0.00527503,  0.0104945 , ..., -0.00171339,\n",
      "        -0.0004511 , -0.01790748],\n",
      "       [-0.00088894,  0.00269296,  0.01412985, ...,  0.00050012,\n",
      "        -0.00352192, -0.0029706 ],\n",
      "       [-0.02044474, -0.00896424,  0.00749229, ...,  0.00058287,\n",
      "        -0.00793951, -0.00025431],\n",
      "       ..., \n",
      "       [-0.00227103, -0.01106693, -0.00300826, ...,  0.00087095,\n",
      "         0.00068058,  0.01224161],\n",
      "       [-0.01651339,  0.00118526,  0.00765692, ..., -0.00316576,\n",
      "        -0.01204318,  0.0030683 ],\n",
      "       [-0.00805305, -0.00653401,  0.00957058, ...,  0.01000872,\n",
      "         0.01888051,  0.0069288 ]], dtype=float32), array([ -9.99968592e-03,  -9.99998208e-03,   2.61491444e-02,\n",
      "        -9.99994576e-03,  -9.99976695e-03,  -9.99998674e-03,\n",
      "         3.53131406e-02,  -9.99992900e-03,   3.59570794e-02,\n",
      "         6.76796865e-03,  -7.39664029e-05,   1.98907219e-04,\n",
      "         1.19569607e-03,  -9.99910943e-03,   1.14581548e-02,\n",
      "         1.31085487e-02,  -9.99996345e-03,  -9.99644678e-03,\n",
      "        -9.99952853e-03,  -9.99867544e-03], dtype=float32)]\n",
      "{'bias_constraint': None, 'name': 'dense_2', 'bias_regularizer': None, 'units': 10, 'kernel_constraint': None, 'activation': 'softmax', 'kernel_regularizer': None, 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}, 'kernel_initializer': {'config': {'distribution': 'uniform', 'scale': 1.0, 'mode': 'fan_avg', 'seed': None}, 'class_name': 'VarianceScaling'}, 'trainable': True, 'use_bias': True, 'activity_regularizer': None}\n",
      "[array([[ 0.03855029, -0.20412464, -0.43154356,  0.01339203, -0.25620857,\n",
      "         0.28315634,  0.14668243,  0.39270467,  0.32299638, -0.42139611],\n",
      "       [-0.21064429, -0.05678514, -0.22717348,  0.40792337, -0.34587604,\n",
      "        -0.16498785,  0.42097196,  0.083704  ,  0.11648631,  0.00970213],\n",
      "       [ 0.21766712, -0.14667955, -0.28506294, -0.40641224,  0.54502875,\n",
      "         0.22150576, -0.21461324, -0.17178994,  0.21522243,  0.30592388],\n",
      "       [ 0.10290504, -0.41593984,  0.38705882,  0.10358489,  0.38494304,\n",
      "         0.33217531, -0.23453799,  0.38405642, -0.02359628,  0.09710849],\n",
      "       [-0.11780225,  0.44248584,  0.31617147, -0.1720015 ,  0.35084283,\n",
      "         0.2998791 , -0.20027661,  0.14872023,  0.00906091,  0.19639476],\n",
      "       [-0.43310013, -0.42519587, -0.31994462, -0.38308001,  0.10712449,\n",
      "        -0.04051692,  0.02267924, -0.09016985,  0.30155933, -0.18621211],\n",
      "       [ 0.41134924,  0.1285136 , -0.14698468,  0.29227895, -0.12120046,\n",
      "        -0.30613133, -0.37802705, -0.49392092,  0.19074918,  0.41497588],\n",
      "       [-0.14855725, -0.09481155,  0.12704529, -0.3790479 ,  0.34858486,\n",
      "        -0.19547842, -0.36249468,  0.03051749,  0.2662676 , -0.2166923 ],\n",
      "       [ 0.40094534, -0.10192473, -0.05648325, -0.4577513 , -0.22988874,\n",
      "        -0.38571456,  0.5348928 , -0.34463578, -0.3178007 , -0.30411497],\n",
      "       [ 0.0032318 ,  0.3949438 ,  0.18941145, -0.26478299, -0.30759305,\n",
      "        -0.14195931,  0.59703326,  0.65533775,  0.42249352,  0.41890389],\n",
      "       [ 0.23288345, -0.44899014, -0.48093632,  0.48854285, -0.328787  ,\n",
      "        -0.15521707, -0.11668062,  0.04321625,  0.06110293, -0.08650612],\n",
      "       [-0.12073032,  0.29783872,  0.21395342, -0.33059943,  0.35075811,\n",
      "         0.23640618,  0.03176703,  0.09047778,  0.00745856,  0.21878429],\n",
      "       [-0.21821962,  0.29644057,  0.34424266,  0.26024577,  0.15634343,\n",
      "        -0.37610027, -0.07728462, -0.18904713,  0.40075061, -0.46661246],\n",
      "       [-0.04883036, -0.42180923, -0.27725485, -0.15930471, -0.31081682,\n",
      "         0.37499255, -0.23395082,  0.20968613, -0.37657553,  0.39433429],\n",
      "       [-0.30474365,  0.33096293, -0.35077155, -0.01030806, -0.27988723,\n",
      "         0.69103897,  0.07788316,  0.04651643, -0.46066248,  0.3840318 ],\n",
      "       [ 0.13208498,  0.14473125,  0.65467685,  0.37075013,  0.3376863 ,\n",
      "         0.43065718,  0.05985806,  0.49322858, -0.18907997, -0.00975014],\n",
      "       [ 0.00847145, -0.19787051,  0.14496928,  0.24975422, -0.07587193,\n",
      "        -0.14167507,  0.42874655,  0.31886646,  0.1018827 , -0.39559942],\n",
      "       [-0.1654727 , -0.24838416,  0.15835752, -0.02459941,  0.0855282 ,\n",
      "        -0.00418181,  0.4012818 , -0.06423897, -0.26319715, -0.38598099],\n",
      "       [-0.03623555, -0.29408142, -0.13409764, -0.09230629, -0.31775254,\n",
      "         0.15518863, -0.45302349, -0.05937614,  0.23391245, -0.17177758],\n",
      "       [ 0.38327646,  0.02466462, -0.44015515, -0.0470049 ,  0.16647138,\n",
      "        -0.24128985, -0.01528456, -0.1606226 ,  0.18715   ,  0.43073583]], dtype=float32), array([-0.07537897, -0.22477522,  0.17054492, -0.07986869,  0.26764336,\n",
      "        0.27828124, -0.11874143,  0.1776811 ,  0.23807149,  0.00041903], dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, 5, 5, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(512, 7, 7, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "model.fit(X_train, Y_train, batch_size=400, nb_epoch=8, verbose=1, \\\n",
    "            validation_data=(X_test, Y_test))\n",
    "weights = model.get_weights()\n",
    "for layer in model.layers:\n",
    "    g=layer.get_config()\n",
    "    h=layer.get_weights()\n",
    "    print (g)\n",
    "    print (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), padding=\"same\", input_shape=(3, 32, 32..., activation=\"relu\")`\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (7, 7), padding=\"same\", activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 16, 32, 32)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 512, 16, 16)       401920    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 512, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                655380    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,058,726\n",
      "Trainable params: 1,058,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/2\n",
      "73257/73257 [==============================] - 277s - loss: 0.2921 - acc: 0.9061 - val_loss: 0.2049 - val_acc: 0.9279\n",
      "Epoch 2/2\n",
      "73257/73257 [==============================] - 276s - loss: 0.1524 - acc: 0.9462 - val_loss: 0.1303 - val_acc: 0.9554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'functor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b2dfb19ffe6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mfunctors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m  \u001b[1;31m# evaluation functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mlayer_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunctor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'functor' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, 5, 5, border_mode='same', activation='relu', input_shape=(3, 32, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(512, 7, 7, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adagrad', metrics=['accuracy'])\n",
    "adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "model.fit(X_train, Y_train, batch_size=100, nb_epoch=2, verbose=1, \\\n",
    "            validation_data=(X_test, Y_test))\n",
    "inp = model.input    \n",
    "outputs = [layer.output for layer in model.layers]\n",
    "for out in outputs\n",
    "    functors = [K.function([inp]+ [K.learning_phase()], [out]) ]  # evaluation functions\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<p>Elija una de las redes entrenadas en esta sección y determine los pares de dígitos (por ejemplo 1 con\n",
    "7) que la red tiende a confundir. Conjeture el motivo de tal confusión.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_outs = [func([X_test, 1.]) for func in functors]\n",
    "print(layer_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
